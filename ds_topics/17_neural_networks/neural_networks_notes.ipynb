{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks\n",
    "\n",
    "* Objectives:\n",
    "    * Know the best use cases for neural networks\n",
    "    * Know the benefits and drawbacks of using a neural network\n",
    "    * Build a simple neural network for binary classification\n",
    "    * Train a neural network using backpropagation\n",
    "    * Understand how neural networks can be used for regression and multi-class classification by using different loss functions and output activations\n",
    "    * Explain the properties (pros/cons) of different activation functions\n",
    "    * Explain some methods to avoid overfitting\n",
    "    * Learn about some more complicated versions of neural networks\n",
    "    * Use Keras to build neural networks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Neural Network Basics\n",
    "* Background - neural networks were introduced in the 1950's as a model which mimics the brain\n",
    "    * biological neurons \"fire\" at a certain voltage threshold\n",
    "    * an artifical neuron will be modeled by an activation function like **sign**, **sigmoid function**, or **tanh**\n",
    "    * otherwise, it is bad analogy since we shouldn't be thinking of neural networks as models for the brain\n",
    "* Why use Neural Networks?\n",
    "    * (+) works well with high dimensional data (images, text, and audio)\n",
    "    * (+) can model *arbitrarily* complicated decision functions (complex decision boundaries)\n",
    "    * (-) not very interpretable\n",
    "    * (-) slow to train (very computationally expensive)\n",
    "    * (-) easy to overfit (complex activation that lowers bias, but has high variance)\n",
    "    * (-) difficult to tune (many parameters/choices when building the neural network architecture)\n",
    "* What makes Neural Network different from other machine learning techniques?\n",
    "    * (+) Deep learning makes problem-solving much easier because it completely **automates feature engineering** (unlike other machine learning workflows)\n",
    "        * Deep learning completely automates the feature engineering step by learning all features in one pass rather than having to engineer them yourself\n",
    "    * Previous machine learning techniques (shallow learning) only involved transforming the input data into one or two successive representation spaces (e.g. SVM).\n",
    "    * Can shallow machine learning methods be applied repeatedly to emulate effects of deep learning?\n",
    "        * (-) In practice, there are fast-diminishing returns to successive applications of shallow-learning methods, because **optimal first representation layer in a three-layer model isn't the optimal first layer in a one-layer or two-layer model**\n",
    "        * (+) What is transformative about deep learning is that it allows a model to learn all layers of representation **jointly**, at the same time, rather than in succession (**greedily**).\n",
    "        * (+) With **joint feature learning**, whenever the model adjusts one of its internal features, all other features that depend on it automatically adapt to change, without requiring human intervention\n",
    "* Why is deep learning popular now?\n",
    "    * Hardware advances\n",
    "        * NVIDIA TITAN X costs only $1000 at the end of 2015 and can deliver peark of 6.6 TFLOPS in a single precision (only takes a couple of days to train an ImageNet model)\n",
    "        * In 2016, Google is developing their own **tensor processing unit (TPU)**: a new chip design developed from the ground up to run deep neural networks (reportedly 10x faster and more energy efficient than top of line GPUs)\n",
    "    * Algorithm advances - key issue is **gradient propagation** through deep stacks of layers where the feedback signal used to train neural networks would fade away as the number of layers increased\n",
    "        * Better **activation functions** for neural layers\n",
    "        * Better **weight-initialization schemes**, starting with layer-wise pretraining, which was quickly abandoned\n",
    "        * Better **optimization schemes**, such as RMSProp and Adam\n",
    "* Examples For Branches of Machine Learning in Neural Networks\n",
    "    * Supervised learning\n",
    "        * Examples: optical character recognition, speech recognition, image classification, and language translation\n",
    "        * **Sequence generation** - given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems (such as repeatedly predicting a word or token in a sequence)\n",
    "        * **Syntax tree prediction** - given a sentence, predict its decomposition into a syntax tree\n",
    "        * **Object detection** - given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem (given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression\n",
    "        * **Image segmentation** - given a picture, draw a pixel-level mask on a specific object\n",
    "    * Unsupervised learning\n",
    "        * Purpose: data visualization, data compression, data denoising, or understanding correlations present in data\n",
    "        * Often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem (e.g. dimensionality reduction and clustering)\n",
    "    * **Self-supervised learning** - supervised learning without human-annotated labels or supervised learning without any humans in the loop\n",
    "        * there are still labels involved, but they're generated from the input data, typically using a heuristic algorithm (e.g. autoencoders)\n",
    "        * **Autoencoders** - generated targets are the input, unmodified\n",
    "        * **Temporally Supervised Learning** - supervision comes from the future input data\n",
    "            * e.g. predicting the nextframe in a video, given past frames\n",
    "            * e.g. predicting next word in a text, given previous words\n",
    "        * Self-supervised learning can be reinterpreted as either supervised or unsupervised learning (depending on whether you pay attention to the learning mechanism or to the context of its application)\n",
    "    * **Reinforcement learning** - an **agent** receives information about its environment and learns to choose actions that will maximize some reward\n",
    "        * e.g. neural network \"looks\" at video-game screen and outputs game actions in order to maximize its score can be trained\n",
    "        * real-world applications: self-driving cars, robotics, resource management, education, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Data Representation For Neural Networks\n",
    "* Tensor Basics:\n",
    "    * Data stored in multidimensional Numpy arrays called **tensors**\n",
    "    * All current machine learning systems use **tensors** as their basic data structure\n",
    "    * At its core, a **tensor** is a container for data (almost always numerical data)\n",
    "* **Scalars (0D tensors)**\n",
    "    * A tensor that contains only one number is called a **scalar** (or scalar tensor, or 0-dimensional tensor, or 0D tensor). A scalar tensor has 0 axes. The number of axes of a tensor is also called its **rank** (a tensor of rank 0)\n",
    "        ```python\n",
    "        >>> x = np.array(12)\n",
    "        >>> x.ndim\n",
    "        0\n",
    "        ```\n",
    "* **Vectors (1D tensors)**\n",
    "    * An array of numbers is called a **vector**, or 1D tensor. A 1D tensor is said to have exactly one axis. Numpy vector below:\n",
    "        ```python\n",
    "        >>> x = np.array([12, 3, 6, 14, 5])\n",
    "        >>> x.ndim\n",
    "        1\n",
    "        ```\n",
    "    * This vector has five entries and so is called **5-dimensional vector**. Don't confuse a 5D vector with a 5D tensor!\n",
    "* **Matrices (2D tensors)**\n",
    "    * An array of vectors is a **matrix**, or 2D tensor. A matrix has two axes (often referred to rows and columns). Numpy matrix below:\n",
    "        ```python\n",
    "        >>> x = np.array([[5, 78, 2, 34, 0],\n",
    "                        [6, 79, 3, 35, 1],\n",
    "                        [7, 80, 4, 36, 2]])\n",
    "        >>> x.ndim\n",
    "        2\n",
    "        ```\n",
    "* **3D tensors and higher-dimensional tensors**\n",
    "    * If you pack such matrices in a new array, you obtain 3D tensor, which you can visually interpret as a cube of numbers. Numpy 3D tensor below:\n",
    "        ```python\n",
    "        >>> x = np.array([[[5, 78, 2, 34, 0],\n",
    "                         [6, 79, 3, 35, 1],\n",
    "                         [7, 80, 4, 36, 2]],\n",
    "                        [[5, 78, 2, 34, 0],\n",
    "                         [6, 79, 3, 35, 1],\n",
    "                         [7, 80, 4, 36, 2]],\n",
    "                        [[5, 78, 2, 34, 0],\n",
    "                         [6, 79, 3, 35, 1],\n",
    "                         [7, 80, 4, 36, 2]]])\n",
    "        >>> x.ndim \n",
    "        3\n",
    "        ```\n",
    "    * By packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learning, you'll generally manipulate tensors that are 0D to 4D, although you may go up to 5D if you process video data\n",
    "* Real-world examples of data tensors:\n",
    "    * **Vector data** - 2D tensors of shape `(samples, features)`\n",
    "        * e.g. 100,000 people with age, zipcode, and income $\\rightarrow$ `(10000, 3)`\n",
    "        * 2D tensors is often processed by **densely connected** layers (also called **fully connected or dense** layers)\n",
    "    * **Timeseries data or sequence data** - 3D tensors of shape `(samples, timesteps, features)`\n",
    "        * e.g. stock prices with current, highest past minute, and lowest past minute in entire day of trading (390 minutes) with total of 250 days $\\rightarrow$ `(250, 390, 3)`\n",
    "        * 3D tensors is typically processed by **recurrent** layers such as an **LSTM** layer\n",
    "    ![timeseries_data](timeseries_data.png)\n",
    "    * **Images** - 4D tensors of shape `(samples, height, width, channels)` or `(samples, channels, height, width)`\n",
    "        * e.g. batch of 128 color images of 256x256 pixels $\\rightarrow$ `(128, 256, 256, 3)`\n",
    "        * 4D tensors is usually processed by 2D convolution layers (`Conv2D`)\n",
    "    ![image_data](image_data.png)\n",
    "        * There are two conventions for shapes of images tensors:\n",
    "            * the **channels-last** convention used by Tensorflow\n",
    "            * the **channels-first** convention used by Theano\n",
    "    * **Video** - 5D tensors of shape `(samples, frames, height, width, channels)` or `(samples, frames, channels, height, width)`\n",
    "        * e.g. a 60-second, 144x256 YouTube video clip sampled at 4 frames per second (for a total of 240 frames) $\\rightarrow$ `(4, 240, 144, 256, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Tensor Operations (Gears of Neural Networks)\n",
    "* Much as any computer program can be ultimately reduced to a small set of binary operations on **binary inputs** (AND, OR, NOR, and so on). All transformations learned by deep neural networks can be reduced to a handful of **tensor operations** applied to tensors of numeric data (e.g. add tensors, multiply tensors, etc.)\n",
    "* Example: A layer that takes input a 2D tensor and returns another 2D tensor (a new representation for the input tensor)\n",
    "    * `output = relu(dot(W, input) + b)`\n",
    "        * `W` is a 2D tensor\n",
    "        * `b` is a vector\n",
    "    * There are three tensor operations:\n",
    "        * a dot product (`dot`) between the input tensor and the `W` tensor\n",
    "        * an addition (`+`) between the resulting 2D tensor and a vector `b`\n",
    "        * a `relu` operation - `relu(x)` is `max(x, 0)`\n",
    "            ```python\n",
    "            def naive_relu(x):\n",
    "                assert len(x.shape) == 2\n",
    "                \n",
    "                x = x.copy()\n",
    "                for i in range(x.shape[0])::\n",
    "                    for j in range(x.shape[1]):\n",
    "                        x[i, j] = max(x[i, j], 0)\n",
    "                return x\n",
    "            ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Data Preprocessing for Neural Networks\n",
    "* **Vectorization** - all inputs and targets in a neural network must be tensors of floating-point data\n",
    "    * **Data vectorization** - convert data that needs to be processed (e.g. sound, images, text) into tensors \n",
    "    * e.g. using one-hot encoding, turn text representations as list of integers into tensor of `float32` data\n",
    "* **Value Normalization** - converting feature values to be within the same range with values between 0 and 1\n",
    "    * In general, it isn't safe to feed into a neural network data that takes relatively large values (e.g. multi-digit integers which are much larger than the initial values taken by the weights of a network) or data is heterogeneous (e.g. data where one feature is in the range 0-1 and another is in the range 100-200)\n",
    "        * Doing so can trigger large gradient updates that will prevent the network from converging\n",
    "    * e.g. in digit classification, started with image data encoded as integers in 0-255 range, encoding grayscale values, which we cast as `float32` then divide by 255 to yield 0-1 value range\n",
    "    * Making learning easier for network:\n",
    "        * **Take small values** - typically most values should be in range 0-1\n",
    "        * **Be homogenous** - all features should take values roughly in the same range\n",
    "    * Additional stricter normalization practices:\n",
    "        * Normalize each feature independently to have mean of 0\n",
    "        * Normalize each feature independently to have a standard deviation of 1\n",
    "* **Handling Missing Values** - labelling missing values as 0\n",
    "    * In general, it's safe to input missing values as 0, with the condition that 0 isn't already a meaningful value\n",
    "    * If you are expecting missing values in the test data, but the network was trained on data without any missing values, the network won't have learned to ignore missing values\n",
    "        * solution: artifically generate training samples with missing entries - copy some training samples several times, drop some of the features that you expect are likely to be missing from the test data\n",
    "    * e.g. house-price first feature was per capita crime rate that had missing samples in training or test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Building Neural Networks Basics\n",
    "![neural_network_relationship](neural_network_relationship.png)\n",
    "* Summary: The neural network, composed of layers that are chained together, maps the input data to predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the network's predictions match what was expected. The optimizer uses this loss value to update the network's weights.\n",
    "* Neural Network Training Loop Steps: (**Mini-batch SGD**)\n",
    "    1. Draw a batch of training samples $x$ and corresponding targets $y$\n",
    "    2. Run the network on $x$ (**forward pass**) to obtain predictions `y_pred`\n",
    "    3. Compute the loss of the network on the batch, a measure of the mismatch between `y_pred` and $y$\n",
    "    4. Compute the gradient of the loss with regard to the network's parameters (**backward pass**)\n",
    "    5. Move parameters a little in the opposite direction from gradient (e.g. `W -= step * gradient`), thus reducing the loss on the batch a bit\n",
    "    ![sgd_1para_1samp](sgd_1para_1samp.png)\n",
    "* **Activation functions** - allows for non-linear transformations\n",
    "    * Keras example of activation functions in `Dense` layers\n",
    "        ```python\n",
    "        from keras import models\n",
    "        from keras import layers\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "        model.add(layers.Dense(16, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        ```\n",
    "    * What are activation functions, and why are they necessary?\n",
    "        * Without an activation function like `relu` (also called non-linearity), the `Dense` layer would consist of two linear operations (a dot product and an addition):\n",
    "            * `output = dot(W, input) + b`\n",
    "        * The **hypothesis space** of layer would be only the set of all possible linear transformations of input data without activation functions\n",
    "            * Would prevent multiple layers of representations\n",
    "            * Wouldn't extend the hypothesis space\n",
    "    * Where have we seen an activation function before? (sigmoid function in Logistic Regression!)\n",
    "        * input: $x$\n",
    "        * weights: $w$\n",
    "        * sigmoid function: $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "        * now, classify $x$ as positive if $\\sigma(w^Tx)>0.5$\n",
    "        * think of $\\sigma$ as an **activation function** which **activates** if the input is larger than 0\n",
    "* Creating two layer neural network with sigmoid activation function\n",
    "![sigmoid_two_layer](sigmoid_two_layer.png)\n",
    "    * Architecture:\n",
    "        * **Input layer**: the nodes which hold the inputs: $1,x_1,x_2,\\dots,x_n$\n",
    "        * **Output layer**: the single node that holds the output value\n",
    "        * **Weights**: the weights: $w_0,w_1,\\dots,w_n$ transition between the two layers\n",
    "    * with the current two layer architecture, $h(x|w) = \\sigma(w^Tx)$, only is able to model **linear** decision functions\n",
    "        * the decision boundary is the set of points where $w^Tx=0$, creating a hyperplane\n",
    "* Moving onto Neural Network with Multiple Layers (e.g. 4 layers)\n",
    "![4_layer_nn_tanh_sigma](4_layer_nn_tanh_sigma.png)\n",
    "    * Architecture:\n",
    "        * **Input layer (layer 0)**: contains the input value $x$ and a bias term $1$\n",
    "        * **Two hidden layer (layer 1 and 2)**\n",
    "        * **Output layer (layer 3)**: contains the output value (or the probability of positive classification)\n",
    "    * Compute output for: $x=3$\n",
    "        * Layer 1:\n",
    "            * The first non-bias node $\\rightarrow tanh((0.1)(1)+(0.3)(3))=0.76$\n",
    "            * The second non-bias node $\\rightarrow tanh((0.2)(1)+(0.4)(3))=0.89$\n",
    "        * Layer 2:\n",
    "            * The non-bias node $\\rightarrow tanh((0.2)(1)+(1)(0.76)+(-3)(0.89))=-0.94$\n",
    "        * Output:\n",
    "            * The value of the output layer $\\rightarrow \\sigma((1)(1)+(2)(-0.94))=0.29$\n",
    "        * Finally, $h(3)=0.29$ for $x=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Neural Network Mathematical Notation\n",
    "* Simplified Neural Network Architecture (At First)\n",
    "    * Stick to networks for binary classification (a single output node)\n",
    "    * Output node will use the sigmoid activation function $\\sigma$\n",
    "    * Hidden layers will use tanh activation function\n",
    "    * $\\theta$ will always represent an activation function (e.g. sign, tanh, sigmoid($\\sigma$), rectifier)\n",
    "* Schematic of Artificial Neuron\n",
    "![artifical_neuron_schematic](artifical_neuron_schematic.png)\n",
    "    * Layers are given by indices: $0,1,2,\\dots,L$\n",
    "        * Input layer: $0$\n",
    "        * Output layer: $L$\n",
    "    * For each layer $l$:\n",
    "        * $s^{(l)} \\rightarrow d^{(l)}$-dimensional input vector\n",
    "        * $x^{(l)} \\rightarrow (d^{(l)}+1)$-dimensional output vector\n",
    "        * $W^{(l)} \\rightarrow (d^{(l-1)}+1)\\times d^{(l)}$ matrix of input weights\n",
    "            * $W_{ij}^{(l)} \\rightarrow$ the weight of the edge from the $i$-th node $l-1$ to the $j$-th node in $l$\n",
    "* Convert Previous 4 Layer Network To Mathematical Notation\n",
    "![4_layer_nn_tanh_sigma](4_layer_nn_tanh_sigma.png)\n",
    "    * Steps For **Forward Propagation**:\n",
    "        1. Multiple Initial Input $x^{(0)}$ by $W^{(1)}$ between layers to yield $s^{(1)}$ through matrix multiplication\n",
    "        2. Apply activation function (e.g. tanh or sigmoid) on $s^{(1)}$ to yield $x^{(1)}$\n",
    "        3. Take the next weights $W^{(2)}$ and apply to previous output of Layer $x^{(1)}$ and repeat cycle until reaching the final output layer $x^{(o)}$\n",
    "    * **Layer 0 $\\rightarrow$ 1:**\n",
    "        * $x^{(0)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            3 \n",
    "            \\end{array}\\right]$\n",
    "        * $W^{(1)}=\\left[\\begin{array}{cc}\n",
    "            0.1 & 0.2 \\\\ \n",
    "            0.3 & 0.4 \n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(1)}$ is the result of applying the weights on the edges between layer 0 and 1: (Applying Weights)\n",
    "            * $\\left[\\begin{array}{cc}\n",
    "                (0.1)(1)+(0.3)(3) \\\\ \n",
    "                (0.2)(1)+(0.4)(3)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                1.4\n",
    "                \\end{array}\\right]$ \n",
    "        * $x^{(1)}$ is the output of layer 1 after applying tanh and adding a bias node: (Output of Layer 1 After Bias/Tanh Function)\n",
    "            * $\\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                tanh(1) \\\\\n",
    "                tanh(1.4)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                0.76 \\\\\n",
    "                0.89\n",
    "                \\end{array}\\right]$ \n",
    "    * **Layer 1 $\\rightarrow$ 2:**\n",
    "        * $W^{(2)}=\\left[\\begin{array}{cc}\n",
    "            0.2 \\\\ \n",
    "            1 \\\\\n",
    "            -3\n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(2)}=\\left[\\begin{array}{cc}\n",
    "            (0.2)(1)+(1)(0.76)+(-3)(0.89)\n",
    "            \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                -1.71\n",
    "                \\end{array}\\right]$\n",
    "        * $x^{(2)}=\\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                tanh(-1.71)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                -0.94\n",
    "                \\end{array}\\right]$ \n",
    "    * **Layer 2 $\\rightarrow$ 3:**\n",
    "        * $W^{(3)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            2\n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(3)}=\\left[\\begin{array}{cc}\n",
    "            (1)(1)+(2)(-0.94)\n",
    "            \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                -0.88\n",
    "                \\end{array}\\right]$\n",
    "        * $x^{(3)}=\\left[\\begin{array}{cc}\n",
    "                \\sigma(-0.88)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                0.29\n",
    "                \\end{array}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Forward Propagation and Backpropagation\n",
    "* **Forward Propagation** - computing the output of a neural network with fixed weights (based on studying the above examples)\n",
    "    * $x^{(l)}=\\left[\\begin{array}{cc}\n",
    "        1 \\\\ \n",
    "        \\theta(s^{(l)})\n",
    "        \\end{array}\\right]$ (Inputs that have applied activation functions)\n",
    "    * $s^{(1)}=(W^{(l)})^Tx^{(l-1)}$ (Weights that are applied to the Inputs)\n",
    "    * Propagation of computations: $x^{(0)}\\xrightarrow{W^{(1)}}s^{(1)}\\xrightarrow{\\theta}x^{(1)}\\xrightarrow{W^{(2)}}s^{(2)}\\cdots\\rightarrow s^{(L)}\\xrightarrow{\\theta}x^{(L)}=h(x^{(0)})$\n",
    "    * In terms of the number of nodes $V$ and weights $E$, what is the algorithmic complexity of forward propagation (in Big-O notation)?\n",
    "* **Backpropagation (or Reverse-mode Differentiation)** - finds the error based on some function via gradient descent and modifies weights (thereby improving the model) based on the predictions made by the Neural Network in Forward Propagation\n",
    "    * Why are we using gradient descent to modify weights?\n",
    "        * Naive approach would be to change one weight at a time holding all other weights same\n",
    "            * This methodology would be terribly expensive and inefficient\n",
    "        * Take advantage of the fact that all operations used in the network are **differentiable** and compute the **gradient** of the loss with regard to the network's coefficients\n",
    "        ![derivative](derivative.png)\n",
    "            * If $a$ is negative, it means a small change of $x$ around $p$ will result in a decrease of $f(x)$\n",
    "            * If $a$ is positive, a small change in $x$ will result in an increase of $f(x)$\n",
    "            * The absolute value of $a$ (the magnitude of the derivative) tells you how quickly this increase or decrease\n",
    "will happen\n",
    "            * If you're trying to update $x$ by a factor `epsilon_x` in order to minimize $f(x)$, and you know the derivative of $f$, then you can reduce the value of $f(x)$ by moving $x$ a little in the opposite direction from the derivative\n",
    "            * Where the derivative of function $f(x)$ of a single coefficient can be interpreted as the slope of curve, $gradient(f)(W_0)$ can be interpreted as the tensor describing the **curvature** of $f(W)$ around $W_0$\n",
    "            * With function $f(W)$ of a tensor, reduce $f(W)$ by moving $W$ in the opposite direction from the gradient (going against the curvature means putting you lower in the curve)\n",
    "    * Training data = $\\{(x_i,y_i)\\}$\n",
    "    * Need to minimize some error function $E$ on our training set over the weights: $w = (W^{(1)},\\dots,W^{(L)})$\n",
    "        * Example error function, MSE: $E(w)=\\frac{1}{N}\\sum_{i=1}(h(x_i|w)-y_i)^2$\n",
    "    * This function can be *extremely* complicated to write algebraically and has no closed form solution for minima\n",
    "    * Use gradient descnet algorithm to train neural network (called Backpropagation)\n",
    "        * Update step in gradient descent: $w(t+1)=w(t)-\\eta\\triangledown E(w(t))$ \n",
    "    * Our total error is a sum of the errors, $e_n$, on each input:\n",
    "        * $E(w)=\\frac{1}{N}\\sum_{i=1}^n e_i$ where $e_i=(h(x_i|w)-y_i)^2$\n",
    "        * take derivative with respect to weights: $\\frac{\\partial E}{\\partial W^{(l)}}=\\frac{1}{N}\\sum{\\frac{\\partial e_n}{\\partial W^{(l)}}}$ (need to review how to take derivatives)\n",
    "        * can consider one data point at a time and add the results to get the total gradient\n",
    "    * Backpropagation uses the **chain rule** to compute the partial derivatives of layer $l$ in terms of layer $l+1$\n",
    "        * the **sensitivity vector** of layer $l$: $\\delta^{(l)}=\\frac{\\partial e}{\\partial s^{(l)}}$\n",
    "        * then, we can compute: $\\frac{\\partial e}{\\partial W^{(l)}} = x^{(l-1)}(\\delta^{(l)})^T$\n",
    "        * for $j$ in $1,\\dots,d^{(l)}$: $\\delta_j^{(l)}=\\theta'(s^{(l)})_j \\times [W^{(l+1)}\\delta^{(l+1)}]_j$\n",
    "        * can compute $\\delta^{(l)}$ from $\\delta^{(l+1)}$\n",
    "        * must still compute $\\delta^{(L)}$ to seed the process\n",
    "            * depends on the error function and the output activation function\n",
    "            * in this case: $\\delta^{(L)}=2(h(x_i|w)-y_i)h(x_i|w)(1-h(x_i|w))$\n",
    "        * $W^{(l)}=W^{(l)}-\\eta\\frac{\\partial E}{\\partial W^{(l)}}$\n",
    "* Complete Computation for Forward propagation and Backpropagation example (using the 4 layer NN from above):\n",
    "    * Forward propagation:\n",
    "        * Data is $x=2,y=1$\n",
    "        * $x^{(0)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            2 \n",
    "            \\end{array}\\right]$; \n",
    "            $s^{(1)}=\\left[\\begin{array}{cc}\n",
    "            0.1 & 0.3 \\\\ \n",
    "            0.2 & 0.4\n",
    "            \\end{array}\\right]\n",
    "            \\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            2\n",
    "            \\end{array}\\right]=\n",
    "            \\left[\\begin{array}{cc}\n",
    "            0.7 \\\\ \n",
    "            1\n",
    "            \\end{array}\\right]$;\n",
    "            $x^{(1)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            0.6 \\\\\n",
    "            0.76\n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(2)}=\\left[\\begin{array}{cc}\n",
    "            -1.48 \n",
    "            \\end{array}\\right]$; \n",
    "            $x^{(2)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            -0.90 \n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(3)}=\\left[\\begin{array}{cc}\n",
    "            -0.8 \n",
    "            \\end{array}\\right]$; \n",
    "            $x^{(3)}=\\left[\\begin{array}{cc}\n",
    "            0.31 \n",
    "            \\end{array}\\right]$\n",
    "    * Backpropagation:\n",
    "        * $\\delta^{(3)}=2(0.31-1)(0.31)(1-0.31)=-0.30$\n",
    "        * $\\delta^{(2)}=(1-0.9^2)(2)(-0.30)=-0.114$\n",
    "        * $\\delta^{(1)}=\\left[\\begin{array}{cc}\n",
    "            -0.072 \\\\\n",
    "            0.144\n",
    "            \\end{array}\\right]$\n",
    "        * $\\frac{\\partial e}{\\partial W^{(1)}}=x^{(0)}(\\delta^{(1)})^T=\\left[\\begin{array}{cc}\n",
    "            -0.072 & 0.144 \\\\\n",
    "            -0.144 & 0.288\n",
    "            \\end{array}\\right]$\n",
    "        * $\\frac{\\partial e}{\\partial W^{(2)}}=x^{(1)}(\\delta^{(2)})^T=\\left[\\begin{array}{cc}\n",
    "            -0.69 \\\\\n",
    "            -0.42 \\\\\n",
    "            -0.53\n",
    "            \\end{array}\\right]$\n",
    "        * $\\frac{\\partial e}{\\partial W^{(3)}}=x^{(2)}(\\delta^{(3)})^T=\\left[\\begin{array}{cc}\n",
    "            -1.85 \\\\\n",
    "            1.67\n",
    "            \\end{array}\\right]$\n",
    "* Another example with Forward and Backpropagation:\n",
    "![example_nn](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5) Stochastic Gradient Descent in Backpropagation\n",
    "* Backpropagation finds the gradient at each observation, adds them up to find the total gradient:\n",
    "    * $\\triangledown E(w)=\\frac{1}{N}\\sum_{i=1}\\triangledown e_i(w)$\n",
    "    * $w(t+1)=w(t)-\\eta\\triangledown E(w(t))$\n",
    "* Instead, **update weights** at **each** observation (or after a small batch of observations):\n",
    "    * $w(t+1)=w(t)-\\eta\\triangledown e_i(w(t))$\n",
    "    ![sgd_backprop](sgd_backprop.png)\n",
    "* Chaining derivatives:\n",
    "    * In practice, a neural network function consists of many tensor operations chained together, each of which has a simple, known derivative\n",
    "        * e.g. a network $f$ composed of three tensor operations: $a$, $b$, and $c$ with weight matrices $W_1$, $W_2$, and $W_3$ $$f(W_1, W_2, W_3) = a(W_1, b(W_2, c(W_3)))$$\n",
    "    * Backpropagation starts with the final loss value and works backwards from the top layers to the bottom layers, applying the chain rule to compute the contribution that each parameter had in the loss value\n",
    "* **Symbolic Differentiation** - Implementation networks in modern frameworks like Tensorflow\n",
    "    * Given a chain of operations with a known derivative, they can compute a gradient **function** for the chain by applying the chain rule that maps network parameters values to gradient values\n",
    "    * With this function, the backward pass is reduced to a call to this gradient function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5) Feature Engineering / Overfitting & Underfitting\n",
    "* **Feature Engineering** - process of using your own knowledge about the data and about the machine-learning algorithm at hand to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data before in goes into the model\n",
    "    * e.g. develop a model that takes as input an image of a clock and can output the time of day\n",
    "        * (-) using raw pixels of image as input data is difficult ML problem using CovNN\n",
    "        * instead, if you understand the problem at high level (understand how humans read time on a clock face), then you can come up with a much better input features for ML algorithm by following the black pixels of clock hands to associate with coordinates with appropriate time of day (which makes the problem easy enough that ML is not required)\n",
    "        * before CovNN were successful on mnist dataset, solutions were typically based on hardcoded features such as the number of loops in the digit image, the height of each digit, and a histogram of pixel values, etc.\n",
    "    * Does this mean you don't need to worry about feature engineering because neural nets are capable of automatically extracting useful features from raw data? **No**, for two reasons:\n",
    "        * Good features still allow you to solve problems more elegantly while using fewer resources (e.g. ridiculous to solve reading clock face with CovNN)\n",
    "        * Good features let you solve problem with far less data. The ability of deep-learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, then the information value in their features becomes critical\n",
    "* **Overfitting/Underfitting**\n",
    "    * The fundamental issue in ML is the tension between optimization and generalization\n",
    "        * **Optimization** - the process of adjusting a model to get the best performance possible on training data (learning in ML)\n",
    "        * **Generalization** - how well the trained model performs on data it has never seen before\n",
    "    * (Underfitting) At the beginning of training, optimization and generalization are correlated: the lower the loss on training data, the lower the loss on test data\n",
    "        * There is still progress to be made\n",
    "        * The network hasn't yet modeled all relevant patterns in the training data\n",
    "    * (Overfitting) In the three examples, the performance of the model on the held-out validation data always peaked after a few epochs and then began to degrade which means the model started to **overfit** to the training data\n",
    "        * After a certain number of iterations, generalization stops improving, and validation metrics stall, and then begin to degrade\n",
    "        * Now it's beginning to learn patterns that are specific to the training data, but are misleading or irrelevant when it comes to new data\n",
    "    * Best solution to prevent a model from learning misleading or irrelevant data? **Get more training data**\n",
    "    * When that isn't possible, the next-best solution is to modulate the quantity of information that your model is allowed to store or to add constraints on what information it's allowed to store\n",
    "        * If a network can only afford to memorize a small number of the patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well\n",
    "    * Methods to reduce overfitting - **regularization**, **dropout**, and **reducing network's size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Neural Network Parameter Tuning\n",
    "* **Learning rate** - keys to configuring the learning process\n",
    "    * **Loss Function (Objective Function)** - the quantity that will be minimized during training. It represents a measure of success for the task at hand\n",
    "        * A neural network that has multiple outputs may have multiple loss functions (one per output). But, the gradient descent process must be based on a **single** scalar loss value; so, for multiloss networks, all losses are combined (via averaging) into a single scalar quantity.\n",
    "        * Choosing the right objective function for the right problem is extremely important as your network will take any shortcut to minimize the loss; so if the objective doesn't fully correlate with the success for the task, your network will end up doing the wrong things (e.g. maximizing the average well-being of all humans alive $\\rightarrow$ AI will kill all humans except a few)\n",
    "    * Types of **objective functions** for problem:\n",
    "        * **Binary Crossentropy** for two-class classification\n",
    "            * **Crossentropy** - is an quantity from the field of Information Theory that **measures the distance** between probability distributions, or in this case, between the **ground-truth distribution and your predictions**\n",
    "        * **Categorical Crossentropy** for many-class classification\n",
    "            * It measures the distance between two probability distributions\n",
    "            * e.g. between the probability distribution output by the network (46 news topics) and the true distribution of the labels\n",
    "        * **Mean-Square Error (MSE)** for regression problem\n",
    "        * **Connectionist Temporal Classification (CTC)** for sequence-learning problem\n",
    "        * Only when you're working on a truly new research problems will you have to develop your own objective functions\n",
    "        * Why aren't we using ROC/AUC metric as loss function?\n",
    "            * No easy way to turn this metric into a loss function\n",
    "            * Loss functions need to be computable given only a mini-batch (or a single data point) of data and must be differentiable (otherwise you can't use backpropagation to train the network)\n",
    "            * We use crossentropy as proxy metric for ROC/AUC where we hope that the **lower the crossentropy** gets, the **higher the ROC/AUC** will be\n",
    "        * Choosing the right last-layer activation and loss function:\n",
    "        ![last_layer](last_layer.png)\n",
    "    * **Optimizers / Optimization Methods (Variants of SGD)** - determines how the network will be updated based on the loss function. It implements a specific variant of SGD.\n",
    "        * **SGD with Momentum**\n",
    "            * **Momentum** addresses two issues with SGD: convergence speed and local minima\n",
    "            * Avoid local minimum by updating the parameter $w$ based not only on the current gradient value, but also on the previous parameter update\n",
    "        * **Adagrad**\n",
    "        * **RMSProp**\n",
    "    * **Momentum** - helps push backpropagation out of local minima (step size)\n",
    "        * adds a fraction of the previous gradient in the new update step: $w(t+1)=w(t)-\\eta\\triangledown E(w(t))+m(w(t)-w(t-1))$\n",
    "        * $m=0.9$ is standard\n",
    "        * Too high of $m$ risks overshooting minimum\n",
    "        * Too lows of $m$ risks getting stuck in local minima\n",
    "    * Custom optimizer, loss function, metrics:\n",
    "        ```python\n",
    "        from keras import optimizers\n",
    "        from keras import losses\n",
    "        from keras import metrics\n",
    "        \n",
    "        model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                      loss=losses.binary_crossentropy,\n",
    "                      metrics=[metrics.binary_accuracy])\n",
    "        ```\n",
    "* **Hidden layers**\n",
    "    * **Number of hidden layers** - how many layers to use\n",
    "    * **Number of neurons on hidden layers** - the size of representation space (more or less hidden units)\n",
    "        * Intuitively understand the dimensionality of your representation space as \"how much freedom you're allowing the network to have when learning internal representations\"\n",
    "        * Have **more** hidden units (higher-dimensional representation space) allows network to **learn more-complex representations**, but it makes the network **more computationally expensive** and may lead to **learning unwanted patterns** (patterns that will improve performance on the training data, but not on the test data)\n",
    "    * e.g. two intermediate hidden layers with 16 hidden units\n",
    "* **Initialization of weights**\n",
    "    * What happens if you set weights to 0 or weights very large?\n",
    "    * don't set weights to 0, instead set sample weights as normal centered around 0\n",
    "    * rule of thumb:\n",
    "        * sample weights from $N(0,\\sigma^2_w)$\n",
    "        * $\\sigma^2_w max_i \\Vert x_i\\Vert^2 << 1$\n",
    "* **Scaling** - normalize data before fitting data to neural network model (depending on the activation function)\n",
    "* **Epoch / Batches** - a single sweep through all of the data\n",
    "    * example: if you have 100,000 observations, and a batch size of 100. Then, each epoch will consist of 1,000 gradient descent update steps \n",
    "* **Termination** - error function is generally an **extremely** non-convex function function\n",
    "    * Lots of local minimia and flat spots\n",
    "    * Often best to terminate after a set number of iterations\n",
    "    * Also, can terminate when the gradient is small and the total error is small\n",
    "* **Activation functions**\n",
    "    1. **Sigmoid** - forces arbitrary values into [0,1] interval\n",
    "    ![sigmoid](sigmoid.png)\n",
    "        * equation: $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "        * output can be interpreted as a probability\n",
    "    2. **Tanh** - a hyperbolic tangent function that forces arbitrary values into [-1,1] interval (an activation that was popular in the early days of neural networks)\n",
    "    ![tanh_func](tanh_func.png)\n",
    "        * equation: $tanh(z)=2\\sigma(2z)-1$\n",
    "        * same shape as the sigmoid function\n",
    "        * output values centered around 0\n",
    "        * smooth or differentiable (unlike sign)\n",
    "            * $tanh(z)=2\\sigma(2z)-1 \\rightarrow tanh'(z)=1-tanh^2(z)$\n",
    "        * trains faster than sigmoid in most cases\n",
    "        * (-) requires normalization of the data\n",
    "    3. **Softmax**\n",
    "        * equation: $\\frac{e^{s_j^{(L)}}}{\\sum_{i=1}^K e^{s_i^{(L)}}}$\n",
    "        * useful activation function for output layer of a multi-class classification neural net\n",
    "    4. **Rectifer / Hard Max / Rectified Linear Unit (ReLU)** - a function that maps $\\{0,x\\}$\n",
    "    ![relu](relu.png)\n",
    "        * gradient does not vanish as $x$ gets large\n",
    "        * 0 if $x<0$, which introduces sparsity into the network\n",
    "            * zeros out negative values\n",
    "        * has faster training\n",
    "        * (+) doesn't require normalization of data\n",
    "* **Reducing Overfitting**\n",
    "    * **Reducing Network's Size** - reduce size of model\n",
    "        * Reduce **Memorization Capacity** - reduce the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer)\n",
    "        * By limiting memorization resources, it won't be able to learn as easily, thus in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets\n",
    "        * Smaller model - overfits later, perofrmance degrades slower\n",
    "        ![smaller_model](smaller_model.png)\n",
    "        * Larger model - overfits almost immediately, training loss gets to zero very quickly (more capacity allows it to quickly model the training data)\n",
    "        ![bigger_model_val_loss](bigger_model_val_loss.png)\n",
    "        ![bigger_model_train_loss](bigger_model_train_loss.png)\n",
    "    * **Regularization** - adding weight regularization\n",
    "        * **Occam's razor** - given two explanations for something, the explanation most likely to be correct is the simplest one (fewer assumptions)\n",
    "            * In the analogy for Neural Nets, the simple model is less likely to overfit than complex ones\n",
    "            * **Simple model** is a model where the distribution of parameter values has less entropy (or a model with fewer parameters)\n",
    "        * **Weight Regularization** - mitigate overfitting by putting constraints on the complexity of network by **forcing its weights** to take only **small values**, which makes the distribution of weight values more **regular**\n",
    "            * Add cost associated with having large weights to the loss function of the network\n",
    "        * **$L1$ Regularization** - the cost added is proportional to the **absolute value of weight coefficients** (the $L1$ norm of the weights)\n",
    "        * **$L2$ Regularization / Weight Decay** - the cost added is proportional to the **square of the value of weight coefficients** (the $L2$ norm of the weights)\n",
    "            * Add a $L2$-regularization term: $\\lambda\\sum_{l,i,j}(w_{ij}^{(l)})^2$ to the error function\n",
    "            * Example of Adding $L2$ weight regularization to the model:\n",
    "            ```python\n",
    "            from keras import regularizers\n",
    "            \n",
    "            model = models.Sequential()\n",
    "            # 12(0.001) means every coefficient in the weight matrix of the layer will add 0.001 * weight cofficient_value to the total loss of the network (only added at training time)\n",
    "            model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,))) \n",
    "            model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "            ```\n",
    "            ![l2_regularization](l2_regularization.png)\n",
    "        * Different weight regularizers available in Keras:\n",
    "        ```python\n",
    "        from keras import regularizers\n",
    "        \n",
    "        regularizers.l1(0.001) # l1 regularization\n",
    "        regularizers.l1_l2(l1=0.001, l2=0.001) # l1 & l2 regularization simultaneously\n",
    "        ```\n",
    "    * **Dropout** - randomly remove nodes from the network each time you run backpropagation\n",
    "        * One of the most effective and most commonly used regularization for neural networks (developed by Geoff Hinton)\n",
    "        * Randomly **dropping out** (setting to zero) a number of output features of the layer during training\n",
    "        * e.g. layer returns `[0.2, 0.5, 1.3, 0.8, 1.1]` for a given input sample during training. Then after applying dropout, this vector will have a few zero entries distributed at random: `[0, 0.5, 1.3, 0, 1.1]`\n",
    "        * **Dropout Rate** - the fraction of the features that are zeroed out (usually between 0.2 to 0.5)\n",
    "        * At test time, no units are dropped out; instead, the layer's output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time\n",
    "        * Dropout example:\n",
    "            * At training time, zero out at random a fraction of values in the matrix:\n",
    "            ```python\n",
    "            # At training time, drops out 50% of the units in the output\n",
    "            layer output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "            ```\n",
    "            * At test time, scale down the output by the dropout rate\n",
    "            ```python\n",
    "            # at test time, scale by 0.5 (since we previously dropped half the units)\n",
    "            layer_output *= 0.5\n",
    "            ```\n",
    "        * This process is normally implemented by doing both operations at training time and leaving the output unchanged at test time\n",
    "        ![dropout](dropout.png)\n",
    "        * The core idea is that **introducing noise** in the output values of a layer can **break up coincidence patterns that aren't significant**, which the network will start memorizing if no noise is present\n",
    "        ![dropout_val_loss](dropout_val_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7) Regression and Multi-Class Classification\n",
    "* Neural Network can also be used for regression and for data with multiple classes\n",
    "    * For regression, replace the output transformation with $\\theta \\rightarrow id$ and use MSE\n",
    "    * For multiple classes:\n",
    "        * The output layer will have multiple ndoes, one for each class $1,2,\\dots,K$\n",
    "        * The output activation function on the $j$-th component of the output layer is the **softmax function**: $\\frac{e^{s_j^{(L)}}}{\\sum_{i=1}^K e^{s_i^{(L)}}}$\n",
    "        * Minimize **cross-entropy**: $\\sum_{i=1}-y_i log h(x_i)$\n",
    "* Classification/Regression Examples:\n",
    "    * Classifying movie reviews as positive or negative (Binary Classification)\n",
    "    * Classifying new wires by topic (Multi-class Classification)\n",
    "    * Estimating the price of a house, given real-estate data (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Types of Network Neural Architecture\n",
    "1. **Fully Connected Neural Networks** - architecture where each layer is *fully connected* to next and has no missing edges between nodes\n",
    "![full_nn](full_nn.png)\n",
    "    * **Deep Learning Neural Network** - a buzzword referring to a neural network with more than 3 layers\n",
    "2. **Recurrent Neural Networks (RNN)** - a NN where a hidden layer feeds back into itself\n",
    "    * This allows the NN to exhibit dynamic temporal behavior\n",
    "    * RNN's provide \"internal\" memory for sequence processing\n",
    "    * **Long Short Term Memory (LSTM)** - a special kind of RNN capable of learning **long-term dependencies**\n",
    "    * They are very applicable to handwriting/speech recognition\n",
    "        * e.g. Used by Google Translate application\n",
    "        * e.g. Used to train AI on human negotiations\n",
    "![rnn](rnn.png)\n",
    "3. **Convolutional Neural Networks (CNN)** - architecture isn't fully connected and employs convolution layers\n",
    "![cnn](cnn.png)\n",
    "    * used mainly for image classification (state of the art)\n",
    "    * **Convolution Layers** - each node only \"sees\" a subset of the previous layer's nodes\n",
    "        * applies convolutions (type of filter) to each sub-image to \"look for\" certain patterns or shapes (which are learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "9) Deep Learning Achievements in 2017\n",
    "1. Text\n",
    "    * Google Translate with RNN\n",
    "    * Facebook Human Negotiation AI with RNN\n",
    "2. Voice\n",
    "    * Google DeepMind autoregressive full-convolution WaveNet (A generative model for raw audio) with PixelRNN/PixelCNN\n",
    "    * Google DeepMind Lip reading from television dataset using LSTM + CNN\n",
    "    * WashU Obama Synchronize Lip with RNN\n",
    "3. Computer Vision\n",
    "    * Google Brain enhances Google Maps with OCR (Optical Character Recognition to recognize street signs and store signs using CNN + LSTM\n",
    "    * Google Deepmind Visual Reasoning on CLEVR dataset with 95.5% accuracy using pre-train LSTM\n",
    "    * Uizard pix2code (GUI interpreted by NN into code) with 77% accuracy\n",
    "    * Google SketchRNN trained on detailed vector representations of drawings using Sequence-to-Sequence Variational Autoencoder (VAE) RNN\n",
    "    * **Generative Adversial Networks (GANs)** - competition of two networks (generator and discriminator)\n",
    "        * e.g. First network creates a picture, and the second one tries to understand whether the picture is real or generated\n",
    "        * Orange Labs France Face Aging with Conditional GANs using IMDB dataset\n",
    "        * Google Improves Professional Photos using GANs with Google Street View dataset\n",
    "        * MichU Synthesization of an image from a text description using GANs\n",
    "        * Berkeley AI Research (BAIR) Image-to-Image Translation with Conditional GANs (e.g. creating a map using a satellite image, or realistic texture of the objects using their sketch)\n",
    "        * Christopher Hesse uses UNet and PatchGAN to make nightmare cat demo\n",
    "        * Authors of Pix2Pix develops CycleGAN for transfer between different domains of images\n",
    "        * Using Adversial Autoencoder (AAE) to find new drugs to fight cancer\n",
    "        * Improvements in Adversial Attacks (tricking NN by injecting noise from recognition) using Fast Gradient Sign Method (FGSM) - important in face recognition/self-driving algorithm from being attacked\n",
    "4. **Reinforcement Learning (RL)** - learn the successful behavior of the agent in an environment that gives a reward through experience (e.g. people learning throughout their lives) - used actively in games (e.g. AlphaGO) robots, and system management (e.g. traffic)\n",
    "    * Google DeepMind Deep Q-network (DQN) plays arcade games better than humans (currently being taught to play complex games like Doom)\n",
    "        * Introduction of additional losses (auxiliary tasks), such as the prediction of a frame change (pixel control) so that the agent better understands the consequences of the actions, significantly speeds up learning\n",
    "    * OpenAI Learning Robots using RL (one-shot learning) by actively studying an agent's training by humans in a virtual environment\n",
    "        * A person shows in VR how to perform a certain task, and one demonstration is enouhg for the algorithm to learn it and then reproduce it in real conditions\n",
    "    * OpenAI/Google DeepMind Learning on Human Preferences using RL\n",
    "        * An agent has a task, and the algorithm provides two possible solutions for the human and indicates which one is better\n",
    "    * Google Deepmind Movement in Complex Environments\n",
    "        * Teaching a robot complex behavior (walk, jump, etc.) using agents (body emulators) to perform complex actions by constructing a complex environment with obstacles and with a simple reward for progress in movement\n",
    "5. Other\n",
    "    * Google Deepmind Cools Data Center (reducing energy costs) based on info on thousands of sensors predicts Power Usage Effectiveness (PUE) using NN ensemble\n",
    "    * Google Brain One Model For All Tasks (currently trained models are poorly transferred from task to task) (tensor2tensor)\n",
    "        * Train a model that performs eight tasks from different domains (text, speech, images)\n",
    "    * Facebook Learn Imagenet in one hour (using Tesla P100 - a cluster of 256 GPUs) using Gloo and Caffe2 for distributed learning\n",
    "6. News\n",
    "    * Self-driving Cars\n",
    "        * Intel MobilEye\n",
    "        * Google Waymo\n",
    "    * Healthcare\n",
    "        * Google Deepmind in Healthcare for medical diagnosis\n",
    "    * Investments\n",
    "        * China invests \\$150 Billion in AI\n",
    "        * Baudi Research employs 1,300 people\n",
    "        * Alibaba runs 100 billion samples with a trillion parameters with ease\n",
    "7. Kaggle\n",
    "    * In 2016 and 2017, Kaggle was dominated by two approaches: **gradient boosting machines** and **deep learning**\n",
    "        * Specifically, **gradient boosting** is used for problems where structured data is available (e.g. XGBoost), whereas deep learning is used for perceptual problems such as image classification (e.g. Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Keras - Deep-learning framework for Python\n",
    "* Key Features:\n",
    "    * It allows the same code to run seamlessly on CPU or GPU\n",
    "    * It has a user-friendly API that makes it easy to quickly prototype deep-learning models\n",
    "    * It has built-in support for convolutional network (for computer vision), recurrent networks (for sequence processing), and any combination of both\n",
    "    * It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. This means Keras is appropriate for building essentially any deep learning model from a generative adversarial network to a neural Turing machine\n",
    "* Keras Framework:\n",
    "![keras_framework](keras_framework.png)\n",
    "    * Keras is a model-level library, providing high-level building blocks for developing deep-learning models\n",
    "        * It doesn't handle low-level operations such as tensor manipulation and differentiation\n",
    "    * Keras relies on a specialized, well-optimized tensor library which serves as the **backend engine** of Keras\n",
    "    * There are three existing backend implementations: **TensorFlow**, **Theano**, and **Microsoft Cognitive Toolkit (CNTK)**\n",
    "    * Tensorflow on CPU/GPU:\n",
    "        * When running on **CPU**, TensorFlow is itself wrapping a low-level library for tensor operations called **Eigen**\n",
    "        * When running on **GPU**, TensorFlow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (`cuDNN`)\n",
    "    * Two ways to define a model:\n",
    "        * **Sequential class** - only for linear stacks of layers (most common architecture)\n",
    "        ```python\n",
    "        from keras import models\n",
    "        from keras import layers\n",
    "        \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "        model.add(layers.Dense(10, activation='softmax'))\n",
    "        ```\n",
    "        * **Functional API** - for directed acyclic graphs of layers, which lets you build completely arbitrary architectures\n",
    "        ```python\n",
    "        input_tensor = layers.Input(shape=(784,))\n",
    "        x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "        output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "        model = models.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "        ```\n",
    "            * manipulate the data tensors that the model processes and apply layers to the tensor as if they were functions\n",
    "    * **Validation Set** - set aside some samples from the training data to compute loss and accuracy after every epoch\n",
    "        * e.g. 25,000 training samples $\\rightarrow$ 10,000 validation samples (40% of training samples)\n",
    "        * Used to tune the configuration of the model (where the model will overfit on the validation set)\n",
    "        * We want to avoid **information leaks** - every time you tune a hyperparameter of your model based on the model's performance on the validation set, some information about the validation data leaks into model\n",
    "            * do not give the model access to **any** information on the test set (even indirectly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
