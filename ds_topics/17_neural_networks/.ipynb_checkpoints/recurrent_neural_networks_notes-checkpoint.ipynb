{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "\n",
    "* Objectives:\n",
    "    * Review of what neural networks are (Multilayer Perceptron)\n",
    "    * Simple RNN vs MLP\n",
    "    * Benefits of Intralayer Recurrent Connections\n",
    "    * Example of RNN in text data\n",
    "    * Multilayer RNNs\n",
    "    * Keras Neural Network API For RNN\n",
    "    * LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Recurrent Neural Networks Basics\n",
    "* What distinguishes neural networks is that connections between neurons can form a **directed cycle**. This gives a network the ability to maintain a state based on previous input. So it can model **temporal, sequential** behavior\n",
    "* RNN Use Cases:\n",
    "    * Pattern recognition: Handwriting, Captioning Images\n",
    "    * Sequential data: Speech Recognition, Stock price prediction, generating text, and news stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Comparison of Vanilla MLP and Vanilla RNN\n",
    "![mlp_vs_rnn](mlp_vs_rnn.png)\n",
    "* The double arrow in RNN indicates a weight in each direction (2 weights)\n",
    "* How many weights are in each architecture?\n",
    "    * Vanilla MLP weights:\n",
    "        * $W_{h\\rightarrow y} = 4$\n",
    "        * $W_{x\\rightarrow h} = 8$\n",
    "    * Vanilla RNN weights:\n",
    "        * $W_{h\\rightarrow y} = 4$\n",
    "        * $W_{h\\rightarrow h} = 16$\n",
    "        * $W_{x\\rightarrow h} = 8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Benefits of Intra-layer Recurrent connections\n",
    "* The previous state of a node in a recurrent hidden layer (`H_prev`) can affect the value of itself or other nodes in the layer in the present time (it's a directed cycle)\n",
    "* This gives the net the ability to model sequential data\n",
    "* Feedforward and backpropagation work the same way\n",
    "* Learn $W_{h\\rightarrow h}$ like all other weights. In a trained model all the weights are fixed. It's the activations of the nodes that changes with changes in sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Example: learn Dr. Seuss text with RNN\n",
    "* As the model trains, it will eventually write some new books!\n",
    "* Dictate the model architecture that works most effective\n",
    "* Looking at how the model predicts new characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Moving into Multilayer RNNs\n",
    "* **Multilayer RNNs** - multiple layers (and more nodes in each layer) allow more difficult sequences to be learned. \n",
    "    * (-) They are also harder to train\n",
    "    * (-) Exploding and vanishing gradients cause convergence problems, too\n",
    "* Vanilla RNN Example:\n",
    "![vanilla_rnn](vanilla_rnn.png)\n",
    "* Multilayer RNN Example:\n",
    "![multilayer_rnn](multilayer_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Keras For RNN\n",
    "* **Keras** - a high-level neural networks API, written in Python and capable of running on top of either Tensorflow, CNTK, or Theano\n",
    "    * Will become Tensorflow's default API\n",
    "    * Available Recurrent Layers:\n",
    "        * Recurrent\n",
    "        * SimpleRNN\n",
    "        * Long Short-Term Memory (LSTM)\n",
    "        * Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **Long Short-Term Memory (LSTM)**\n",
    "![lstm](lstm.png)\n",
    "* LSTM is an architecture proposed in 1997\n",
    "* LSTM network is well-suited when there are time lags of unknown size and bound between important events\n",
    "* LSTM practical applications:\n",
    "    * Natural language text compression\n",
    "    * Handwriting recognition\n",
    "    * Speech recognition\n",
    "    * Translation\n",
    "* Example: Use LSTM in Keras to predict stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
