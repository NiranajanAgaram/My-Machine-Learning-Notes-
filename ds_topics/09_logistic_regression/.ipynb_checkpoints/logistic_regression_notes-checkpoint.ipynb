{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "\n",
    "1) Logistic regression basics/details\n",
    "* Motivation:\n",
    "    * With linear regression, we are modeling a continuous response and finding the linear function that give the best fit\n",
    "    * But, what happens if we try linear regression for *binary* response (e.g. yes/no)\n",
    "![lin_reg_binary](linear_regression_binary.png)\n",
    "* Logistic regression properties:\n",
    "    * takes continuous input (e.g. $(-\\infty,\\infty)$\n",
    "    * produces output between 0 and 1\n",
    "    * transitions from outputting 0 to outputting 1 quickly\n",
    "    * has interpretable coefficients (similar to linear regression)\n",
    "![log_reg](logistic_regression.png)\n",
    "* **Sigmoid (logistic) function** - an S-shaped curve (sigmoid curve) that have domain of all real numbers, with return value monotonically increasing most often from 0 to 1 or alternatively from âˆ’1 to 1, depending on convention\n",
    "    * equation: $S(x)=\\frac{1}{1+e^{-x}}=\\frac{e^x}{e^x + 1}$\n",
    "    * for logistic regression: $p(y)=\\frac{1}{1+e^{-X\\beta}}$ where $p(y)$ denotes the probability of success of $y$ (mean of the response)\n",
    "    * how do we get this function?\n",
    "        * **Link function** - provides the relationship between a linear combination of our inputs ($X\\beta$) and the mean of the response ($p(y)$)\n",
    "        * equation: $\\begin{align} ln(\\frac{p(y)}{1-p(y)}) & = X\\beta \\\\\n",
    "            \\frac{p(y_i)}{1-p(y_i)} & = e^{X\\beta} \\\\\n",
    "            p(y_i) & = (1-p(y_i))e^{X\\beta} \\\\\n",
    "            p(y_i) & = e^{X\\beta}-p(y_i)e^{X\\beta} \\\\\n",
    "            p(y_i)+p(y_i)e^{X\\beta} & = e^{X\\beta} \\\\\n",
    "            p(y_i)(1+e^{X\\beta}) & = e^{X\\beta} \\\\\n",
    "            p(y_i) & = \\frac{e^{X\\beta}}{1+e^{X\\beta}} \\\\\n",
    "            p(y_i) & = \\frac{\\frac{e^{X\\beta}}{e^{X\\beta}}}{\\frac{1+e^{X\\beta}}{e^{X\\beta}}} \\\\\n",
    "            p(y_i) & = \\frac{1}{1+e^{-X\\beta}} \\\\\n",
    "            \\end{align}$\n",
    "* Linear vs Logistic regression assumptions:\n",
    "    * in linear regression, we assume: $y_i | X $~$N(X\\beta,\\sigma^2)$ (Gaussian/Normal Distribution)\n",
    "    * in logistic regression, we assume: $y_i | X$~$Bernoulli(p)$ (Bernoulli distribution)\n",
    "        * in binary classification: $y_i = $$\\begin{cases} \n",
    "        1  & \\text{if event occurs} \\\\\n",
    "        0 & \\text{if event doesn't occur}\n",
    "        \\end{cases}$\n",
    "* Estimating through **Maximum Likelihood Estimation (MLE)** - parameters of logistic regression is estimated through maximum likelihood\n",
    "    * each individual observation follows Bernoulli Distribution: $y_i | X$~$Bernoulli(p) \\rightarrow P(X) = p^x(1-p)^{1-x}$\n",
    "    * given the distribution type, the likelihood of our $\\beta$ matrix is: $L(\\beta|y)=\\prod_{i=1}^N p(y_i)^{y_i}+(1-p(y_i))^{(1-y_i)}$\n",
    "    * then convert to log likelihood: $l=\\sum_{i=1}^N y_i log(p(y_i))+(1-y_i)log(1-p(y_i))$\n",
    "    * unfortunately, no closed form solution, thus, iterative methods are typically used (e.g. **SGD**)\n",
    "        * work with the first/second derivatives to try to take clever steps towards an optimal solution with a random guess start\n",
    "* Interpreting the results\n",
    "    * example: fit logistic regression model with outcome/response as whether or not a person works (1/0 -> y/n) and only one predictor, income\n",
    "        * $p(y)=\\frac{1}{1+e^{-(\\beta_0+X_{income}\\beta_{income})}}$\n",
    "        * to interpret the coefficients, use link function: \n",
    "            * $\\begin{align} ln(\\frac{p(y)}{1-p(y)}) & = \\beta_0+X_{income}\\beta_{income} \\\\\n",
    "                \\frac{p(y)}{1-p(y)} & = e^{\\beta_0+X_{income}\\beta_{income}} \\\\\n",
    "                \\frac{p(y)}{1-p(y)} & = e^{\\beta_0}e^{X_{income}\\beta_{income}} \\\\\n",
    "                \\end{align}$\n",
    "        * $\\frac{p(y)}{1-p(y)}$ is the **odds ratio**\n",
    "        * interpretation: for a one-unit increase in $X_1$, the odds increases by $e^{\\beta_{income}}$\n",
    "            * $\\beta_1=0.00001 \\rightarrow$ one-unit increase in income, $\\$1$, causes an $e^{0.00001}$ increase in the odds of somebody working: $e^{0.00001}=1.00001$\n",
    "            * basically, for each additional dollar that a person makes, we expect a $0.001\\%$ increase in the odds that they are working\n",
    "            * for an additional $\\$1000$ dollars that a person makes, we expect $1\\%$ increase in the odds that they work: $0.0001*1000.0=1.0\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Classification metrics and the Confusion matrix\n",
    "![log_reg_matrix](log_reg_matrix.png)\n",
    "\n",
    "| True $\\rightarrow$<br/>$\\downarrow$Predicted  \t| Positive \t| Negative \t|  \t|\n",
    "|----------------------------------------------:\t|:---------------------------------------------:\t|:--------------------------------------:\t|:---------------------------------------:\t|\n",
    "| **Predicted Positive** \t| True Positive (TP) \t| False Positive (FP)<br/>(Type I error) \t| Precision /<br/>Positive Predictive Value \t|\n",
    "| **Predicted Negative** \t| False Negative (FN)<br/>(Type II error) \t| True Negative (TN) \t|  \t|\n",
    "|  \t| Recall /<br/>True Positive Rate /<br/>Sensitivity \t| False Positive Rate<br/> \t| $F_1$ Score \t|\n",
    "|  \t| False Negative Rate \t| True Negative Rate /<br/>Specificity \t| Accuracy \t|\n",
    "* Metrics:\n",
    "    * **Accuracy** - how many observations in total were labelled correctly (for positive and negative labels)?\n",
    "        * $\\frac{\\text{True Positives + True Negatives}}{\\text{True Positives + False Negatives + True Negatives + False Positive}}$\n",
    "    * **Recall / True Positive Rate / Sensitivity** - Of those observations that are truly positive, which ones were labelled positive correctly?\n",
    "        * $\\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$\n",
    "    * **True Negative Rate / Specificity** - Of those observations that are truly negative, which ones were labelled negative correctly?\n",
    "        * $\\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}$\n",
    "    * **Precision / Positive Predictive Value** - Of those observations that are labelled positive, which ones are actually positive?\n",
    "        * $\\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$\n",
    "    * **False Positive Rate** - Of those observations that are truly negatives, which ones did were labelled positive incorrectly?\n",
    "        * $\\frac{\\text{False Positives}}{\\text{True Negatives + False Positives}}$\n",
    "    * **$F_1$ Score** - considers both precision and recall, which emphasis both Type I and Type II errors\n",
    "        * $\\frac{2}{ \\frac{1}{\\text{Recall}}+\\frac{1}{\\text{Precision}} } = \\frac{2\\text{(Precision)(Recall)}}{\\text{Precision + Recall}}$\n",
    "        * $F_{\\beta} = (1+\\beta^2) \\frac{\\text{(Precision)(Recall)}}{\\beta^2\\text{* Precision + Recall}}$\n",
    "            * $\\beta > 1 \\rightarrow$ recall weights increases\n",
    "            * $\\beta < 1 \\rightarrow$ precision weights increases\n",
    "* **Receiver Operating Curve (ROC Curve)** - visualizes the performance of a given *binary classifier* by examining **True Positive Rate** changes as the **False Positive Rate** changes (or vice versa)\n",
    "<img src=ROC_curve.png text=\"ROC Curve\" width=60% />\n",
    "    * aim to choose the model that minimizes the FPR and maximizes the TPR (e.g. the model that plots closes to the top-left)\n",
    "    * compare across model curves to determine which model gives the best TPR for a given FPR\n",
    "    * **Area Under Curve (AUC)** - examine the area under the curve to try to differentiate one model from another\n",
    "        * greater area under curve is typically better, but this will also depend on what *True/False positive rate you are willing to accept*\n",
    "        * we can typically achieve *45 degree line through random guessing (50% AUC)* so we should aim to have a model better than that line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
