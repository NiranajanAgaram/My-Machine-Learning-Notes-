{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation & Sampling\n",
    "\n",
    "1) Modeling\n",
    "* **Parametric** - assumes data comes from a type of probability distribution and makes inferences about the parameters\n",
    "    * example: $Normal(\\mu,\\sigma^2), Poisson(\\lambda)$\n",
    "    * makes use of common sample statistics:\n",
    "        * $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "        * $s^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2$\n",
    "* **Non-parametric** - makes no assumption about the underlying probability distribution from which the variables arise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Inference\n",
    "* **Method of Moments (MOM)** - derives equations related to population moments (parameter estimation strategy)\n",
    "    * what is a moment? $E[X]$ - first moment, $E[X^2]$ - second moment, $E[X^3]$ - third moment\n",
    "    * example: assumes data comes from Binomial distribution\n",
    "        * $X_i$~$Binomial(N,\\underline{p})$\n",
    "        * $E[X_i]=\\bar{x}=Np$ - compute first moment from **sample data**\n",
    "        * $\\hat{p}=\\frac{\\bar{x}}{N}$ - estimate parameter $p$ based on first moment\n",
    "    * example: assumes data comes from Uniform distribution\n",
    "        * $X_i$~$Uniform(-\\theta,\\underline{\\theta})$\n",
    "        * $E[X_i]=\\frac{\\theta-\\theta}{2}=0$ - **cannot** compute based on first moment from sample data\n",
    "        * $Var(X_i) = \\frac{(2\\theta)^2}{12}=\\frac{\\theta^2}{3}$ - compute second moment from sample data\n",
    "        * $s^2 =\\frac{\\hat{\\theta}^2}{3}$ - estimate parameter $\\theta$ based on first/second moments\n",
    "* **Maximum Likelihood (MLE)** - sets values of parameters to maximize the likelihood $f(n)$ (parameter estimation strategy)\n",
    "    * what is a likelihood function?\n",
    "        * assume $x_1,x_2,\\dots,x_n$ are independent and identically distributed random variables (same probability distribution and mutually independent)\n",
    "        * a joint density function (**likelihood, $L$**):\n",
    "            * $\\begin{align} L(\\theta\\mid x_1,\\dots,x_n) \n",
    "                & = f(x_1\\mid \\theta)f(x_2\\mid \\theta)f(x_3\\mid \\theta)\\cdots f(x_n\\mid \\theta) \\\\\n",
    "                & = f(x_1,x_2,\\dots,x_n\\mid \\theta) \\\\\n",
    "                & = \\prod_{i=1}^n f(x_i\\mid \\theta) \\\\\n",
    "                \\end{align}$\n",
    "        * **log likelihood** (makes the calculus easier)\n",
    "            * $logL(\\theta\\mid x_1,\\dots,x_n) = \\sum_{i=1}^n log[f(x_i\\mid \\theta)]$\n",
    "        * maximizing the log likelihood gives us the parameter estimate\n",
    "            * $\\hat{\\theta}_{mle} = argmax_{\\theta \\in \\Theta}$ $logL(\\theta\\mid x_1,\\dots,x_n)$\n",
    "    * example: assume data comes from Binomial distribution\n",
    "        * $X_i$~$Binomial(N,\\underline{p})$\n",
    "        * $\\begin{align} \n",
    "            & f(x_i) = (_{x_i}^{N})p^{x_i}(1-p)^{N-x_i} \\\\\n",
    "            & L(p\\mid x) = \\prod_{i=1}^n (_{x_i}^{N}p^{x_i}(1-p)^{N-x_i}) \\\\\n",
    "            & logL(p\\mid x) = \\sum_{i=1}^n log (_{x_i}^{N}) + x_i log(p) + (N-x_i)log(1-p) \\\\\n",
    "            & \\frac{\\delta logL(p\\mid x)}{\\delta p} = \\sum_{i=1}^n \\bigg[\\frac{x_i}{\\hat{p}}-\\frac{N-x_i}{1-\\hat{p}} = 0\\bigg] \\\\\n",
    "            & \\hat{p} = \\frac{\\bar{x}}{N} \\\\\n",
    "            \\end{align}$\n",
    "    * example: logistic regression has observations that can be treated like Bernoulli trials with feature vector, $x_i$, and observed response $y_i$\n",
    "        * pick coefficients that maximize the joint likelihood\n",
    "        * $L(B_0,B\\mid x_1,\\dots,x_n) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}$\n",
    "        * no closed form solution\n",
    "        * use numerical method like gradient descent\n",
    "* **MOM vs MLE**:\n",
    "    * MOM is an older method and most people now prefer MLE\n",
    "    * Advantages of MOM:\n",
    "        * fairly simple\n",
    "        * useful if MLE computationally intractable\n",
    "        * can be useful as stepping stone to solving MLE by using first approximation to solutions of likelihood equations\n",
    "* **Maximum a posteriori (MAP)** - finds the mode of the posterior distribution\n",
    "    * similar to MLE, but assumes prior $g$ over $\\Theta$\n",
    "    * MLE: $\\hat{\\theta}_{mle} = argmax_{\\theta \\in \\Theta}$ $logL(\\theta\\mid x_1,\\dots,x_n)$\n",
    "    * assume a prior $g$ over $\\Theta$, and then obtain the posterior\n",
    "    * MAP: $f(\\theta\\mid x)=\\frac{f(x\\mid \\theta)g(\\theta)}{\\int_{\\upsilon \\in \\Theta}f(x\\mid \\upsilon)g(\\upsilon)d\\upsilon}$\n",
    "    * $\\hat{\\theta}_{map} = argmax_{\\theta \\in \\Theta}$ $\\frac{f(x\\mid \\theta)g(\\theta)}{\\int_{\\upsilon \\in \\Theta}f(x\\mid \\upsilon)g(\\upsilon)d\\upsilon} = argmax_{\\theta \\in \\Theta} f(x\\mid \\theta)g(\\theta)$\n",
    "* **Kernel Density Estimation (KDE)** - non-parametric way to estimate PDF of a random variable\n",
    "    * varying bandwidths for histograms in terms of bins presents a problem\n",
    "    * instead of summing rectangles, can sum using Gaussian kernels\n",
    "    * instead, use Gaussian kernels to determine which bandwidth is overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Sampling\n",
    "* Statistical Data Discovery Steps\n",
    "    1. Begin with a question or hypothesis\n",
    "    2. Design an experiment\n",
    "    3. Collect sample data\n",
    "    4. Check the results / Make inference\n",
    "    5. Repeat? Redesign?\n",
    "* Obtaining Good Data\n",
    "    * a sample should be representative of the population\n",
    "    * if a sample has poor representation, that is same as putting junk in (junk in = junk out)\n",
    "    * **random sampling** is often the best way\n",
    "* Sampling methods\n",
    "    * **simple random sampling (SRS)** - each subject has an equal chance of being in the sample\n",
    "    * **systematic sampling** - each subject is selected from ordered sampling frame where the sample is divided by population size by desired sample size (fixed periodic interval)\n",
    "    * **stratified sampling** - each subject is drawn via SRS from each group independently where each group, called strata, arises from division of the population\n",
    "    * **cluster sampling** - when mutually homogeneous yet internally heterogeneous grouping is relevant in the population. These groups are divided into clusters and then subjects are randomly selects $n$ clusters\n",
    "* Population Inference\n",
    "    * we want to know an answer about a question in the population\n",
    "    * we randomly select subjects as our sample\n",
    "    * we obtain the sample mean statistic $\\bar{x}$\n",
    "    * we make inference about the population mean $\\mu$ to obtain understanding of the population\n",
    "* **Central Limit Theorem (CLT)**\n",
    "    * given certain conditions, the **mean** of a sufficiently large number of i.i.d. random variables, will be approximately normal, regardless of the underlying distribution\n",
    "    * in other words, if we **draw enough i.i.d. samples** from the underlying distribution and average them, we should get an approximately normal distribution\n",
    "    * sample mean is normally distributed: $\\bar{X}$~$Normal(\\mu,\\frac{\\sigma^2}{n})$\n",
    "    * from any normally distributed random variable, we can derive a standard normal variable: $Z = \\frac{\\bar{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}$\n",
    "* **Confidence Intervals (CI)** - an interval estimate for the population parameter\n",
    "    * confidence level is typically stated at 95%, but it can be shown at any CI e.g. 50%, 90%, 99%\n",
    "    * confidence interval for mean is given by: $(\\bar{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\bar{x}+1.96\\frac{\\sigma}{\\sqrt{n}})$ or $\\bar{x}\\pm1.96\\frac{\\sigma}{\\sqrt{n}}$ where $\\sigma$ is the population standard deviation\n",
    "    * since we don't know $\\sigma$, estimate CI by:\n",
    "        * N>=30: $\\bar{x}\\pm1.96\\frac{s}{\\sqrt{n}}$\n",
    "        * N<30: $\\bar{x}\\pm t_{(\\frac{\\alpha}{2},n-1)}\\frac{s}{\\sqrt{n}}$\n",
    "* Resampling - drawing repeated samples from the given data\n",
    "    * **Bootstrapping** - estimates sampling distribution of an estimator by random sampling with replacement\n",
    "        * why use bootstrapping? to obtain accuracy of a sample estimate using estimated standard errors and confidence intervals that reflects the population parameter\n",
    "        * when to use bootstrapping?\n",
    "            * when theoretical distribution of statistic is complication or unknown\n",
    "            * when sample size is too small\n",
    "            * when estimating the variance of statistic using a small pilot sample for power calculations\n",
    "        * Real World: $F \\rightarrow \\textbf{X}=(x_1,x_2,\\dots,x_n) \\rightarrow \\hat{\\theta}=s(\\textbf{x})$\n",
    "            * $F$ - unknown probability model\n",
    "            * $\\textbf{X}$ - observed data\n",
    "            * $\\hat{\\theta}$ - statistic of interest\n",
    "        * Bootstrap World: $\\textbf{X} \\rightarrow \\hat{F} \\rightarrow \\textbf{x}^*=(x_1^*,x_2^*,\\dots,x_n^*) \\rightarrow \\hat{\\theta}^*=s(\\textbf{x}^*)$\n",
    "            * $\\hat{F}$ - estimated probability model\n",
    "            * $\\textbf{x}^*$ - bootstrap sample\n",
    "            * $\\hat{\\theta}^*$ - bootstrap replication\n",
    "        * steps for bootstrapping: (bootstrap variance estimation)\n",
    "        * an observed sample: $\\hat{\\theta} = t(\\hat{F}_n^1)$\n",
    "            1. draw sample with replacement $n$ times: $X_1^*,\\dots,X_n^*$~$\\hat{F}_n$\n",
    "            2. compute $\\theta$ estimates from $n$ samples: $\\hat{\\theta}^* = t(X_1^*,\\dots,X_n^*)$\n",
    "            3. repeat steps A & B, $K$ times, to get: $\\hat{\\theta}_1^*,\\dots,\\hat{\\theta}_K^*$\n",
    "            4. obtain standard errors, confidence intervals, variance: \n",
    "                * variance: $v_{boot} = \\frac{1}{K}\\sum_{k=1}^K (\\hat{\\theta}_k^*-\\frac{1}{K}\\sum_{\\tau=1}^K \\hat{\\theta}_{\\tau}^*)^2$\n",
    "                * standard error: $\\hat{se}_{boot}=\\sqrt{v_{boot}}$\n",
    "                * percentile method: $C_n = (\\theta_{\\frac{\\alpha}{2}}^*, \\theta_{\\frac{1-\\alpha}{2}}^*)$\n",
    "                * Normal interval: $\\hat{\\theta} \\pm z_{\\frac{\\alpha}{2}}\\hat{se}_{boot}$\n",
    "    * **Jackknifing** - a resampling technique that estimates the parameter for each subsample omitting the $i$-th observation\n",
    "        * useful for variance and bias estimation\n",
    "        * estimate population mean $x$:\n",
    "        * mean of sampling distribution is avg of $n$ estimates:\n",
    "        * estimate of variance from distribution:\n",
    "    * **Cross-validation** - a resampling technique where subsets of data are held out as validation set, and the rest of data is used to fit to the model and used to predict the validation set\n",
    "    * **Permutation tests** - a statistical significance test in which the test statistic under null hypothesis is obtained by calculating all possible values of test statistic under rearrangements of the labels on the observed data points\n",
    "        * is a subset of non-parametric statistics\n",
    "        * also called randomization test or re-randomization test or exact test\n",
    "        * example: two groups $A$ and $B$ whose sample means are $\\bar{x}_A$ and $\\bar{x}_B$, and that we want to test, at 5% significance level, whether they come from the same distribution\n",
    "            * the permutation test is designed to determine whether the observed difference between the sample means is large enough to reject the null hypothesis, $H_0$, that the two groups have identical probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
