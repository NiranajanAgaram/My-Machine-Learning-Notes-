{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means and Hierarchial Clustering\n",
    "\n",
    "* Objectives:\n",
    "    * Explain the difference between supervised and unsupervised learning\n",
    "    * Implement a k-means algorithm for clustering\n",
    "    * Discuss how curse of dimensionality affects clustering\n",
    "    * Choose the best k using the elbow method or silhouette scores\n",
    "    * Implement and interpret hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Supervised vs Unsupervised Learning\n",
    "* Unsupervised Learning Properties:\n",
    "    * No response variable, $y$\n",
    "        * Just based on predictors: $X_1,X_2,X_3,\\dots,X_p$\n",
    "    * No cross-validating to choose best \"model\" in usual sense\n",
    "    * No cross-validating to know how well you're doing\n",
    "    * Can be useful as **preprocessing step** for supervised learning\n",
    "    * Can be useful for better **understanding features**\n",
    "* Most Common Unsupervised Techniques\n",
    "    1. **PCA** - **low-dimensional representation** of data that explains good fraction of variance\n",
    "    ![pca](pca.png)\n",
    "    2. **Clustering** - finding **homogeneous subgroups** among data\n",
    "    ![clustering](clustering.png)\n",
    "* Unsupervised learning algorithms:\n",
    "    * K-means, Hierarchical Clustering (can be used in supervised learning)\n",
    "    * PCA (can be used in supervised learning)\n",
    "* Supervising learning algorithms:\n",
    "    * Linear, Logistic, Lasso, Ridge\n",
    "    * Decision Trees, Bagging, Random Forest, Boosting\n",
    "    * SVM\n",
    "    * kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Curse of Dimensionality Review - points are \"far away\" in high dimensions, and it's easy to overfit small datasets (sparsity of sample data points)\n",
    "* Linear models vs k-Nearest Neighbor\n",
    "    * Linear Models:\n",
    "        * Very structured\n",
    "        * Stable, but possibly inaccurate\n",
    "        * Low variance, High bias\n",
    "    * k-Nearest Neighbor:\n",
    "        * Very mildy structural\n",
    "        * Often accurate, but unstable\n",
    "        * High variance, Low bias\n",
    "    * kNN is problematic in high-dimensional spaces\n",
    "        * Usually pretty good for $p \\leq 4$ and $N$ on the large side\n",
    "        * Need to get a reasonable fraction of the $N$ values of $y_i$ to average to bring down the variance\n",
    "        * Nearest neighbors can be \"far\" in high dimensions\n",
    "            * Let's consider 10% to be a reasonable fraction of distance:\n",
    "            ![knn_radius](knn_radius.png)\n",
    "            * $p=1$ involves variable x1\n",
    "            * $p=2$ involves variable x1 and x2\n",
    "                * Radius of circle in 2 dimensions is much bigger than radius in 1 dimension\n",
    "            ![dim_radius](dim_radius.png)\n",
    "        * Hyper-cubical neighborhood about target point to capture fraction $\\mathbf{v}$ of the unit volume\n",
    "            * expected edge length: $e_p(v)=v^{\\frac{1}{p}}$\n",
    "            * sampling density proportional to: $N^{\\frac{1}{p}}$\n",
    "                * $p$ = dimensions of input space\n",
    "                * $N$ = number of points\n",
    "            ![volume_edge_length](volume_edge_length.png)\n",
    "            * edge length example:\n",
    "                * Suppose interested in a $v=10\\%$ neighborhood\n",
    "                * $p=1 \\rightarrow e_p(v)=(0.1)^{\\frac{1}{1}}=0.1$\n",
    "                * $p=10 \\rightarrow e_p(v)=(0.1)^{\\frac{1}{10}}=0.794$\n",
    "            * sampling density example:\n",
    "                * How to achieve equivalent density in higher dimensions\n",
    "                * If $N_1=100^{1}$ represents dense sample for one dimensional feature space. \n",
    "                * To achieve same density for 10 inputs, we need $N_{10}=100^{\\frac{1}{10}}$ points\n",
    "        * kNN, or **any method involving this sort of distancing**, suffers majorly from curse of dimensionality\n",
    "            * kNN with $p=10$ is \"far\" in high dimensions\n",
    "            * K-means and hierarchical clustering also has issues in high dimensions\n",
    "        * Idea of \"far\" and sparsity of points in high dimensions can be thought of in **radii approach** and **hypercube approaches**\n",
    "        * It takes up a lot of data to make up for increased in dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) K-means Clustering\n",
    "* What is clustering? Divide data into distinct **subgroups** such that observations **within each group are quite similar**\n",
    "![uncluster_to_cluster](uncluster_to_cluster.png)\n",
    "* **K-means clustering** - partitions data into $K$ subgroups while **minimizing** within-cluster variation\n",
    "    * Example: With a fixed $K=3$, assign each of $n$ data point to one of 3 clusters, such that **within-cluster variation (WCV)** is smallest\n",
    "        * There are $K^n$ possible choices\n",
    "    * Equation: $$\\text{minimize}_{C_1,\\dots,C_K}\\Big\\{\\sum_{k=1}^K \\mathbf{WCV}(C_k)\\Big\\}$$ where **WCV** for k$^{th}$ cluster is the sum of all the pairwise Euclidean distances\n",
    "        * $$\\mathbf{WCV}(C_k)=\\frac{1}{|C_k|}\\sum_{i,i'\\in C_k}\\sum_{j=1}^p (x_{ij}-x_{x'j})^2$$ where $|C_k|$ is number of observations in k$^{th}$ cluster\n",
    "    * Problem is that there are $K^n$ ways (too many!)\n",
    "    * K-means clustering algorithm:\n",
    "    ![kmeans_alg](kmeans_alg.png)\n",
    "        1. Randomly assign number, from 1 to $K$, to each data point\n",
    "        2. Repeat until cluster assignments stop changing\n",
    "            1. For each of $K$ clusters, compute cluster **centroid** by taking vector of $p$ feature means\n",
    "            2. Assign data point to cluster for which centroid is closest (Euclidean distance)\n",
    "    * Issues in K-means:\n",
    "        * Results in local optimum because of **random initialization**\n",
    "        * Solution: Try **multiple initializations** and pick one with the lowest **WCV**\n",
    "            * Also consider **K-means++** method that allows for smarter initializations\n",
    "    * Choosing $K$:\n",
    "        * Issues with choosing $K$:\n",
    "            * No easy answer\n",
    "            * May just want $K$ similar groups\n",
    "            * But more often, want something **interpretable** that exposes some interesting aspect of data\n",
    "                * Presence/absence of natural distinct groups\n",
    "                * Descriptive statistics about groups\n",
    "            * Example: Are there certain segments of my market that tend to be alike?\n",
    "                * e.g. middle-aged living in suburbs who log-in infrequently\n",
    "        * Methods for choosing $K$:\n",
    "            1. **\"Elbow\" method** - choose a number of clusters so that adding another cluster doesn't minimize **WCV** much more\n",
    "                ![elbow_method](elbow_method.png)\n",
    "                * **Within Cluster Point Scatter (WCPS)** - a natural loss function is the sum pairwise distances of the points within each cluster, summed over all clusters. In particular, we could specify $d(x_i,x_{i'})$\n",
    "                * WCPS Equation:\n",
    "                    * $$W(C)=\\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\\sum_{C(i')=k}d(x_i,x_{i'})$$\n",
    "                    * Let $d_{ii'}=d(x_i,x_{i'})$:\n",
    "                    * **Total Point Scatter**: $$\\begin{align}\n",
    "                    T & = \\frac{1}{2}\\sum_{i=1}^N\\sum_{i'=1}^N d_{ii'} \\\\\n",
    "                    & = \\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\\Big( \\sum_{C(i')=k}d_{ii'}+\\sum_{C(i')\\neq k}d_{ii'} \\Big) \\\\\n",
    "                    & = W(C)+B(C)\n",
    "                    \\end{align}$$\n",
    "                    * **Between Cluster Point Scatter**: $$B(C)=\\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\\sum_{C(i')\\neq k}d_{ii'}$$\n",
    "                * Alternative Form: \n",
    "                    * $$\\begin{align}\n",
    "                    W(C) & = \\frac{1}{2}\\sum_{k=1}^K\\sum_{C(i)=k}\\sum_{C(i')=k}\\Vert x_i-x_{i'} \\Vert^2 \\\\\n",
    "                    & = \\sum_{k=1}^K N_k \\sum_{C(i)=k}\\Vert x_i-\\bar{x}_k \\Vert^2\n",
    "                    \\end{align}$$\n",
    "                        * $\\bar{x}_k=(\\bar{x}_{1k},\\dots,\\bar{x}_{pk})$ is mean vector associated with k$^{th}$ cluster\n",
    "                        * $N_k=\\sum_{i=1}^N I(C(i)=k)$\n",
    "            2. **GAP statistic** - compare within-cluster scatter, $W_1,\\dots,W_k$, to uniformly distributed rectangle containing data. Find largest gap.\n",
    "                ![gap_statistic](gap_statistic.png)\n",
    "                * For each $K$, compare $W_k$ (within-cluster sum of squares) with that of randomly generated \"reference distributions\"\n",
    "                    * Generate $B$ distributions: $Gap(K)=\\frac{1}{B}\\sum_{b=1}^B log W_{Kb}-log W_K$\n",
    "                    * Choose smallest $K$ such that $Gap(K) \\geq Gap(K+1)-s_{N+1}$ where $s_K$ is the standard error of $Gap(K)$\n",
    "                * GAP Statistics Steps:\n",
    "                    1. Observed vs. Expected value of $log(W_k)$ over 20 simulations from uniform data\n",
    "                    2. Translate curves so that $log(W_k)=0$ for $k=1$\n",
    "                    3. GAP statistic $K^*$ is smallest $K$ producing gap within one standard deviation of gap at $K+1$\n",
    "                * Arguably best method!\n",
    "                * Notice as number of clusters increase, within cluster scatter decreases\n",
    "                * What happens when number of clusters is number of points?\n",
    "            3. **Silhouette Coefficient** - general method for interpreting and validating clusters of data\n",
    "                * For each observation $i$:\n",
    "                    * $a_i$ = average dissimilarity of $i$ with all other data points **within same cluster**\n",
    "                        * a measure of how well $i$ is assigned to the cluster\n",
    "                        * the **smaller** $a_i$ is, the better the assignment\n",
    "                    * $b_i$ = lowest average dissimilarity of $i$ to any other cluster, of which $i$ is not member\n",
    "                        * other cluster can be thought of as a \"neighboring cluster\"\n",
    "                * Equation:\n",
    "                    * $S_i = \\frac{b_i-a_i}{max(a_i, b_i)}$\n",
    "                    * range: $-1<S_i<1$\n",
    "                * Want $a_i$ small, $b_i$ large $\\rightarrow$ want silhouette, $S_i$, large\n",
    "                    * near 1 $\\rightarrow$ dense and well separated\n",
    "                    * near 0 $\\rightarrow$ overlapping clusters; could well belong to another cluster\n",
    "                    * near -1 $\\rightarrow$ misclustered\n",
    "                * Example: 38 data points, 3 clusters\n",
    "                ![silhouette](silhouette.png)\n",
    "                    * 1st cluster - 8 data points and avg silhouette of 0.78\n",
    "                    * 2nd cluster - 19 data points and avg silhouette of 0.64\n",
    "                    * 3rd cluster - 11 data points and avg silhouette of 0.51\n",
    "                    * Overall avg silhouette of **0.63**\n",
    "                * Guidelines for Overall Avg Silhouette:\n",
    "\n",
    "|   Range  |       Interpretation      |\n",
    "|:--------:|:-------------------------:|\n",
    "| 0.71-1.0 | Strong structure found    |\n",
    "| 0.51-0.7 | Reasonable structure      |\n",
    "| 0.26-0.5 | Structure weak/artificial |\n",
    "| < 0.25   | No substantial structure  |               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Hierarchical Clustering** - method of cluster analysis which seeks to build a hierarchy of clusters\n",
    "* Hierarchical Clustering Algorithm:\n",
    "    ![hierarchical_clustering](hierarchical_clustering.png)\n",
    "    1. Each point as its own cluster\n",
    "    2. Merge closest clusters\n",
    "    3. End when all points in single cluster\n",
    "* Still need to account for \"distance\" between clusters\n",
    "* **Height of Fusion** - indicates the proximity of clusters\n",
    "    * Example (from above): A & C are close (height: 1.2)\n",
    "    * Red and Green clusters are not close (height: ~4.1)\n",
    "* Varying $K$\n",
    "![varying_k](varying_k.png)\n",
    "    * In contrast to K-means, it is not necessarily to choose $K$ from the start\n",
    "        * Depending on where the cut is precisely, there could be **1 to $n$ clusters**\n",
    "    * Choosing $K$: Can again use Elbow Method, GAP Statistic, Silhouette Coefficient\n",
    "        * But, notice the **height of dendrogram** give a **sense of separation of clusters** depending on the cut\n",
    "* Distance Between Two Clusters:\n",
    "\n",
    "|  Linkage |                                                                                                                                                                 Description                                                                                                                                                                |                          Usage                          |\n",
    "|:--------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------:|\n",
    "| Complete | Maximal intercluster dissimilarity. Compute all pairwise dissimilarities<br/> between the observations in cluster A and the observations in cluster B, and<br/> record the **largest** of these dissimilarities                                                                                                                            | - More commonly used<br/>- Tends to be balanced         |\n",
    "| Single   | Minimal intercluster dissimilarity. Compute all pairwise dissimilarities<br/> between the observations in cluster A and the observations in cluster B, and<br/> record the **smallest** of these dissimilarities. Single linkage can<br/> result in extended, trailing clusters in which single observations are<br/> fused one-at-a-time. | - Less commonly used<br/>- Extended trailing clusters   |\n",
    "| Average  | Mean intercluster dissimilarity. Compute all pairwise dissimilarities between<br/> the observations in cluster A and the observations in cluster B,<br/> and record the **average** of these dissimilarities                                                                                                                               | - More commonly used<br/>- Tends to be balanced         |\n",
    "| Centroid | Dissimilarity between the centroid for cluster A (a mean vector of length $p$)<br/> and the centroid for cluster B. Centroid linkage can result in undesirable<br/> **inversions**.                                                                                                                                                        | - Less commonly used<br/>- Although popular in Genomics |\n",
    "![linkages](linkages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Clustering Intuitions\n",
    "* Is it important to standardize features?\n",
    "    * Yes, most likely\n",
    "    * How do we deal with categorial features?\n",
    "* Outliers can be problematic\n",
    "    * Especially using squared Euclidean as a distance metric\n",
    "    * What if small subset of observations is very different from all others?\n",
    "        * K-means and hierarchical clustering **forces** every data point into clusters, potentially **distorting** clusters\n",
    "        * Mixture models (e.g. **soft clustering**) are attractive alternative as they accommodate outliers\n",
    "* Generally **not** very robust\n",
    "    * Can test by clustering subsets of data\n",
    "* Clustering is a simple, elegant method, but can be problematic in a lot of ways\n",
    "    * Only intended for **quantitative** features (think centroid calculation for categorical data) and squared **Euclidean** distance (which is not robust to outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Alternative Clustering Methods\n",
    "* **K-medoids** - minimizes pairwise dissimilarities and chooses one of the data points as center, or \"medoid\"\n",
    "    ![kmedoids](https://i.stack.imgur.com/wBlqF.png)\n",
    "    * (-) Computationally more intensive (large proximity matrix computation)\n",
    "    * (+) Handles **categorical features** more naturally (though still must define distance metric for mixed data carefully), and **more robust to outliers**\n",
    "* **DBSCAN** - density-based spatial clustering of applications with noise\n",
    "    ![dbscan](https://camo.githubusercontent.com/08d18a2ecf4bc19cc496c73867f540e4e54334c5/68747470733a2f2f64337676366c703535716a6171632e636c6f756466726f6e742e6e65742f6974656d732f30483068336932773176307a33653136306c31462f64627363616e2d7072696e6369706c652e706e673f582d436c6f75644170702d56697369746f722d49643d31303934343231)\n",
    "    * Two parameters (number of clusters not specified)\n",
    "        * $\\epsilon$ - distance between points for them to be connected\n",
    "        * $minPts$ - number of connected points for a point to be a \"core\" point\n",
    "        * A cluster is all connected core points, plus others within $\\epsilon$ of one of those. Other points are noise.\n",
    "    * very popular clustering algorithm\n",
    "    * groups together close together points, and marks low density regions as outliers\n",
    "    * Distribution-based clustering:\n",
    "    ![dbscan_2](dbscan_2.png)\n",
    "        * Assume clusters follow some (generally gaussian) distribution\n",
    "        * Find distributions with the **maximum likelihood** to produce this result\n",
    "        * Except, don't know which point is part of which cluster, so need to add some hidden variables and follow an **expectation-maximization (EM)** algorithm\n",
    "    * Example: Cluster similar shoppers to show items and ads they'll like\n",
    "\n",
    "| Shopper | Computers | Keyboards | Peanut Butter | Oreos |\n",
    "|:-------:|:---------:|:---------:|:-------------:|:-----:|\n",
    "| Aditi   | 1         | 2         | 0             | 0     |\n",
    "| Rohit   | 0         | 0         | 30            | 50    |\n",
    "| Aaron   | 0         | 0         | 50            | 50    |\n",
    "| Jia     | 0         | 0         | 0             | 1     |\n",
    "| Jack    | 2         | 4         | 10            | 20    |\n",
    "| William | 3         | 6         | 0             | 0     |\n",
    "| ...     | ...       | ...       | ...           | ...   |\n",
    "* Who is Aditi most \"similar\" to in Euclidean distance?\n",
    "* Who is Jack most \"similar\" to? \n",
    "    * Do we care more about selling a jar of Peanut Butter or a Computer?\n",
    "* What can we do so that distance isn't just based on Peanut Butter and Oreos?\n",
    "    * But, William is still far from Aditi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
