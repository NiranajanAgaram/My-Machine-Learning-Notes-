{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP)\n",
    "\n",
    "* Objectives:\n",
    "    * What are the stop words and where do we get some?\n",
    "    * What does it mean to stem or lemmatize your text?\n",
    "    * What is an $n$-gram and why might they help?\n",
    "    * Term frequency is useful but what are some of the issues?\n",
    "    * Processing text via vectorization and TF-IDF\n",
    "    * Understanding document similarity\n",
    "    * Knowing basic usage examples of NLP\n",
    "    * Explaining word2vec algorithm\n",
    "    * Applying Naive Bayes model to text data\n",
    "    * Understanding the Laplace Smoothing technique\n",
    "    * Knowing example of multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What is NLP?\n",
    "* Use Cases:\n",
    "    * Conversational Agents:\n",
    "        * Siri, Cortana, Google Now, Alexa\n",
    "        * Talking to your car\n",
    "        * Communicating with robots\n",
    "    * Machine Translation:\n",
    "        * Google Translate\n",
    "        * Google's Neural Machine Translation\n",
    "    * Speech Recognition, Speech Synthesis\n",
    "    * Lexical Semantics, Sentiment Analysis\n",
    "    * Dialogue Systems, Question Answering\n",
    "* What are the challenges with NLP?\n",
    "    * Ambiguity\n",
    "        * \"Court to Try Shooting Defendant\"\n",
    "        * \"Hospitals are sued by seven foot doctors\"\n",
    "        * What does it mean when we say: \"I made her duck\"\n",
    "            * I cooked waterfowl for her\n",
    "            * I cooked waterfowl belonging to her\n",
    "            * I created the paper mache duck she owns\n",
    "            * I caused her to quickly lower her head or body\n",
    "            * I waved my magic wand and turned her into undifferentiated waterfowl\n",
    "        * **Word Sense Disambiguation** - the problem of determining which sense was meant by a specific word\n",
    "    * Part of speech tagging\n",
    "    * Syntactic disambiguation - \"I made her duck\" example\n",
    "* Knowledge of language:\n",
    "    * Phonetics & Phonology - linguistic sounds\n",
    "    * Morphology - meaningful components of words\n",
    "    * Semantics - meaning of word\n",
    "    * Pragmatics - meaning wrt goals and intentions\n",
    "    * Discourse - linguistic units larger than a single utterance\n",
    "* NLP vocabulary\n",
    "    * **Corpus** - a collection of documents\n",
    "    * **Tokens** - each document has a collection of tokens\n",
    "    * Each token is a \"word\"\n",
    "        * There are cases where the \"word\" is \"the\" or \"a\"\n",
    "        * There are also different versions of a word\n",
    "        * e.g. \"Banks working with that bank on the east bank were banking on a banker's strike\"\n",
    "    * **$n$-grams** - a block of $n$ words\n",
    "        * \"little\" - unigram\n",
    "        * \"little boy\" - bigram\n",
    "        * \"little boy blue\" - trigram\n",
    "        * \"little boy blue and the man on the moon\" - 9-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) \"Parameters\" to Tune in NLP\n",
    "* **Stopwords** - words which ahve no real meaning but make the sentence grammatically correct\n",
    "    * e.g. \"I\", \"me\", \"my\", \"you\", etc.\n",
    "    * scikit-learn's contains 318 words for the English set of stop words\n",
    "* **Sentence Segmentation**\n",
    "    * **Bag of Words**\n",
    "* **$N$-grams**\n",
    "* **Normalization**\n",
    "* Specific abberviations and meaningful word coupling e.g. New York, POTUS, LDAP\n",
    "* **Stemming** - the process of reducing words to their stem, base or root form\n",
    "* **Lemmatization** - removes inflectional endings only and returns word to the based or dictionary form\n",
    "    * e.g. (car, cars, car's, cars') $\\rightarrow$ car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Text Processing Steps\n",
    "1. Lower all of your text (depending on the parts of speech (POS))\n",
    "2. Strip out miscellaneous spacing and punctuation\n",
    "3. Remove stop words (but be careful as they may be domain or use-case specific)\n",
    "4. Stem/Lemmatize the text\n",
    "5. Part-of-Speech Tagging\n",
    "6. Expand feature matrix with $N$-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Making text machine consumable (Term-Frequency Matrix / TF-IDF)\n",
    "* **Term-Frequency Matrix** - converts the corpus of text data into some form of numeric matrix representation\n",
    "    * Each column of the matrix is a word\n",
    "    * Each row is a document\n",
    "    * Each cell therein contains the count of that word in a document\n",
    "    * e.g. \"oh,the,thinks,you,can,think,if,only,you,try\"\n",
    "        * \"if,you,try,you,can,think,up,a,guff,going,by\"\n",
    "        * \"and,what,would,you,do,if,you,meet,a,jaboo\"\n",
    "\n",
    "| jaboo | if | try | can | you | guff | think | going | would | up | only | thinks | met | oh | by | what | do |\n",
    "|:-----:|:--:|:---:|:---:|:---:|:----:|:-----:|:-----:|:-----:|:--:|:----:|:------:|:---:|:--:|:--:|:----:|:--:|\n",
    "| 0     | 1  | 1   | 1   | 2   | 0    | 1     | 0     | 0     | 0  | 1    | 1      | 0   | 1  | 0  | 0    | 0  |\n",
    "| 0     | 1  | 1   | 1   | 2   | 1    | 1     | 1     | 0     | 1  | 0    | 0      | 0   | 0  | 1  | 0    | 0  |\n",
    "| 1     | 1  | 0   | 0   | 2   | 0    | 0     | 0     | 1     | 0  | 0    | 0      | 1   | 0  | 0  | 1    | 1  |\n",
    "\n",
    "* Stopwords removed: \"the\", \"a\", \"and\"\n",
    "* What are problems with this approach?\n",
    "    * Documents may have different lengths\n",
    "    * (-) May have over or underrepresentation issues due to terms with high frequency\n",
    "* Solutions:\n",
    "    * (Okay Solution) Normalize the term counts by the length of a document which would alleviate some problems (e.g. **L2 Normalization** in sklearn): $tf(t,d)=\\frac{f_{t,d}}{\\sqrt{\\sum_{i\\in V}(f_{i,d})^2}}$\n",
    "    * (Better Solution) Have the value associated with a document-term be a measure of the importance in relation to the rest of the corpus (**TF-IDF**)\n",
    "* **Term Frequency and Inverse Document Frequency (TF-IDF)** - having a value associated with document-term be a measure of the importance in relation to the rest of the corpus\n",
    "    * There two parts to TF-IDF that we need to answer in order to calculate it:\n",
    "        1. How apparently important was this token in this document?\n",
    "            * **Term-Frequency (TF)** - $|t|$ in this document\n",
    "        2. How common is this term in general?\n",
    "            * **Document Frequency** - $\\frac{|\\text{documents containing }t|}{|\\text{documents}|}$\n",
    "            * **Inverse Document Frequency (IDF)** - inverse the previous term and log it (if you want). Then, add 1 to the denominator to avoid divide-by-zero scenario: $IDF(t,D) = log\\big(\\frac{|\\text{documents}|}{1+|\\text{documents containing }t|}\\big)$\n",
    "    * TF-IDF is calculated by multiplying the **Term-Frequency** by **Inverse-Document-Frequency**\n",
    "        * Log scale is used so terms that occurs 10x times more than another are not 10x times more important\n",
    "        * The \"1\" term on the bottom of the equation is known as a **smoothing constant** and is there to ensure that we don't have a zero in the denominator\n",
    "    * Adjusting thresholds for inclusion/exclusion (TfidfVectorizer):\n",
    "        * **max_df** - specifies words which should be excluded due to appearing in **more** than a given number of documents. Can either be absolute counts or a number between 0 and 1 indicating a proportion.\n",
    "        * **min_df** - specifies words which should be excluded due to appearing in **less** than a given number of documents. Can either be absolute counts or a number between 0 and 1 indicating a proportion.\n",
    "        * **max_features** - specifies the number of features to include in the resulting matrix. If not None, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus.\n",
    "* Now that we have a matrix representation of the corpus, how should we go about comparing documents to identify those which are most similar to one another? Using distance metrics!\n",
    "    * **Cosine Similarity** - $dist(a,b) =\\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2}\\sqrt{\\sum_{i=1}^n b_i^2}}$\n",
    "    * **Euclidean Distance** - $dist(a,b) = \\sqrt{\\sum_{i=1}^n (a_i-b_i)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Great NLP tools - spaCy/word2vec\n",
    "* **spaCy** - leveraging the power of **Cython**, it is the fastest syntactic parser in the world and is capable of parsing over 13,000 words per minute\n",
    "    * industrial-strength NLP tool in Python\n",
    "    * can perform:\n",
    "        1. lemmatization\n",
    "        2. part-of-speech tagging\n",
    "        3. sentence extraction\n",
    "        4. entity extraction\n",
    "    * excels at large-scale information extraction tasks\n",
    "* **word2vec** - computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model.\n",
    "    * group of related models that are used to produce word embeddings\n",
    "    * typically they are two-layer neural networks that are trained to reconstruct linguistic contexts of word\n",
    "    * input is a large corpus of text and output is a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space\n",
    "    * word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Naive Bayes Classifier / Laplace Smoothing\n",
    "* **Naive Bayes Classifier**\n",
    "    * **Bayes' Theorem** - allows switch of the events $X$ and $Y$ in a $P(X|Y)$ situation, provided we know certain other probabilities\n",
    "        * $\\begin{align}\n",
    "        P(A\\cap B) = & P(B\\cap A) \\\\\n",
    "        P(A|B)P(B) = & P(B|A)P(A) \\\\\n",
    "        P(A|B)P(B) = & P(A\\cap B) \\\\\n",
    "        P(A|B) = & \\frac{P(B|A)P(A)}{P(B)} \\\\\n",
    "        Posterior = & \\frac{\\text{(Prior)(Likelihood)}}{\\text{Evidence Normalizing Constant}}\n",
    "        \\end{align}$\n",
    "    * Naive Bayes classifiers are considered naive because we assume that **all words in the string are assumed independent from one another**\n",
    "    * While this clearly isn't true, they still perform remarkably well and historically were deployed as spam classifiers in the 90's. Naive Bayes handles cases where our number of features **vastly outnumber** our data points (e.g. more words than documents). These methods are also computationally efficient in that we just have to calculate sums.\n",
    "    * Example: arbitrary document, $w_1,\\dots,w_n$, and we would like to calculate the probability that it was from the sports section\n",
    "        * $P(y_c|w_1,\\dots,w_n)=P(y_c)\\prod_{i=1}^n P(w_i|y_c)$ where $P(y_c)=\\frac{\\sum_{i=1}y==y_c}{|D|}$\n",
    "* **Laplace Smoothing** - serves to remove the possibility of having a 0 in the denominator or the numerator, both of which would break our calculation, by adding 1 to the numerator and denominator\n",
    "    * $\\begin{align} P(w_i|y_c)\n",
    "    = & \\frac{count(w_{D,i}|y_c)+1}{\\sum_{w\\in V}[count(w|y_c)+1]} \\\\\n",
    "    = & \\frac{count(w_{D,i}|y_c)+1}{\\sum_{w\\in V}[count(w|y_c)+|V|]}\n",
    "    \\end{align}$\n",
    "    * $\\begin{align} \n",
    "    P(y_c|w_{d,1},\\dots,w_{d,n}) = & P(y_c)\\prod_{i=1}^n P(w_{d,i}|y_c) \\\\\n",
    "    log(P(y_c|w_{d,1},\\dots,w_{d,n})) = & log(P(y_c))+\\sum_{i=1}^n log(P(w_{d,i}|y_c)) \\\\\n",
    "    \\end{align}$\n",
    "* Naive Bayes Classifier Intuition\n",
    "    * For unknown words, use Laplace Smoothing\n",
    "    * Useful for online learning\n",
    "    * Load of extensions and variants out there\n",
    "* Pros of Naive Bayes:\n",
    "    * (+) Good with wide data ($p>>n$)\n",
    "    * (+) Good if $n$ is small or $n$ is quite big\n",
    "    * (+) Fast to train\n",
    "    * (+) Good at online learning, streaming data\n",
    "    * (+) Simple to implement, not necessarily memory-bound (DB implementations)\n",
    "    * (+) Multi-class classification possible\n",
    "* Cons of Naive Bayes:\n",
    "    * (-) Naive assumption means correlated features are not actually treated right\n",
    "    * (-) Sometimes outperformed by other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
