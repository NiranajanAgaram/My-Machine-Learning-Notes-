{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrames\n",
    "\n",
    "* Objectives:\n",
    "    * Understand the benefits of Spark DataFrames over traditional RDDs\n",
    "    * Know how to instantiate and interact with a Spark DataFrame\n",
    "    * Know how to register a Spark DataFrame in order to be able to use SQL queries on the data\n",
    "    * Know how to spin up a spark cluster on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why use DataFrames?\n",
    "* Benefits of DataFrames:\n",
    "    * (+) They provide an abstraction that simplifies working with structured datasets\n",
    "    * (+) They can read and write data in a variety of structured formats\n",
    "    * (+) They let you query the data using SQL\n",
    "    * (+) They are much faster than traditional RDDs\n",
    "* What if we have datasets with multiple different data types?\n",
    "    * Example JSON dataset:\n",
    "    ```python\n",
    "    rdd = sc.parallelize(['{\"name\": \"Amy\", \"age\": 18, \"hobby\": \"drinking\"}',\n",
    "    '{\"name\": \"Greg\", \"age\": 60, \"hobby\": \"fishing\"}',\n",
    "    '{\"name\": \"Susan\", \"age\": 30}'])\n",
    "    ```\n",
    "    * Get older than 18, with hobbies\n",
    "        * Using RDD method:\n",
    "        ```python\n",
    "        rdd.filter(lambda d: d['age'] > 18) \\\n",
    "            .filter(lambda d: 'hobby' in d.keys()) \\\n",
    "            .map(lambda d: d['name'])\n",
    "        ```\n",
    "        * Using DataFrame method: (Easier to perform operations)\n",
    "        ```python\n",
    "        df = hive_context.jsonRDD(rdd)\n",
    "        df.registerTempTable(\"df\")\n",
    "        hive_context.sql(\"\"\"SELECT name\n",
    "                            FROM df\n",
    "                            WHERE age > 18\n",
    "                            AND hobby IS NOT NULL\n",
    "                            \"\"\")\n",
    "        # OR\n",
    "        df.filter((col(\"age\") > 18) & (col(\"hobby\").isNotNull()))\n",
    "        # OR\n",
    "        df.filter((df.age > 18) & (df.hobby.isNotNull()))\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) DataFrames Basics\n",
    "* DataFrame Structure\n",
    "    * Spark DataFrames are basically just RDDs with schema (data type structure)\n",
    "    * A DataFrame contains an RDD of **Row** objects, each representing a record. (A DataFrame is not technically an RDD, but we can effectively treat it as such.)\n",
    "    * A DataFrame knows the schema of its rows, which means it can store and process data in a more efficient manner\n",
    "* Why is Schema Important?\n",
    "    * Allows for logical optimizations (e.g. predicate pushdown)\n",
    "    * Allows for compilation to bytecode (python specific)\n",
    "    ![dataframes_py_vs_scala](dataframes_py_vs_scala.png)\n",
    "* What is the difference between HiveContext vs SQLContext?\n",
    "    * HiveContext offers more functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3) DataFrame-Based API\n",
    "* Why is MLlib **switching** to the DataFrame-based API for MLlib?\n",
    "    * (+) DataFrames provide a more user-friendly API than RDDs\n",
    "    * (+) Spark Datasources\n",
    "    * (+) SQL/DataFrame queries\n",
    "    * (+) Tungsten and Catalyst optimizations\n",
    "    * (+) Uniform APIs across languages\n",
    "    * (+) DataFrames facilitates ML Pipelines, particularly feature transformations\n",
    "* **DataFrame-Based** Machine Learning Libraries\n",
    "    * Basic Statistics:\n",
    "        * Correlations\n",
    "        * Hypothesis Testing\n",
    "    * Pipelines:\n",
    "        * Transformers\n",
    "        * Estimators\n",
    "        * Parameters\n",
    "        * Saving and Loading Pipelines\n",
    "    * Extracting, Transforming, and Selecting Features:\n",
    "        * Feature Extractors - Extracting features from \"raw\" data\n",
    "            * TF-IDF\n",
    "            * Word2Vec\n",
    "            * CountVectorizer\n",
    "        * Feature Transformers - Scaling, converting, or modifying features\n",
    "            * Tokenizer\n",
    "            * StopWordsRemover\n",
    "            * $n$-gram\n",
    "            * Binarizer\n",
    "            * PCA\n",
    "            * PolynomialExpansion\n",
    "            * Discrete Cosine Transform (DCT)\n",
    "            * StringIndexer\n",
    "            * IndexToString\n",
    "            * OneHotEncoder\n",
    "            * VectorIndexer\n",
    "            * Interaction\n",
    "            * Normalizer\n",
    "            * StandardScaler\n",
    "            * MinMaxScaler\n",
    "            * Bucketizer\n",
    "            * ElementwiseProduct\n",
    "            * SQLTransformer\n",
    "            * VectorAssembler\n",
    "            * QuantileDiscretizer\n",
    "            * Imputer\n",
    "        * Feature Selectors - Selecting a subset from a larger set of features\n",
    "            * VectorSlicer\n",
    "            * RFormula\n",
    "            * ChiSqSelector\n",
    "        * Locality Sensitive Hashing - This class of algorithms combines aspects of feature transformation with other algorithms.\n",
    "            * LSH Operations\n",
    "                * Feature Transformation\n",
    "                * Approximate Similarity Join\n",
    "                * Approximate Nearest Neighbor Search\n",
    "            * LSH Algorithms\n",
    "                * Bucketed Random Projection for Euclidean Distance\n",
    "                * MinHash for Jaccard Distance\n",
    "    * Classification/Regression:\n",
    "        * Classification:\n",
    "            * Logistic Regression\n",
    "                * Binomial Logistic Regression\n",
    "                * Multinomail Logistic Regression\n",
    "            * One-vs-Rest Classifier (aka One-vs-All)\n",
    "            * Naive Bayes\n",
    "        * Regression:\n",
    "            * Generalization Linear Regression (GLM)\n",
    "            * Survival Regression\n",
    "            * Isotonic Regression\n",
    "        * Both Classification/Regression:\n",
    "            * Support Vector Machines (SVM)\n",
    "            * Decision Trees/Random Forests\n",
    "            * Gradient Boosted Trees\n",
    "        * Multilayer Perceptron (e.g. Neural Network)\n",
    "    * Clustering\n",
    "        * K-means Clustering\n",
    "        * Gaussian Mixture Model (GMM)\n",
    "        * Power Iteration Clustering (PIC)\n",
    "        * Latent Dirichlet Allocation (LDA)\n",
    "        * Bisecting K-means\n",
    "        * Streaming K-means\n",
    "    * Decomposition\n",
    "        * Singular Value Decomposition (SVD)\n",
    "        * Principal Component Analysis (PCA)\n",
    "        * Non-matrix Factorization (NMF)\n",
    "    * Recommenders/Collaborative Filtering\n",
    "        * Alternative Least Squares (ALS)\n",
    "    * Optimization:\n",
    "        * Stochastic Gradient Descent (SGD)\n",
    "        * Limit-Memory BFGS (L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4) **Datasets** - map over the values with a user defined function and convert it into a new arbitrary case class object\n",
    "* Spark allows JVM users to create their own objects (via case classes or java beans) and manipulate them using function programming concepts\n",
    "* Can automatically turn it back into a DataFrame and we can manipulate it further using the hundreds of functions\n",
    "```scala\n",
    "case class ValueAndDouble(value:Long, valueDoubled:Long)\n",
    "spark.range(2000)\n",
    "    .map(value => ValueAndDouble(value, value * 2))\n",
    "    .filter(vAndD => vAndD.valueDoubled % 2 == 0)\n",
    "    .where(\"value % 3 = 0\")\n",
    "    .count()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
