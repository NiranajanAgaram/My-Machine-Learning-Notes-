{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark Framework\n",
    "\n",
    "* Objectives:\n",
    "    * Describe the pros/cons of Spark compared to Hadoop MapReduce\n",
    "    * Define what an RDD is, by its properties\n",
    "    * Explain the difference between transformations and actions on an RDD\n",
    "    * Implement the different transformations through use cases\n",
    "    * Explain what persisting/caching an RDD means, and situations where this is useful\n",
    "    * Know how many partitions we should have for a Spark RDD\n",
    "    * Know how to join two Spark RDDs\n",
    "    * Define what accumulators and broadcast variables are, and use cases for each\n",
    "    * Describe what type of input format MLlib machine learning algorithms typically expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why use Spark?\n",
    "* (+) Distributed parallel computing (Open-source cluster computing framework)\n",
    "    * Processes masssive data sets\n",
    "    * Highly efficient distributed operations\n",
    "    * More use cases than just MapReduce (\"Successor\" to Hadoop MapReduce)\n",
    "    * Scala/Java/Python/R/SQL supported natively\n",
    "        * Python API is very thorough, but has several drawbacks\n",
    "            * (-) Python is duck-typed, so Spark has to work out what data type everything is as it is passed on to the JVM\n",
    "            * (-) Slower execution for Spark code written in PySpark vs Spark code written in Java (except if you switch to DataFrame API)\n",
    "* (+) Apache Hadoop integration\n",
    "    * Relatively easy integration into existing ecosystem (HDFS)\n",
    "    * Scalability, reliability, and resilience\n",
    "* (+) Machine learning libraries natively available\n",
    "* Spark vs. MapReduce\n",
    "    * Storage Compatibility - Spark can be built on top of any filesystem whereas MapReduce only works in HDFS\n",
    "    * Speed - Spark can be 100x faster than MapReduce in memory, and 10x faster on disk. MapReduce writes data to disk after each map step, and after each reduce step. This I/O is very costly in terms of performance, especially for iterative algorithms\n",
    "        * Spark uses a lot of memory and tries to keep everything in memory when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Spark Ecosystem\n",
    "![spark_ecosystem](spark_ecosystem.png)\n",
    "![spark_functionality](spark_functionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Resilient Distributed Datasets (RDD)\n",
    "![rdd](rdd.png)\n",
    "* **Many data sources** - create from HDFS, S3, HBase, JSON, text, local\n",
    "    * Create SparkContext in two ways:\n",
    "        * Parallelize an existing collection of objects (e.g. local)\n",
    "        ```python\n",
    "        rdd = sc.parallelize([1,3,4,5,6])\n",
    "        ```\n",
    "        * Read in an external data set (e.g. text, HDFS, S3, JSON)\n",
    "        ```python\n",
    "        rdd = sc.textFile('path/to/file')\n",
    "        ```\n",
    "* **Parallel operation (Partitioned)** - distributed across the cluster as **partitions** (atomic chunks of data)\n",
    "* **Fault-tolerant** - can recover from errors (node failures, slow process) \n",
    "    * traceability of each partition, can re-run the processing (using directed acyclic graph (DAG))\n",
    "* **Immutable** - cannot modify an RDD in place\n",
    "* **Lazily Evalulated** - doesn't execute any of the tasks until an action function is executed\n",
    "* **Cachable (or Persistable)** - keeps certain parts of the data in memory/disk to allow for repeated and faster execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Functional Programming Paradigm\n",
    "![functional_prog](https://image.slidesharecdn.com/apachesparkmaster-150830121025-lva1-app6892/95/apache-spark-core-20-638.jpg?cb=1498977510)\n",
    "* Since RDDs are immutable, **only transformations** to an existing RDD to another one can happen\n",
    "* Spark provides many **transformations functions**\n",
    "* Programming multiple RDD transformations results in **directed acyclic graph (DAG)**\n",
    "* Execution of DAG tasks is **passed from client to master (cluster manager/SparkContext/Master Node)**, who then distributes them to workers (**Worker Nodes**), who applies them across their partitions of the RDD\n",
    "    * **SparkContext** - acts as a gateway between the client and Spark master\n",
    "        * sends code/data to the master who then sends it to the workers\n",
    "![cluster_manager](cluster_manager.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Directed Acyclic Graph (DAG)** - the sequence of transformations kept by the driver for execution\n",
    "![dag](dag.png)\n",
    "* Construct sequence of transformations\n",
    "* Spark functional programming interface builds up a DAG\n",
    "* This DAG is sent by the driver for execution to the cluster manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "6) RDD Operations\n",
    "* Types of RDD Operations:\n",
    "![rdd_operations](https://summerofhpc.prace-ri.eu/wp-content/uploads/2015/12/project_1605_01.png)\n",
    "    * **Transformations** - normal functions that do not execute, but start to queue up the DAG\n",
    "        * e.g. filter, map, flatMap, join, reduceByKey, groupByKey, sortBy\n",
    "    * **Actions** - executing these functions will cause the DAG to execute (all transformations prior to that action will be run)\n",
    "    * e.g. first, take, collect, count, reduce, countByKey, saveAsTextFile\n",
    "* **Pair RDDs** - operations on tuples (key, value)\n",
    "    * offers better partitioning\n",
    "    * exposes new functionality\n",
    "* **Persisting/Caching** - explicitly keeps an RDD in memory\n",
    "    * Use if you have an RDD that is or will be used for different operations many times\n",
    "    * Persist allows you to have variation in storage details (memory/disk preferences)\n",
    "        * **MEMORY_ONLY** - Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.\n",
    "        * **MEMORY_AND_DISK** - Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.\n",
    "        * **MEMORY_ONLY_SER** - Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\n",
    "        * **MEMORY_AND_DISK_SER** - Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.\n",
    "        * **DISK_ONLY** - Store the RDD partitions only on disk.\n",
    "        * **MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.** - Same as the levels above, but replicate each partition on two cluster nodes.\n",
    "        * **OFF_HEAP (experimental)** - Store RDD in serialized format in Tachyon. Compared to MEMORY_ONLY_SER, OFF_HEAP reduces garbage collection overhead and allows executors to be smaller and to share a pool of memory, making it attractive in environments with large heaps or multiple concurrent applications. Furthermore, as the RDDs reside in Tachyon, the crash of an executor does not lead to losing the in-memory cache. In this mode, the memory in Tachyon is discardable. Thus, Tachyon does not attempt to reconstruct a block that it evicts from memory.\n",
    "* **Transformation Complexity**\n",
    "![transformation_complexity](transformation_complexity.png)\n",
    "    * Narrow transformations - doesn't cause shuffling (not computationally expensive)\n",
    "        * e.g. filter, map\n",
    "    * Wide transformation - causes shuffling which can be computationally expensive\n",
    "        * e.g. join, groupByKey, reduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7) Advanced Spark Programming\n",
    "* **Partitioning**\n",
    "    * Deciding number of partitions:\n",
    "        * by default, spark chooses the number of partitions based off the size of your cluster\n",
    "        * have option of parallelizing your RDD over more partitions\n",
    "        * more partitions means more parallel processes, but more overhead (I/O). Need to choose $k$ partitions based on: \n",
    "            * Gain in parallelization > Loss in overhead\n",
    "        * spark documentation recommends 2-4 partitions per CPU (core) in the cluster\n",
    "        * can setup when or after initializing the RDD:\n",
    "            * when initializing RDD:\n",
    "            ```python\n",
    "            rdd = sc.parallelize([1,3,4,5,6], 16)\n",
    "            ```\n",
    "            * after initializing RDD:\n",
    "            ```python\n",
    "            rdd.repartition(16)\n",
    "            ```\n",
    "    * Partitioning By Key (when RDDs are Pair RDDs):\n",
    "        * in a distributed program, communication between different machines is often very expensive\n",
    "        * control the way that Spark partitions our RDDs when they are composed of key/value pairs\n",
    "        * assure that all key/value pairs for a give key will end up on the same machine (preventing unnecessary shuffling). This can reduce the communication that is necessary between machines and greatly speed up the program\n",
    "        ```python\n",
    "        rdd.partitionBy(100)\n",
    "        ```\n",
    "            * This will partition the data into 100 partitions by the current key\n",
    "            * Only useful when a dataset is reused multiple times in key-oriented operations (groupByKey, reduceByKey, join, etc.)\n",
    "* **Joins**\n",
    "    * Spark RDDs offer all of our standard SQL joins:\n",
    "        * e.g. inner(default), left outer, right outer, full outer\n",
    "    * Each RDD in the join must be in the format of (key,value) pairs, where the key in each corresponds to the same variable\n",
    "    * Example: Transactions By Customers and Store Lookup Table\n",
    "    ![joins](joins.png)\n",
    "    ```python\n",
    "    transactions_rdd = sc.parallelize([(100156, 1),(100156, 2),(100157, 1)])\n",
    "    store_lookup_rdd = sc.parallelize([(1,\"REI\"),(2,\"Sports\"),(3,\"Target\"),(4,\"Hippy\")])\n",
    "    joined_rdd = transactions_rdd.map(lambda (key, value): (value, key)).join(store_lookup_rdd)\n",
    "    \n",
    "    result: [(1, (100156, 'REI')), (1, (100157, 'REI')), (2, (100156, 'Sports!'))]\n",
    "    ```\n",
    "* **Accumulators**\n",
    "    * A type of shared variable across all worker machines\n",
    "    * Provide a simple syntax for **aggregating** values from worker nodes back to the driver program (Spark's version of MapReduce's counters function)\n",
    "    * Most common use is to count events that occur during the job for debugging purposes\n",
    "    ```python\n",
    "    myfile = sc.textFile(inputFile)\n",
    "    blank_lines = sc.accumulator(0) # 0 is initialization value\n",
    "    def extract_call_signs(line):\n",
    "        global blank_lines\n",
    "        if(line == \"\"):\n",
    "            blank_lines += 1\n",
    "        return line.split(\" \")\n",
    "    call_signs = myfile.flatMap(extract_call_signs)\n",
    "    ```\n",
    "* **Broadcast Variables**\n",
    "    * Another type of shared variable across all worker machines\n",
    "    * Allow the program to efficiently send a large, read-only value (or values) to all the worker nodes\n",
    "    * By default, Spark automatically sends all variables referenced in our functions to the worker nodes for each task, which **can be highly inefficient**. We might end up sending **multiple copies** of the same variables to the same workers\n",
    "    * Broadcast variables are a solution to this problem\n",
    "    * Can be particularly useful to **broadcast** a small lookup table across our worker nodes\n",
    "        * Example: Transactions By Customers and Store Lookup Table (above)\n",
    "        ```python\n",
    "        transactions_rdd = sc.parallelize([(100156, 1),(100156, 2),(100157, 1)])\n",
    "        store_lookup_broadcasted = sc.broadcast({1:\"REI\",2:\"Sports\",3:\"Target\",4:\"Hippy\"})\n",
    "        def process_transactions(transaction, store_lookup_broadcasted):\n",
    "            store_id = transaction[0]\n",
    "            store_name = store_lookup_broadcasted.value.get(store_id)\n",
    "            user_id = transaction[1]\n",
    "            return (store_id, (user_id, store_name))\n",
    "        transactions_rdd = transactions_rdd.map(lambda (key, value): (value, key))\n",
    "        lookedup_rdd = transactions_rdd.map(lambda transaction: process_transactions(transaction, store_lookup_broadcasted))\n",
    "\n",
    "        result: [(1, (100156, 'REI')), (1, (100157, 'REI')), (2, (100156, 'Sports!'))]\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Spark MLlib\n",
    "* Conventions For Supervised Learning:\n",
    "```python\n",
    "LabelPoint(target, feature)\n",
    "    # think of X and y\n",
    "    # feature is a row of X\n",
    "    # y is associated label\n",
    "target(numeric)\n",
    "feature(numeric_vector)\n",
    "```\n",
    "* **RDD-Based** Machine Learning Libraries in pre-Spark 2.0 (Removed in Spark 3.0):\n",
    "    * Basic Statistics:\n",
    "        * Summary statistics\n",
    "        * Correlations\n",
    "        * Stratified Sampling\n",
    "        * Hypothesis Testing\n",
    "        * Streaming Significance Testing\n",
    "        * Random Data Generation\n",
    "    * Classification/Regression:\n",
    "        * Generalized Linear Regression (GLM)\n",
    "        * Logistic Regression\n",
    "        * Naive Bayes\n",
    "        * Support Vector Machines (SVM)\n",
    "        * Decision Trees/Random Forests\n",
    "        * Gradient Boosted Trees\n",
    "        * Isotonic Regression\n",
    "        * Multilayer Perceptron (e.g. Neural Network)\n",
    "    * Clustering\n",
    "        * K-means Clustering\n",
    "        * Gaussian Mixture\n",
    "        * Power Iteration Clustering (PIC)\n",
    "        * Latent Dirichlet Allocation (LDA)\n",
    "        * Bisecting K-means\n",
    "        * Streaming K-means\n",
    "    * Decomposition\n",
    "        * Singular Value Decomposition (SVD)\n",
    "        * Principal Component Analysis (PCA)\n",
    "        * Non-matrix Factorization (NMF)\n",
    "    * Recommenders/Collaborative Filtering\n",
    "        * Alternative Least Squares (ALS)\n",
    "    * Optimization:\n",
    "        * Stochastic Gradient Descent (SGD)\n",
    "        * Limit-Memory BFGS (L-BFGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
