{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Machine Learning Library (MLlib)\n",
    "\n",
    "* Objectives:\n",
    "    * How to preprocess data with MLlib\n",
    "    * How to munge data with MLlib\n",
    "    * How to train models with MLlib\n",
    "    * How to make predictions at scale on data with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Machine Learning Library (MLlib)** - perform large scale machine learning with a built-in library of machine learning algorithms\n",
    "* Example: Retail Store with series of transformations\n",
    "    * Prepping retail store data into numerical representation\n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.sql.functions.date_format\n",
    "    val preppedDataFrame = staticDataFrame\n",
    "        .na.fill(0)\n",
    "        .withColumn(\"day_of_week\", date_format($\"InvoiceDate\", \"EEEE\"))\n",
    "        .coalesce(5) // (partitions data) It avoids a full shuffle. If it's known that the number is decreasing then the executor can safely keep data on the minimum number of partitions, only moving the data off the extra nodes, onto the nodes that we kept\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.sql.functions import date_format, col\n",
    "    preppedDataFrame = staticDataFrame \\\n",
    "        .na.fill(0) \\\n",
    "        .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\")) \\\n",
    "        .coalesce(5)\n",
    "    ```\n",
    "    * Split data into training and test sets by date that a certain purchase occurred\n",
    "        * Use validation splits or cross-validation to create training/test set\n",
    "    ```scala\n",
    "    %scala\n",
    "    val trainDataFrame = preppedDataFrame\n",
    "       .where(\"InvoiceDate < '2011-07-01'\")\n",
    "    val testDataFrame = preppedDataFrame\n",
    "       .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    trainDataFrame = preppedDataFrame \\\n",
    "        .where(\"InvoiceDate < '2011-07-01'\")\n",
    "    testDataFrame = preppedDataFrame \\\n",
    "        .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "    ```\n",
    "    * Turn days of weeks into corresponding numerical values (Saturday as 6 and Monday as 1)\n",
    "        * However this implicitly stating that Saturday is greater than Monday by pure numerical values, which is incorrect\n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.ml.feature.StringIndexer\n",
    "    val indexer = new StringIndexer()\n",
    "        .setInputCol(\"day_of_week\")\n",
    "        .setOutputCol(\"day_of_week_index\")\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    indexer = StringIndexer() \\\n",
    "        .setInputCol(\"day_of_week\") \\\n",
    "        .setOutputCol(\"day_of_week_index\")\n",
    "    ```\n",
    "    * Turn days of week into numerical representation using boolean flags\n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.ml.feature.OneHotEncoder\n",
    "    val indexer = new OneHotEncoder()\n",
    "        .setInputCol(\"day_of_week\")\n",
    "        .setOutputCol(\"day_of_week_index\")\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "    indexer = OneHotEncoder() \\\n",
    "        .setInputCol(\"day_of_week\") \\\n",
    "        .setOutputCol(\"day_of_week_index\")\n",
    "    ```\n",
    "    * Each of these will result in a set of columns that we will \"assemble\" into a vector. All machine learning algorithms in Spark take as input a `Vector` type, which must be a set of numerical values.\n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.ml.feature.VectorAssembler\n",
    "    val vectorAssembler = new VectorAssembler()\n",
    "        .setInputCols(Array(\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"))\n",
    "        .setOutputCol(\"features\")\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    vectorAssembler = VectorAssembler() \\\n",
    "        .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"]) \\\n",
    "        .setOutputCol(\"features\")\n",
    "    ```\n",
    "    * We can see that we have 4 key features, the price, the quantity, and the day of week. Now we’ll set this up into a pipeline so any future data we need to transform can go through the exact same process.\n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.ml.Pipeline\n",
    "    val transformationPipeline = new Pipeline()\n",
    "        .setStages(Array(indexer, encoder, vectorAssembler))\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.ml import Pipeline\n",
    "    transformationPipeline = Pipeline() \\\n",
    "        .setStages([indexer, encoder, vectorAssembler])\n",
    "    ```\n",
    "    * Now preparing for training is a two step process. We first need to fit our transformers to this dataset. Once we fit the training data, we are now create to take that fitted pipeline and use it to transform all of our data in a consistent and repeatable way.\n",
    "    ```scala\n",
    "    %scala\n",
    "    val fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "    val transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "    transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "    ```\n",
    "    * At this point, it’s worth mentioning that we could have included our model training in our pipeline. We chose not to in order to demonstrate a use case for caching the data. At this point, we’re going to perform some hyperparameter tuning on the model, since we do not want to repeat the exact same transformations over and over again, we’ll instead cache our training set. This is worth putting it into memory because that will allow us to e iciently, and repeatedly access it in an already transformed state. If you’re curious to see how much of a di erence this makes, skip this line and run the training without caching the data. Then try it a er caching, you’ll see the results are (very) significant.\n",
    "    ```scala\n",
    "    %scala\n",
    "    transformedTraining.cache()\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    transformedTraining.cache()\n",
    "    ```\n",
    "    * Now we have a training set, now it’s time to train the model. First we initialize an untrained model, then we train it. There are always two types for every algorithm in MLlib’s DataFrame API. The algorithm Kmeans and then the trained version which is a KMeansModel. We can see the resulting cost at this point. Which is quite high, that’s likely because we didn’t necessary scale our data or transform. \n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.ml.clustering.KMeans\n",
    "    val kmeans = new KMeans()\n",
    "        .setK(20)\n",
    "        .setSeed(1L)\n",
    "    val kmModel = kmeans.fit(transformedTraining)\n",
    "    kmModel.computeCost(transformedTraining)\n",
    "    val transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "    kmModel.computeCost(transformedTest)\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.ml.clustering import KMeans\n",
    "    kmeans = KMeans() \\\n",
    "        .setK(20) \\\n",
    "        .setSeed(1L)\n",
    "    kmModel = kmeans.fit(transformedTraining)\n",
    "    kmModel.computeCost(transformedTraining)\n",
    "    transformedTest = fittedPipeline.transform(testDataFrame)\n",
    "    kmModel.computeCost(transformedTest)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
