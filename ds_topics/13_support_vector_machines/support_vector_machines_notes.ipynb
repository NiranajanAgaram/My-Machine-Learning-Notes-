{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM)\n",
    "\n",
    "1) **Maximum Margin Classifier** (Old SVM) - a hyperplane which separates two data of points and is at equal distance from the two. The margin between the hyperplane and the data point is maximal.\n",
    "* Creating a different type of decision boundary\n",
    "* **Hyperplane** - linear separation of data points with a $(p-1)$ dimensional classifier\n",
    "![hyperplane](https://upload.wikimedia.org/wikipedia/commons/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg)\n",
    "    * In general, an n-dimensional space can be separated by a $(n-1)$-dimensional hyperplane\n",
    "        * example: split a plane (2D) with a line (1D)\n",
    "            * $\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}>0$ when $y_i=+1$\n",
    "            * $\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}<0$ when $y_i=-1$\n",
    "        * example: split a space (3D) with a plane (2D)\n",
    "    * In an $n$-dimensional space any hyperplane can be defined by $w \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. The hyperplane includes all $x \\in \\mathbb{R}^n$ where:\n",
    "    * $w_0x_0+w_1x_1+\\cdots+w_{n-1}x_{n-1}-b=w \\cdot x-b=0$\n",
    "        * $w$ and $b$ define a hyperplane\n",
    "        * $\\frac{w}{\\Vert w \\Vert}$ is the hyperplane's normal vector\n",
    "        * $\\frac{b}{\\Vert w \\Vert}$ is the hyperplane's distance from the origin\n",
    "* Defining Margin - the distance from the hyperplane to the nearest training data point\n",
    "    * Maximize Margin - larger margin means better generalization (lower variance)\n",
    "    * Goal of Maximum Margin Classifier - calculate $w$ and $b$ of the hyperplane such that the classes are split correctly and the margin is maximized\n",
    "        * equation: $\\big|w\\cdot x^{(i)}-b\\big|=1$ where $x^{(i)}$ is the closest point to the hyperplane\n",
    "        * What happens to the hyperplane when we scale $w$ and $b$ by some factor $c$?\n",
    "    * If $x^{(i)}$ is the closest point to the hyperplane, then the distance from $x^{(i)}$ to the hyperplane is our margin. What is that distance?\n",
    "        * margin  $\\rightarrow \\begin{align} d \n",
    "            & = \\big|\\frac{w}{\\Vert w \\Vert} \\cdot (x^{(i)}-x)\\big| \\\\\n",
    "            & = \\frac{w\\cdot x^{(i)}-w\\cdot x}{\\Vert w \\Vert} \\\\\n",
    "            & = \\frac{w\\cdot x^{(i)}-b-w\\cdot x+b}{\\Vert w \\Vert} \\\\\n",
    "            & = \\frac{1}{\\Vert w \\Vert} \\\\\n",
    "            \\end{align}$\n",
    "    * Initial idea: Maximize $\\frac{1}{\\Vert w \\Vert}$\n",
    "        * However, this optimization problem is not solvable\n",
    "    * Reformulated: Minimize $\\frac{1}{2}\\Vert w \\Vert^2$\n",
    "* **Support Vectors** - the maximum margin hyperplane is defined by the points that touch the margin\n",
    "![max_margin_hyperplane](https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Soft Margins and the \"Kernel Trick\" (Modern SVM)\n",
    "* **Soft Margin (Support Vector Classifier)** - an extension to Maximum Margin Classifier with $C$ constant giving misclassification error penalty\n",
    "    * Useful for data that is not linearly separable, noisy, has outliers\n",
    "    * **Large $C$ (Harder margins)** - values classification accuracy over a large margin (generally: high bias, low variance)\n",
    "    * **Small $C$ (Softer margins)** - values a large margin over classification accuracy (generally: low bias, high variance)\n",
    "        * Even though it values a large margin over accuracy, it can account for datasets where there is inseparable data\n",
    "        ![softer_margin_in_sep](inseparable_data_softer_margin.png)\n",
    "        * Softer margins creates a better margin in cases of outliers that separates the data in generalized way\n",
    "        ![softer_margin_in_out](soft_margins_outlier.png)\n",
    "* **Kernel Trick (Support Vector Machine)** - increasing the dimensional space to allow for data that is inseparable in lower dimensional space without ever computing the coordinates of the data in the space, but rather by simply computing the inner products between the images of all pairs of data in the feature space\n",
    "![kernel_trick_dimension](kernel_trick_dimension.png)\n",
    "    * The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function: $K(x^{(i)},x^{(i)}=\\phi(x^{(i)})\\cdot\\phi(x^{(j)})\\in \\mathbb{R}$\n",
    "        * saves some computation since we never need to compute $\\phi$\n",
    "        * opens new possibilites since kernel can operate in infinite dimensions\n",
    "* Kernel Functions:\n",
    "    1. **Polynomial Kernel** - represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables / expand feature space by simply creating new features\n",
    "![poly_kernel](poly_kernel.png)\n",
    "        * equation: $(X_1,X_2) \\rightarrow (X_1,X_2,X_1^2,X_2^2,X_1X_2)$\n",
    "        * requires an extra hyperparameter, $d$, for \"degree\"\n",
    "    2. **(Gaussian) Radial Basis Function (RBF) Kernel** - equivalent to the dot product in the Hilbert space of infinite dimensions\n",
    "![rbf_kernel](rbf_kernel.png)\n",
    "        * equation: $K(x^{(i)},x^{(j)})=e^{(-\\gamma||x^{(i)}-x^{(j)}||^2)}$\n",
    "        * requires an extra hyperparameter, $\\gamma$, for \"gamma\"\n",
    "* Unbalanced Classes with SVM - adjust weights inversely proportional to class frequencies\n",
    "![unbalanced_classes_svm](unbalanced_classes_svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Bias-Variance Tradeoff / SVM vs Logistic Regression\n",
    "* **Bias** (For SVM)\n",
    "    * example: a linear SVM looks for dividing hyperplanes in the input space *only*\n",
    "    * for complex data, high-bias models often *underfit the data*\n",
    "* **Variance** (For SVM)\n",
    "    * example: a RBF SVM looks for dividing hyperplanes in a infinite-dimensional space\n",
    "    * for simple data, high-variance models often *overfit* the data\n",
    "* SVM vs Logistic Regression:\n",
    "    * SVM maximizes the **margin** (whereas Logistic Reg maximizes Binomial Log Likelihood function)\n",
    "    * (+) When classes are nearly separable, SVMs tends to do better than Logistic Regression\n",
    "        * If not, Logistic Regression with Ridge and SVMs are similar\n",
    "    * (-) When estimating probabilities, Logistic Regression is a better choice\n",
    "    * (+) With kernels, SVMs work well, however, with Logistic Regression with kernels can get too computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Grid Search CV (Hyperparameter Tuning)\n",
    "![grid_search_C_gamma](grid_search_C_gamma.png)\n",
    "* Find $C$ and $\\gamma$ by searching through values we expect might work well\n",
    "* Use cross-validation accuracy to determine which values are best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) SVM intuition\n",
    "* Components of SVM:\n",
    "    1. Hyperplane that separates data as well as possible\n",
    "    2. Allowing some room for error (soft margin, $C$)\n",
    "    3. Using kernels to accomodate non-linear class boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
