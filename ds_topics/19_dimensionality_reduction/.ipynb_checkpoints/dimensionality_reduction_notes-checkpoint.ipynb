{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "\n",
    "* Objectives:\n",
    "    * Why reduce dimensionality?\n",
    "    * Methods for reducing dimensionality\n",
    "    * One common technique: Principal Components Analysis (PCA)\n",
    "    * Crude facial recognition with PCA and kNN (e.g. Eigenfaces)\n",
    "    * Singular Value Decomposition (SVD)\n",
    "    * SVD vs. PCA\n",
    "    * SVD for capturing latent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The Importance of Reducing Dimensionality\n",
    "* dimensionality = number of features = number of predictors\n",
    "* Why reduce dimensionality?\n",
    "    * High dimensional data causes many problems\n",
    "    * **Curse of Dimensionality** - points are \"far away\" in high dimensions, and it's easy to overfit small datasets (sparsity of sample data points)\n",
    "        * Distance between nearest neighbors is very large\n",
    "        * Stimulation: uniformly distributed points in a cube\n",
    "        ![distance_in_hyperspace](distance_in_hyperspace.png)\n",
    "        * Stimulation: uniformly distributed points in a hypercube (most data points are closer to the boundary of the sample space)\n",
    "        ![hypercube](hypercube.png)\n",
    "        * **Sample sparsity** - when the dimensionality $d$ increases, the volume of the space increases so fast that the available data becomes sparse\n",
    "            * Example: uniformly distributed data points ($N=1000$) and cover 20% of range of each space\n",
    "            ![sample_sparsity](sample_sparsity.png)\n",
    "            * In ten dimensions we need to cover 80% of the range of each coordinate to capture 10% of the data\n",
    "            ![10_dimensions](10_dimensions.png)\n",
    "                * For a fraction $r$ of unit volume: (Edge Length) $e(r)=r^{\\frac{1}{d}}$\n",
    "                * Reducing $r$ gives fewer observations to average, and higher variance of fit\n",
    "        * **Classifier Performance** - as the dimensionality increases, the classifier's performance increases until the optimal number of features is reached\n",
    "        ![classifier_performance](classifier_performance.png)\n",
    "            * Increasing the dimensionality further **without** increasing the number of training samples results in a **decrease** in classifier performance\n",
    "    * **Difficult Visualization** - it's hard visualize anything more than 3 dimensions\n",
    "    * **Finding Latent Features** - often the most relevant features are not explicitly present in the raw high dimensional data (especially for image/video data)\n",
    "    * **Removing Correlation** - with many, many features (dimensions), there will most likely be a lot of correlations (e.g. consider neighboring pixels in an image dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Other Methods For Reducing Dimensionality\n",
    "* Subset Selection of Features (e.g. **Forward Stepwise Selection**) - selecting the best model with the best subset of features based on some metric (e.g. Mallow's C, AIC, BIC, Adjusted $R^2$, or cross-validation)\n",
    "* **Lasso Regression** - creates sparse models by zeroing out features that less importance\n",
    "* **Relaxed Lasso**\n",
    "    1. Using cross-validation and lasso regression, find the best value for $\\lambda$\n",
    "    2. Keep only the features with non-zero coefficients\n",
    "    3. Re-fit using ordinary least squares (OLS) regrsesion (which is refitting using no regularization)\n",
    "* **Upper-layer Features** (in NN for labeled training data) - train the neural network, then interpret the output of the hidden neurons (fully-connected layer before output) as high-level features\n",
    "* **Autoencoders** (in NN for labeled or unlabeled training data) - autoencoders are neural networks that has the network learn to **reconstruct the input** instead of learning the target\n",
    "![autoencoders](autoencoders.png)\n",
    "    * Force the information through a **bottleneck hidden layer**\n",
    "    * Converges on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Principal Components Analysis (PCA)** - common dimensionality reduction technique that doesn't require labeled data\n",
    "* First goal of PCA is to remove correlation between features\n",
    "* A side effect is that we can use the process to reduce the dimensionality of the data while preserving most of the variance in our data\n",
    "* **Rank and Dimensionality**\n",
    "    * $\\mathbf{x}_i^T$ are dimensional row vectors (feature space)\n",
    "    * $\\mathbf{y}_j$ are column vectors (dependent space)\n",
    "    * Linear Independence:\n",
    "        * For any set of $n$ vectors, a linear combination is an expression of the form: $c_1\\mathbf{y}_1+c_2\\mathbf{y}_2+\\cdots+c_n\\mathbf{y}_n=0$\n",
    "            * If this equation holds only if all $c_i$'s are zero, then $y_i$'s are **linear independent** vectors\n",
    "            * If this equation holds with $c_i$'s **not equal** to zero, then $y_i$'s are **linear dependent** vectors. This means we can express at least one of the vectors as linear combination of the others: $\\mathbf{y}_1=k_2\\mathbf{y}_2+\\cdots+k_n\\mathbf{y}_n$ where $k_j=\\frac{-c_j}{c_1}$\n",
    "    * Rank of Matrix $\\mathbf{D}$:\n",
    "        * Maximum number of linearly independent column vectors of $\\mathbf{D}$\n",
    "        * Maximum number of linearly independent row vectors of $\\mathbf{D}$\n",
    "        * $\\mathbf{D}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{y}_1 & \\mathbf{y}_2 & \\cdots & \\mathbf{y}_d\n",
    "            \\end{array}\\right]$\n",
    "            * $\\mathbf{D}$ is a $[n \\times d]$\n",
    "            * $rank(\\mathbf{D})=r\\leq min(n,d)$\n",
    "        * Rank of data matrix gives the dimensionality of the data\n",
    "    * The number of linearly independent **basis vectors** needed to represent a data vector, gives the dimensionality of the data\n",
    "        * equation: $\\mathbf{x}=x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+\\cdots+x_d\\mathbf{e}_d$\n",
    "        ![basis_vectors](basis_vectors.png)\n",
    "        * The data points apparently reside in a $d$-dimensional **attribute space**\n",
    "        * But, if $r<d$, then the data points actually reside in a lower $r$-dimensional space\n",
    "    * Determining vector space for orthonormal basis vectors:\n",
    "        ![orthonormal_basis_vectors](orthonormal_basis_vectors.png)\n",
    "        * $\\mathbf{D}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{x}_1^T \\\\\n",
    "            \\mathbf{x}_2^T \\\\\n",
    "            \\vdots \\\\\n",
    "            \\mathbf{x}_n^T\n",
    "            \\end{array}\\right]$\n",
    "        * each point $\\mathbf{x}_i^T=(x_1,x_2,\\dots,x_d)^T$ is a vector in $d$-dimensional vector space\n",
    "        * write $\\mathbf{x}$ as: $\\mathbf{x}=\\sum_{i=1}^d x_i\\mathbf{e}_i$ where $e_i$ are **orthonormal basis vectors**:\n",
    "            * $\\mathbf{e}_i^Te_j=1$ if $i=j$\n",
    "            * $\\mathbf{e}_i^Te_j=0$ if $i\\neq j$\n",
    "    * Changing the dimensional space to fit along the data vector space\n",
    "        ![orthonormal_basis_vectors_2](orthonormal_basis_vectors_2.png)\n",
    "        * $\\mathbf{D}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{x}_1^T \\\\\n",
    "            \\mathbf{x}_2^T \\\\\n",
    "            \\vdots \\\\\n",
    "            \\mathbf{x}_n^T\n",
    "            \\end{array}\\right]$\n",
    "        * each point $\\mathbf{x}_i^T=(x_1,x_2,\\dots,x_d)^T$ is a vector in $d$-dimensional vector space\n",
    "        * given any other set of $d$ orthonormal vectors: $\\mathbf{x}$ as: $\\mathbf{x}=\\sum_{i=1}^d a_i\\mathbf{u}_i$ where $u_i$ are **orthonormal basis vectors**:\n",
    "            * $\\mathbf{u}_i^Tu_j=1$ if $i=j$\n",
    "            * $\\mathbf{u}_i^Tu_j=0$ if $i\\neq j$  \n",
    "        * $\\mathbf{A}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{a}_1^T \\\\\n",
    "            \\mathbf{a}_2^T \\\\\n",
    "            \\vdots \\\\\n",
    "            \\mathbf{a}_n^T\n",
    "            \\end{array}\\right]$\n",
    "        * each point $\\mathbf{a}_i^T=(a_1,a_2,\\dots,a_d)^T$ is a vector in $d$-dimensional vector space\n",
    "        * $a_j=\\mathbf{u}_j^T\\mathbf{x}$\n",
    "        * in vector form:\n",
    "            * $\\mathbf{a}=\\mathbf{U}^T\\mathbf{x}$\n",
    "            * $\\mathbf{U}=(\\mathbf{u}_1,\\mathbf{u}_2,\\dots,\\mathbf{u}_d)$\n",
    "    * Finding the optimal set of orthonormal basis vectors:\n",
    "        * Because there are potentially infinite choices for the set of orthonormal basis vectors, one natural question is whether there exists an **optimal** basis, for a suitable notion of optimality\n",
    "        * Finding a reduced dimensionality subspace that still preserves the essential characteristics of the data (high variance explained)\n",
    "        * Project data points from a $d$-dimensional space to an $r$-dimensional space where $r<d$\n",
    "        * $\\mathbf{x}'=\\sum_{i=1}^r a_i \\mathbf{u}_i$\n",
    "        * $\\mathbf{\\epsilon}=\\sum_{i=1}^d a_i \\mathbf{u}_i = \\mathbf{x}-\\mathbf{x}'$ (error vector)\n",
    "* PCA basic terminology:\n",
    "    * **Principle Component Analysis (PCA)** is a technique that seeks a $r$-dimensional basis that best captures the variance in the data\n",
    "    * **First Principal Component** - the direction with the largest projected variance\n",
    "    * **Second Principal Component** - the orthogonal direction that captures the second largest projected variance\n",
    "    ![principal_component](principal_component.png)\n",
    "* First Principle Component calculation:\n",
    "    * choose the direction $\\mathbf{u}$ such that the variance of the projected points is maximized\n",
    "    * the projected variance along $\\mathbf{u}$ is: $\\sigma_u^2=\\frac{1}{n}\\sum_{i=1}^n (a_i-\\mu_{\\mathbf{u}})^2$\n",
    "    * for centered data: $\\sigma_u^2=\\mathbf{u}^T\\mathbf{\\Sigma}\\mathbf{u}$\n",
    "    * $\\mathbf{\\Sigma}$ is covariance matrix of centered $\\mathbf{D}$: $\\mathbf{\\Sigma}=\\frac{1}{n}\\mathbf{D}^T\\mathbf{D}$\n",
    "    * maximizing $\\sigma$ (with constraint $\\mathbf{u}^T\\mathbf{u}=1$) gives:\n",
    "        * $\\mathbf{\\Sigma}\\mathbf{u}=\\mathbf{\\lambda}\\mathbf{u}$\n",
    "        * $\\sigma_{\\mathbf{u}}^2=\\mathbf{\\lambda}$\n",
    "    * to maximize projected variance, maximize the eigenvalue of $\\mathbf{\\Sigma}$\n",
    "    * eigenvector $\\mathbf{u}$ with maximum $\\mathbf{\\lambda}$ specifies the direction of most variance (First Principal Component)\n",
    "* Finding all eigenvectors corresponding to $\\lambda$:\n",
    "    * to find the best $r$-dim approximation to $\\mathbf{D}$, compute the eigenvalues of the covariance matrix $\\mathbf{\\Sigma}$\n",
    "    * eigenvalues of $\\mathbf{\\Sigma}$ are non-negative, and be sorted in decreasing order: $\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_r \\geq \\lambda_{r+1} \\cdots \\geq \\lambda_d \\geq 0$\n",
    "        * eigenvector corresponding to $\\lambda_1$ gives first principle component\n",
    "        * eigenvector corresponding to $\\lambda_2$ gives second principle component\n",
    "* Projected variance and minimizing MSE:\n",
    "    * reduced $r$-dimensional data matrix: $\\mathbf{A}=[\\mathbf{\\alpha}_1,\\mathbf{\\alpha}_2,\\cdots,\\mathbf{\\alpha}_r]$\n",
    "    * total variance of $\\mathbf{A}$: $var(\\mathbf{A})=\\sum_{i=1}^r \\lambda_i$\n",
    "    * the first $r$-principal components maximize the projected variance, $var(\\mathbf{A})$, and thus minimize the MSE:\n",
    "        * $MSE=\\frac{1}{n}\\sum_{i=1}^n \\mathbf{\\epsilon}_i^T\\mathbf{\\epsilon}_i=var(\\mathbf{D})-var(\\mathbf{A})$\n",
    "        * $\\mathbf{\\epsilon}_i = \\mathbf{x}_i-\\mathbf{x}_i'$\n",
    "* **Choosing the dimensionality** - how many dimensions, $r$, to use for a good approximation\n",
    "    * compute the fraction of the total variance captured by the first $r$ principal components: $f(r)=\\frac{\\lambda_1+\\lambda_2+\\cdots+\\lambda_r}{\\lambda_1+\\lambda_2+\\cdots+\\lambda_d}=\\frac{var(\\mathbf{A})}{var(\\mathbf{D})}$\n",
    "    * starting from the first principal component, then keep on adding additional components, and stop at the smallest value $r$, for which $f(r)\\geq\\alpha$\n",
    "    * in practice, $\\alpha$ is usually set to 0.9 or higher, so that the reduced dataset captures at least 90% of the total variance\n",
    "* PCA for $n<d$:\n",
    "    * the covariance matrix is a $d\\times d$ matrix\n",
    "    * typical algorithms for finding eigenvectors of $d \\times d$ matrix have a computational cost that scales like $O(d^3)$\n",
    "    * for $n<d$, there are only $n$ non-zero eigenvalues of covariance matrix\n",
    "    * starting from the eigenvalue equation: \n",
    "        * $\\mathbf{\\Sigma}\\mathbf{u}_i=\\mathbf{\\lambda}_i\\mathbf{u}_i$\n",
    "        * $\\frac{1}{n}\\mathbf{D}\\mathbf{D}^T\\mathbf{v}_i=\\mathbf{\\lambda}_i\\mathbf{v}_i$ where $\\mathbf{v}_i=\\mathbf{D}\\mathbf{u}_i$\n",
    "    * converted to eigenvalue equation of $n \\times n$ matrix:\n",
    "        * $\\mathbf{u}_i=\\frac{1}{\\sqrt{n\\lambda_i}}\\mathbf{D}^T\\mathbf{v}_i$\n",
    "    * Summary of steps:\n",
    "        1. Evaluate $\\mathbf{D}\\mathbf{D}^T$\n",
    "        2. Find its eigenvectors and eigenvalues\n",
    "        3. Compute the eigenvectors in the original data space\n",
    "* **Summary of PCA Steps**:\n",
    "    1. create the **centered design matrix** ($M$) - center the data by subtracting the mean. Transform centered data into matrix form where each row is one example\n",
    "    2. compute the **covariance matrix** ($M^TM$)\n",
    "    3. principal components are the **eigenvectors** of the covariance matrix - order the eigenvectors by decreasing corresponding eigenvalues to get an uncorrelated and orthogonal basis vector capturing the directions of most-to-least variance in your data\n",
    "        * **Eigenvector** - size of each eigenvalue denotes the amount of variance captured by that eigenvector\n",
    "* Example: Yale Face Database (using a subset of 105 face images)\n",
    "    * each image is 320x240 pixels, grayscale, centered, and cropped identically\n",
    "    ![face_images](face_images.png)\n",
    "    * **Eigenfaces** - result of applying PCA on images and looking at the eigenvectors of the face database covariance matrix:\n",
    "    ![eigenfaces](eigenfaces.png)\n",
    "        * What is each eigenface capturing?\n",
    "    * Drawbacks of Eigenfaces: (Using the old method in 1987-1991)\n",
    "        * (-) Faces must be aligned eyes to eyes, mouth to mouth - differences in translation and scale are captured by PCA (which isn't what we want)\n",
    "        * (-) Faces must be lit the same - differences in lighting are captured by PCA (which isn't what we want)\n",
    "    * Improvements to methodology:\n",
    "        * **Fisherfaces** - uses LDA and labels to help remove lighting effects\n",
    "        * **Active Shape Model** - use PCA on shapes detected in the image\n",
    "    * Crude Facial Recognition via PCA and kNN - using PCA on cropped face images (aka Eigenfaces) and combined with kNN, the model yields a rough facial recognition system\n",
    "    ![pca_knn](pca_knn.png)\n",
    "* Example: MNIST dataset (dataset of handwritten digits, 10 classes (0-9), 28x28 pixels (yielding 784-dimensional vector space), grayscale, 60,000 training images / 10,000 test images)\n",
    "    * Each eigenvector of the covariance matrix is a vector in the original $d$-dimensional space that can be represented as images of the same size as the data points\n",
    "    ![lambda_eigenvectors](lambda_eigenvectors.png)\n",
    "    * What are the sizes of the eigenvalues? (recall that the size of each eigenvector's eigenvalue denotes the amount of variance captured by that eigenvector)\n",
    "        * Plot of the complete spectrum of eigenvalues, sorted into decreasing order\n",
    "    ![eigenvalues](eigenvalues.png)\n",
    "    * Reconstructing the input by a linear combination of eigenvectors:\n",
    "    ![comb_eigenvectors](comb_eigenvectors.png)\n",
    "    * Embedding in 2D:\n",
    "    ![embedding_2d](embedding_2d.png)\n",
    "* When to use PCA (generally):\n",
    "    * kNN on high dimensional data\n",
    "    * clustering high dimensional data\n",
    "    * visualization (e.g. embeddings)\n",
    "    * working with images (e.g. feeding an image into a decision tree model)\n",
    "* When **not** to use PCA:\n",
    "    * retain interpretability of the feature space\n",
    "    * model doesn't need reduced dimensional data (e.g. OLS on relatively small dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Singular Value Decomposition (SVD) - more generalized matrix decomposition\n",
    "* **Singular Value Decomposition** - factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix to any $m \\times n$\n",
    "    * an $n \\times d$ data matrix $\\mathbf{D}$ can be factorized as: \n",
    "        * $\\mathbf{D}=\\mathbf{L}\\mathbf{\\Delta}\\mathbf{R}^T$\n",
    "            * $\\mathbf{L} = (n \\times n)$ left singular matrix\n",
    "            * $\\mathbf{R} = (d \\times d)$ right singular matrix\n",
    "            * $\\mathbf{\\Delta} = (n \\times d)$ diagonal matrix\n",
    "    * $\\mathbf{\\Delta}(i,j) = $$\\begin{cases} \n",
    "            \\delta_i & \\text{if } i=j \\\\\n",
    "            0 & \\text{if } i\\neq j \n",
    "            \\end{cases}$\n",
    "        * $i=1,\\dots,n$\n",
    "        * $j=1,\\dots,d$\n",
    "        * $\\delta_i$ = singular values\n",
    "    * if the rank of $\\mathbf{D}$ is $r \\leq min(n,d)$, then there will be only $r$ non-zero singular values: \n",
    "        * $\\delta_1 \\geq \\delta_2\\geq \\cdots \\geq \\delta_r \\geq 0$\n",
    "    * can discard those **left** and **right** singular vectors that correspond to zero singular values, to obtain the reduced **SVD** as: \n",
    "        * $\\mathbf{D}_r=\\mathbf{L}_r\\mathbf{\\Delta}_r\\mathbf{R}_r^T$\n",
    "        ![svd](svd.png)\n",
    "* SVD vs PCA:\n",
    "    * PCA is a **special case** of more general matrix decomposition (SVD)\n",
    "    * PCA yields the following decomposition of covariance matrix:\n",
    "        * $\\mathbf{\\Sigma}=\\mathbf{U}\\mathbf{\\Lambda} \\mathbf{U}^T$\n",
    "        * Covariance matrix in new basis: $\\mathbf{\\Lambda}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{\\lambda}_1 & 0 & \\cdots & 0 \\\\\n",
    "            0 & \\mathbf{\\lambda}_2 & \\cdots & 0 \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            0 & 0 & \\cdots & \\mathbf{\\lambda}_d \\\\\n",
    "            \\end{array}\\right]$\n",
    "        * $\\mathbf{\\sigma}_i^2=\\mathbf{\\lambda}_i=\\mathbf{u}_i^T\\mathbf{\\Sigma}\\mathbf{u}_i$\n",
    "    * Matrix comparison:\n",
    "        * PCA gives: $\\mathbf{D}^T\\mathbf{D}=n\\mathbf{\\Sigma}=\\mathbf{U}(n\\mathbf{\\Lambda})\\mathbf{U}^T$\n",
    "        * SVD gives: $\\mathbf{D}^T\\mathbf{D}=(\\mathbf{R}\\mathbf{\\Delta}\\mathbf{L}^T)^T(\\mathbf{R}\\mathbf{\\Delta}\\mathbf{L}^T)=\\mathbf{R}\\mathbf{\\Delta}_d^2\\mathbf{R}^T$\n",
    "            * where $\\mathbf{\\Delta}_d^2$ is the ($d\\times d$) diagonal matrix defined as: $\\mathbf{\\Delta}_d^2(i,i)=\\delta_i^2$\n",
    "        * Comparing both: $\\delta_i^2=n\\lambda_i$, $\\mathbf{R}=\\mathbf{U}$\n",
    "    * Singular vector comparison:\n",
    "        * The right singular vectors in $\\mathbf{R}$ are the same as eigenvectors of $\\mathbf{\\Sigma}$\n",
    "        * The left singular vectors in $\\mathbf{L}$ are the eigenvectors of the matrix ($n \\times n$), matrix $\\mathbf{D}\\mathbf{D}^T$, and the corresponding eigenvalues are given as $\\delta_i^2$ \n",
    "* Capturing Latent Features in SVD (Finding Topics/Concepts)\n",
    "    * SVD can be used to find latent topics in the data\n",
    "    * Example: People and Food Preferences Matrix\n",
    "    ![latent_topics](latent_topics.png)\n",
    "    * $\\mathbf{\\Delta}=$\n",
    "    ```python\n",
    "    array([[ 8.45, 0.  , 0.  , 0.  , 0.  ],\n",
    "           [ 0.  , 6.98, 0.  , 0.  , 0.  ],\n",
    "           [ 0.  , 0.  , 1.83, 0.  , 0.  ],\n",
    "           [ 0.  , 0.  , 0.  , 1.5 , 0.  ],\n",
    "           [ 0.  , 0.  , 0.  , 0.  , 1.1 ],\n",
    "           [ 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
    "           [ 0.  , 0.  , 0.  , 0.  , 0.  ]])\n",
    "    ```\n",
    "    * $\\mathbf{R}^T=$\n",
    "    ![latent_topics_rt](latent_topics_rt.png)\n",
    "    * $\\mathbf{R}_2=$\n",
    "    ![latent_topics_r2](latent_topics_r2.png)\n",
    "* **Multidimensional Scaling (MDS)** - a process used to find lower dimensional representaion that give that same distance between points\n",
    "    * distance matrix or proximity matrix:\n",
    "        * $\\mathbf{\\Delta}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{\\delta}_{11} & \\mathbf{\\delta}_{12} & \\cdots & \\mathbf{\\delta}_{1n} \\\\\n",
    "            \\mathbf{\\delta}_{21} & \\mathbf{\\delta}_{22} & \\cdots & \\mathbf{\\delta}_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            \\mathbf{\\delta}_{n1} & \\mathbf{\\delta}_{n2} & \\cdots & \\mathbf{\\delta}_{nn} \\\\\n",
    "            \\end{array}\\right]$\n",
    "        * $\\mathbf{\\delta}_{ij}=\\Vert \\mathbf{x}_i-\\mathbf{x}_j\\Vert$ (Euclidean distance between two vectors)\n",
    "    * MDS algorithm steps:\n",
    "        1. calculate the matrix of squared proximities: $\\mathbf{\\Delta}^2$\n",
    "        2. calculate the matrix: $\\mathbf{B}=\\frac{1}{2}\\mathbf{J}\\mathbf{\\Delta}^2\\mathbf{J}$, $\\mathbf{J}=\\mathbf{I}-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^T$\n",
    "        3. Obtain SVD of $\\mathbf{B}$: $\\mathbf{B}=\\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^T$\n",
    "    * For a 2-dimensional representation, keep two eigenvectors corresponding to the largest eigenvalues:\n",
    "    ![mds](mds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5) **t-Distributed Stochastic Neighbor Embedding (t-SNE)** - a nonlinear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot\n",
    "* Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points\n",
    "    * It is often used to visualize high-level representations learned by an artificial neural network\n",
    "    ![t_sne_digits](http://2.bp.blogspot.com/--l8yNRipldU/Vg5ECxalQmI/AAAAAAAAAws/s1rDpWuvtaY/s1600/tsne.png)\n",
    "* t-SNE algorithm comprises two main stages:\n",
    "    1. t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked\n",
    "    2. t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the **Kullback–Leibler (KL) divergence** between the two distributions with respect to the locations of the points in the map\n",
    "        * Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate\n",
    "* t-SNE algorithm:\n",
    "    * Given a set of $N$ high-dimensional objects $x_1,\\dots,x_N$, t-SNE first computes probabilities $p_{ij}$ that are proportional to the similarity of the objects $x_i$ and $x_j$: $$p_{j\\mid i} = \\frac{e^{\\big(\\frac{-\\Vert x_i - x_j \\Vert^2}{2\\sigma^2_i}\\big)}}{\\sum_{k \\neq i}e^{\\big(\\frac{-\\Vert x_i - x_k \\Vert^2}{2\\sigma^2_i}\\big)}}$$\n",
    "        * The similarity of datapoint $x_j$ to datapoint $x_i$ is the conditional probability, $p_{j \\mid i}$, that $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$: $$p_{ij}=\\frac{p_{j \\mid i} + p_{i \\mid j}}{2N}$$\n",
    "    * t-SNE aims to learn a $d$-dimensional map $y_1,\\dots,y_N$ (with $y_i \\in \\mathbb{R}^d$ that reflects the similarities $p_{ij}$ as well as possible.\n",
    "        * To this end, it measure similarities $q_{ij}$ between two points in the map $y_i$ and $y_j$, using very similar approach: $$q_{ij} = \\frac{\\frac{1}{1+\\Vert y_i-y_j \\Vert^2}}{\\sum_{k \\neq i} \\frac{1}{1+\\Vert y_i-y_k \\Vert^2}}$$\n",
    "        * Herein a heavy-tailed **Student-t distribution** (with one-degree of freedom) is used to measure similarities between low-dimensional points in order to allow dissimilar objects to be modeled far apart in the map\n",
    "* t-SNE has been used in a wide range of applications:\n",
    "    * computer security research\n",
    "    * music analysis\n",
    "    * cancer research\n",
    "    * bioinformatics\n",
    "    * biomedical signal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
