{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map Reduce\n",
    "\n",
    "* Objectives:\n",
    "    * What is \"Big Data\" and why is it important?\n",
    "    * How do distributed filesystems and processing work?\n",
    "    * Describe distributed systems architecture\n",
    "    * What is Hadoop HDFS?\n",
    "    * How does MapReduce work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Criteria For \"Big Data\" - Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization\n",
    "* **High Volume** - Data so large that it can't be worked with on a single computer\n",
    "* **High Velocity** - Data input/output too quick for it to be processed by a single computer\n",
    "* **High Variety** - Data in many different, disparately or not at all, structured formats (e.g. text, log file, video, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What is consider \"High\" Volume or \"Big\"?\n",
    "\n",
    "|  Class |        Size       |                  Tools                  |              Storage              |          Examples          |\n",
    "|:------:|:-----------------:|:---------------------------------------:|:---------------------------------:|:--------------------------:|\n",
    "| Small  | less than 10GB    | R / Python                              | Fits in a single machine's memory | Thousands of sales figures |\n",
    "| Medium | 10GB - 1TB        | Python w/ indexed files, large database | Fits on a single machine's disk   | Millions of web pages      |\n",
    "| Large  | greater than 1 TB | Hadoop, Spark, distributed databases    | Stored across multiple machines   | Billions of web clicks     |\n",
    "\n",
    "![world_of_data](https://assets.goodstatic.com/s3/magazine/assets/536765/original/open-uri20140703-4674-ghcvv3)\n",
    "\n",
    "* As more data is generated, and inevitably collected, the size of data you'll likely work with is going to increase\n",
    "* Tools to store and process these larger data sets are going to be increasingly required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Distributed Systems\n",
    "* Local vs Distributed\n",
    "    * **Local** - uses resources of one machine. Doesn't communicate with any others\n",
    "        * (+) Simple, fast when computations are small enough\n",
    "        * (-) Physical limits to memory, disk space, and CPU power\n",
    "    * **Distributed** - use the resources, processing and memory, of multiple machines. However, they need to be able to communicate with one another\n",
    "        * (+) Easily linearly scalable, can be designed to be fault-tolerant\n",
    "        * (-) Slow to communicate over network, need to solve problems with parallelizable techniques\n",
    "* Scaling\n",
    "    ![scaling](scaling.png)\n",
    "    * **Scale Up** - Make the computer bigger: disk, RAM, CPU cores\n",
    "    * **Scale Out** - Add more computers: take advantage of parallelizable algorithms\n",
    "* Do I need to use distributed computing?\n",
    "    * Because of overhead involved with having multiple computers in a network communicate with each other, and the restrained set of problems that can readily be solved in a parallelizable way, it's not a good enough reason to choose a distributed solution just because things are \"taking awhile\" on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Distributed Systems Architecture / HDFS\n",
    "* High Level Architecture\n",
    "![master_workers](master_workers.png)\n",
    "* **Hadoop Distributed File System (HDFS)** - distributed storage system\n",
    "![hdfs](hdfs.png)\n",
    "    * **Data nodes** - stores the data\n",
    "    * **Name node** - keeps track of where the data is stored\n",
    "* Fault-tolerance through Replication - Each of these blocks is replicated on 3, by default, of the data nodes in the cluster\n",
    "    ![replication](replication.png)\n",
    "    * Each file is broken up into blocks, default size is 64/128 MB\n",
    "    * How many nodes can be lost before the original file isn't recoverable?\n",
    "        ![name_node](name_node.png)\n",
    "        * Since the **name node knows about where all the copies of each block is**, it can automatically make new ones if some are lost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Hadoop MapReduce - processes all the distributed data\n",
    "* Processing in Hadoop:\n",
    "    * Move the computation portion of our endeavors to the computer that is storing each part of the data in HDFS\n",
    "    * This requires us to have a processing framework which can natively work on data that isn't all stored in same location\n",
    "* Advantages of MapReduce:\n",
    "    * Splits up data\n",
    "    * Moves data between nodes\n",
    "    * Manages resources, computational and memory\n",
    "    * Receives status and monitors health\n",
    "    * Fault-tolerance\n",
    "* MapReduce Framework Strategy - divide and conquer\n",
    "    1. Split a task into smaller subtasks\n",
    "    2. Solve these independently of one another (in parallel)\n",
    "    3. Recombine the output of each subtask into a final result\n",
    "* MapReduce Concepts - Mapping and reducing are concepts that belong to the functional programming paradigm\n",
    "    * **Map** - applies a function to each of the elements of a data structure\n",
    "    * **Reduce** - takes a function which aggregates the elements of a data structure\n",
    "    * This strategy is particularly useful in distributed computing because the elements of the data structure referenced in the map step don't need to be on the same machine, they can be the partitions of a file that live on different data nodes\n",
    "* MapReduce Diagram\n",
    "![mr_diagram](mr_diagram.png)\n",
    "    * **Map Step** - mapping is simply the action of taking in some form of data and filter/tranforming it into another form. As mapping step should operate on a single element of our data and output 0 or more possibly transformed versions of that data\n",
    "        * e.g. Taking lines from a click-through log file and outputs a tuple of user id and the page id they clicked on from weekdays\n",
    "        * By default, the \"elements\" will be passed as a single data points from your input partitions to your mapping function in Hadoop are **lines from a file**\n",
    "    * **Reduce Step** - reducing is the act of taking a bunch of grouped data and combining it in some way. This grouped data will be passed to it as key-values pairs where Hadoop will automatically bundle like values by their key into an iterable with all the values associated with that key\n",
    "        * e.g. Reducer takes in user ids as keys and an iterable of page ids they've gone to and outputs the page id that a user visits most frequently\n",
    "        * Frequently, the input to our reducers will be coming from a mapper, though this isn't strictly necessary\n",
    "* Efficieny of MapReduce\n",
    "    * At the end of both the mapping and reducing steps, the output is **written to disk into the HDFS**. This can potentially have efficiency ramifications if we are **performing many mapping and reducing operations** in sequence since writing to disk is **time consuming**\n",
    "    * This means that we'll want to condense our mapping and reducing operations, which may make our algorithms hard to understand (or use a different framework, e.g. Spark)\n",
    "* MapReduce Word Count Example:\n",
    "![word_count](word_count.png)\n",
    "    ```python\n",
    "    # wordcounts.py\n",
    "    from mrjob.job import MRJob\n",
    "    from string import punctuation\n",
    "\n",
    "    class MRWordCount(MRJob):\n",
    "        def mapper(self, _, line):\n",
    "            for word in line.split():\n",
    "                yield (word.strip(punctuation).lower(), 1)\n",
    "\n",
    "        def reducer(self, word, counts):\n",
    "            yield (word, sum(counts))\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        MRWordCount.run()\n",
    "\n",
    "    # python wordcounts.py > counts.txt\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Advanced MapReduce\n",
    "* MapReduce deals with most of the details of distributed computing (the ones that we would otherwise like to not think about) and automatically handles most of the concerns\n",
    "    * Parallelization and distribution of data and computation\n",
    "    * Fault-tolerance\n",
    "    * Resouce management\n",
    "    * Status monitoring\n",
    "* **Shuffle and Sort** - how data is re-distributed during a MapReduce job to continually take advantage of all the worker nodes\n",
    "    * **Shuffle Step** - process of deciding how and where data from mappers will go to the reducers\n",
    "    * **Sort Step** - sorts data it gets from the mappesr by key\n",
    "    * Tracking these steps:\n",
    "    ![tracking](tracking.png)\n",
    "    * **Partitioning** - each mapper takes care of any local sorting it can do to the data it's outputting and reducers take care of merging all the sorted partitions that get sent to them during the shuffle\n",
    "    ![partitioning](partitioning.png)\n",
    "    * Scarce Resources:\n",
    "        * In a distributed computing framework like Hadoop, what is one of the most scarce resources?\n",
    "            * **Bandwidth** - the time cost of transmitting data around the cluster\n",
    "        * What if we could decrease the amount of data we have to move in our shuffle and sort, or eliminate it all together?\n",
    "* **Counters** - used to keep track of events that occur over a given map or reduce step\n",
    "    * Mostly used for debugging purposes (e.g. verify that all of the folders in your file system are getting accessed correctly)\n",
    "    ```python\n",
    "    class MRCountingJob(MRJob):\n",
    "        def mapper(self, _, value):\n",
    "            self.increment_counter('group', 'counter_name', 1)\n",
    "                yield _, value\n",
    "    ```\n",
    "    * Counters can also be used to avoid using a reducer in some cases. If all you're trying to do is count by some key then a counter can help you avoid the time consuming process of the shuffle and sort when you use a reducer.\n",
    "    ```python\n",
    "    import os\n",
    "    class MRCountingJob(MRJob):\n",
    "        def mapper(self, _, line):\n",
    "            filepath = os.environ['map_input_file']\n",
    "            filename = filepath.split('/')[-1]\n",
    "            \n",
    "            for word in line.split():\n",
    "                self.increment_counter(filename, 'word_counts', 1)\n",
    "    ```\n",
    "* **Combiners** - help decrease the amount of data that needs to be passed from mappers to reducers\n",
    "    * Frequently, looks like reducers that aggregate like data on the mapper so that there is \"less\" of it to copy to the reducers\n",
    "    * The same information needs to get passed to the reducers no matter what, but combiners generally find a way to achieve this while using fewer bits to represent that information\n",
    "    * Example: illustrate unnecessary amount of data moving\n",
    "    ```python\n",
    "    from string import punctuation\n",
    "    \n",
    "    class MRWordCount(MRJob):\n",
    "        def mapper(self, _, line):\n",
    "            for word in line.split():\n",
    "                yield (word.strip(punctuation).lower(), 1)\n",
    "        \n",
    "        def combiner(self, word, counts):\n",
    "            yield (word, sum(counts))\n",
    "            \n",
    "        def reducer(self, word, counts):\n",
    "            yield (word, sum(counts))\n",
    "    # while the combiner and reducer implement the same function in this example, this is not generally the case when using a combiner\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7) Hadoop Ecosystem\n",
    "* **Apache HBase** - a mutable big data storage system on top of HDFS. HDFS is mutable and batch oriented\n",
    "* **Apache Hive** - a high level language for writing MapReduce jobs in a SQL style\n",
    "* **Apache Pig** - another high level language for writing MapReduce jobs in. Kind of like Hive, but with less structure and less familiarity from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
