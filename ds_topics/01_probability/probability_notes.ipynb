{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability Theory - the study of uncertainty \n",
    "* it is important to machine learning because the design of learning algorithms often relies on probability assumption of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Basic Concepts\n",
    "* **Probability Space**:\n",
    " * $\\Omega$ = space of possible outcomes (outcome space)\n",
    " * $F$ = space of measurable events (event space)\n",
    " * $P$ = probability measure or distribution\n",
    " * **six-sided dice example:**\n",
    "    * $\\Omega = \\{1,2,3,4,5,6\\}$\n",
    "    * events of interest: odd or even dice throw\n",
    "        * $F = \\{\\varnothing,\\{1,3,5\\},\\{2,4,6\\},\\Omega\\}$\n",
    "    * define probability distribution $P$ over $F$ (defined over events)\n",
    "        * $P(\\{1\\}) = P(\\{2\\}) = \\dots = P(\\{6\\}) = \\frac{1}{6}$\n",
    "        * $P(\\{2,4,6\\}) = P(\\{2\\}) + P(\\{4\\}) + P(\\{6\\}) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}$\n",
    "* Probability Interpretations:\n",
    " * **frequentist** - how likely an event occurs\n",
    " * **bayesian** - degree of belief in an event\n",
    "* **Random Variables**:\n",
    " * random variables are not variables, but functions that map outcomes to real values\n",
    " * allows us to abstract away from formal notion of event space, and instead use random variables to define appropriate events\n",
    " * **six-sided dice example:**\n",
    "    * let $X$ be a random variable that maps outcome $i$ to the value $i$\n",
    "    * for example: mapping event of throwing a \"one\" to the value of $1$\n",
    "    * another example: map events of throwing a odd roll to $1$ and even roll to $0$\n",
    " * indicate probability with respect to random variable notation via: \n",
    "    * $P(X = a)$ or $P_{x}(a)$ (probability of a random variable $X$ taking on the value of $a$)\n",
    "    * $Val(X)$ denotes the range of random variable $X$\n",
    "* **Distributions, Joint distributions, and Marginal distributions**:\n",
    " * what is a distribution of a variable? this is the probability of a random variable taking on certain values\n",
    " * **six-sided dice example:**\n",
    "    * if dice is fair, then the distribution of $X$: (defined over random variables)\n",
    "        * $P(X = 1) = P(X = 2) = \\dots = P(X = 6) = \\frac{1}{6}$\n",
    " * $P(X)$ denotes distribution of random variable $X$\n",
    " * **joint distributions** - refers to a distribution that is defined by more than one variable (probability is determined jointly by all variables involved)\n",
    " * **1 fair six-sided dice and 1 fair coin example:**\n",
    "    * let $X$ be a random variable defined on the outcome space of a dice throw and $Y$ be an indicator variable that takes on value of 1 if coin flip is heads and 0 if tails\n",
    "    * $P(X = a,Y = b)$ or $P_{X,Y}(a,b)$\n",
    "    * joint distribution notated: $P(X,Y)$\n",
    "    * e.g. $P(X = 1, Y = 0) = \\frac{1}{12}$\n",
    "    * e.g. $P(X = 2, Y = 1) = \\frac{1}{12}$\n",
    "    * e.g. $P(X = 3, Y = 0) = \\frac{1}{12}$\n",
    " * **marginal distribution** - refers to the probability of a distribution of random variable on its own\n",
    "    * from the example above, finding the marginal distribution of $X$ or $Y$ alone\n",
    "    * sum out all other random variables from the distribution: $P(X) = \\sum_{b\\in Val(Y)}P(X,Y = b)$ (sum out random variable $Y$ to find marginal distribution of $X$)\n",
    "* **Conditional Distributions**:\n",
    " * conditional distributions allows us to reason about uncertainty\n",
    " * allows us to specify distribution of random variable when the value of another random variable is known (or event is known to be true)\n",
    " * conditional probability of $X = a$ *given* $Y = b$ is defined as: \n",
    "     * $P(X = a \\mid  Y = b) = \\frac{P(X = a, Y = b)}{P(Y = b)}$\n",
    " * **1 fair six-sided dice example:**\n",
    "     * suppose we know that a dice throw was odd and want to know the probability of an \"one\" has been thrown\n",
    "     * let $X$ be random variable of dice throw and $Y$ be indicator variable that takes on value of $1$ if dice throws is odd\n",
    "     * $P(X = 1\\mid  Y = 1) = \\frac{P(X = 1, Y = 1)}{P(Y = 1)} = \\frac {\\frac{1}{6}}{\\frac{1}{2}} = \\frac{1}{3}$ (top: dice has 1/6 chance of throwing 1, bottom: dice has 1/2 chance of being odd)\n",
    " * when there is a distribution of random variable conditioned on several variables: \n",
    "     * $P(X = a \\mid  Y = b, Z = c) = \\frac{P(X = a, Y = b, Z = c)}{P(Y = b, Z = c)}$\n",
    " * $P(X\\mid Y = b)$ denotes distribution of random variable $X$ when $Y = b$\n",
    " * $P(X\\mid Y)$ denotes a set of distributions of $X$, one for each of the different values of $Y$\n",
    "* **Independence**:\n",
    " * independence means that the distribution of a random variable does *not* change on learning the value of an another random variable\n",
    " * in ML, we often make such assumptions about our data (e.g. training samples are assumed to be drawn independently; label of sample $i$ is assumed to be independent of the features of sample $j$ $(i \\neq j$))\n",
    " * probability of event A and B occurring:\n",
    "     * $P(A\\cap B) = P(A\\mid B) * P(B) = P(B\\mid A) * P(A)$\n",
    " * mathematically, a random variable $X$ is independent of $Y$ when \n",
    "     * $P(X) = P(X\\mid Y)$ (holds true for any values of $X$ and $Y$)\n",
    " * mathmatically, $X$ and $Y$ both are independent random variables of each other\n",
    "     * $P(X,Y) = P(X)P(Y)$\n",
    "     * $P(A\\cap B) = P(A)P(B)$\n",
    " * **conditional independence** - means if we know the value of a random variable (or set of random variables), then some other random variables will be independent of each other\n",
    "     * $X$ and $Y$ are conditionally independent given $Z$\n",
    "     * $P(X\\mid Z) = P(X\\mid Y,Z)$ or\n",
    "     * $P(X,Y\\mid Z) = P(X\\mid Z)P(Y\\mid Z)$\n",
    "     * assumption made by Naive Bayes algorithm\n",
    "         * e.g. classifying emails as spam or non-spam\n",
    "         * assumes that the probability of a word $x$ appearing in the email is conditionally independent of a word $y$ appearing given whether the email is spam or not\n",
    "         * why is this bad? loss of generality as some words invariably come in pairs\n",
    "         * why is this good? allows classification of spams quickly\n",
    "* **Chain rule and Bayes rule**:\n",
    " * **chain rule** - evalulates joint probability of some random variables (is especially useful when there are conditional independence across variables)\n",
    "     * important in study of Bayesian Networks and Probability Graphical Models\n",
    "     * $P(X_{1},X_{2},\\dots,X_{n}) = P(X_{1})P(X_{2}\\mid X_{1}) \\cdots P(X_{n}\\mid X_{1},X_{2},\\dots,X_{n-1})$\n",
    "     * $P(X_{1},X_{2},\\dots,X_{n}) = \\prod_{n=1}^k P(X_{n}\\mid X_{1},X_{2},\\dots,X_{n-1})$\n",
    "         * $P(X_1,X_2,X_3)=P(X_1)P(X_2\\mid X_1) * P(X_3\\mid X_1,X_1,X_2)$\n",
    "         * where $P(X_1,X_2) = P(X_1)P(X_2\\mid X_1)$\n",
    " * **bayes rule** - allows computation of conditional probability $P(X\\mid Y)$ from $P(Y\\mid X)$\n",
    "     * $P(X\\mid Y) = \\frac{P(Y\\mid X)P(X)}{P(Y)}$\n",
    "     * if $P(Y)$ is not given, find marginal distribution of $X$:\n",
    "     * (law of total probability) $P(Y) = \\sum_{b\\in Val(X)}P(X = a,Y) = \\sum_{b\\in Val(X)}P(Y\\mid X = a)P(X = a)$ \n",
    " * bayes rule for multiple random variables:\n",
    "     * $P(X,Y\\mid Z) = \\frac{P(Z\\mid X,Y)P(X,Y)}{P(Z)} = \\frac{P(Y,Z\\mid X)P(X)}{P(Z)}$\n",
    "     * $P(X\\mid Y,Z) = \\frac{P(Y\\mid X,Z)P(X,Z)}{P(Y,Z)} = \\frac{P(Y\\mid X,Z)P(X\\mid Z)P(Z)}{P(Y\\mid Z)P(Z)} = \\frac{P(Y\\mid X,Z)P(X\\mid Z)}{P(Y\\mid Z)}$\n",
    "* **Sets**:\n",
    " * **complement (e.g. $A^c$ or $A'$)** - denotes all elements that are not in the complementary set \n",
    " * **union (e.g. $A \\cup B$)** - denotes all elements that are in A or B\n",
    " * **intersection (e.g. $A \\cap B$ = $A,B$ = $A\\&B$)** - denotes all elements in A and B\n",
    "     * disjoint - two sets are disjoint if their intersection is null $\\varnothing$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Defining Probability Distributions\n",
    "* **Discrete Distribution (Probability Mass Function)**:\n",
    " * discrete - random variable of the underlying distribution can only take on finite many different values (outcome space is finite)\n",
    " * **PMF** - enumeration the probability of random variable taking on each of the possible values\n",
    "     * divides up a unit mass (total probability) and places them on different values a random variable can take\n",
    "     * can extend to joint distributions and conditional distributions\n",
    "* **Continuous Distribution (Probability Density Function)**:\n",
    " * continuous - random variable of the underlying distribution can take on infinitely many different values (outcome space is infnite)\n",
    " * cannot place a non-zero amount of mass on each of the values since it will add up to infinity (violating requirement of total probability to sum to 1)\n",
    " * **PDF**, $f$, is a non-negative, integrable function that defines the continuous distribution such that:\n",
    "     * $\\int_{Val(X)} f(x)dx = 1$\n",
    " * compute probability of a random variable $X$ distributed to a PDF $f$:\n",
    "     * $P(a \\leq X \\leq b) = \\int_{a}^{b} f(x)dx$\n",
    " * uniform distribution example: \n",
    "     * let random variable $X$ be uniformly distributed in the range $[0,1]$:\n",
    "     * $f(x) = $$\\begin{cases} \n",
    "        1  & \\text{if $0 \\leq x \\leq 1$} \\\\\n",
    "        0 & \\text{$otherwise$}\n",
    "        \\end{cases}$\n",
    "     * more generally, if $X$ is distributed uniformly over the range [a,b], the PDF:\n",
    "     * $f(x) = $$\\begin{cases} \n",
    "        \\frac{1}{b-a}  & \\text{if $a \\leq x \\leq b$} \\\\\n",
    "        0 & \\text{$otherwise$}\n",
    "        \\end{cases}$\n",
    " * **cumulative distribution function** - gives the probability of a random variable being smaller than some value\n",
    "    * PDF -> CDF: $F(b) = P(X \\leq b) = \\int_{-\\infty}^{b} f(x)dx$\n",
    "    * CDF -> PDF: $F'(x) = \\frac{d/dx}F(x) = f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Expectations and Variance\n",
    "* **Expectations: (aka mean, expected value, first moment)**\n",
    " * $E(X) = \\sum_{a\\in Val(X)}aP(X = a)$ or $E(X) = \\int_{a\\in Val(X)}xf(x)dx$\n",
    " * $E(X) = x_1p_1 + x_2p_2 + \\cdots +  x_kp_k$ or $E(X) = \\int_{-\\infty}^{\\infty}xf(x)dx$\n",
    " * example: let $X$ be the outcome of rolling a fair dice\n",
    "     * expectation of $X$ is\n",
    "     * $E(X) = (1)\\frac{1}{6} + (2)\\frac{1}{6} + \\cdots + (6)\\frac{1}{6} = (3)\\frac{1}{2}$\n",
    " * **linearity of expectations theorem**:\n",
    "     * sums of random variables\n",
    "         * $E(X_{1}+X_{2}+\\cdots+X_{n}) = E(X_{1})+E(X_{2})+\\cdots+E(X_{n})$\n",
    "         * no restrictions on whether the random variables are independent or not\n",
    "     * on products of random variables when independent then:\n",
    "         * $E(XY) = E(X)E(Y)$\n",
    "* **Variance: (aka spread, second moment)**\n",
    " * variance of a distribution is a measure of the \"spread\" of a distribution\n",
    " * mean squared distance between random variable $X$ and mean $\\mu$\n",
    "     * $\\begin{align} Var(X) & = \\sigma^2 \\\\\n",
    "         & = E\\big[(X-\\mu)^2\\big] \\\\\n",
    "         & = E\\big[(X-E(X))^2\\big] \\\\\n",
    "         & = E\\big[X^2-2XE[X] + (E[X])^2\\big] \\\\\n",
    "         & = E[X^2]-2E[X]E[X] + (E[X])^2 \\\\\n",
    "         & = E[X^2]-(E[X])^2\n",
    "     \\end{align}$\n",
    "     * $Var(X) = E[(X-\\mu)^2] = \\sum_{i=1}^k p_i * (x_i-\\mu)^2$\n",
    "     * $Var(X) = \\sigma^2 = \\int (x-\\mu)^2f(x)dx$\n",
    " * **standard deviation** = $\\sigma = \\sqrt{Var(X)}$\n",
    " * variance is not a linear function of a random variable $X$\n",
    "     * $Var(aX+b) = a^2Var(X)$\n",
    " * if random variables $X$ and $Y$ are independent:\n",
    "     * $Var(X+Y)=Var(X)Var(Y)$ if $X\\bot Y$\n",
    " * **covariance** - measures how close two random variables are closely related\n",
    "     * $\\begin{align} Cov(X,Y) & = E((X-E(X))(Y-E(Y))) \\\\\n",
    "         & = E[(X-\\mu_x)(Y-\\mu_y)] \\\\\n",
    "         & = E(XY)-E(X)E(Y) \\\\\n",
    "         & = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n",
    "     \\end{align}$\n",
    " * **correlation** - normalized version of correlation coefficient\n",
    "     * also measures strength of linear relationship between two variables\n",
    "     * $\\rho_{X,Y} = corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} = \\frac{E[(X-\\mu_x)(Y-\\mu_y)]}{\\sigma_x\\sigma_y}$\n",
    "     * $\\tau_{X,Y} = \\frac{ \\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) } { \\sqrt{ \\sum_{i=1}^n(x_i - \\bar{x})^2 \\sum_{i=1}^n(y_i - \\bar{y})^2} }$\n",
    "     * correlation measures strength of relationship between two variables but doesn't capture some qualities:\n",
    "         * e.g. anscombe's quartet\n",
    "         * reflects noisiness and direction\n",
    "         * not slope and non-linearly qualities \n",
    " * **scatterplot matrix** - captures the $Var(X_1)$, $Cov(X_1,X_2)$, $Corr(X_1,X_2)$\n",
    "      * let $X$ be data matrix $m$ features: $X=[X_1,X_2,\\dots,X_m]$\n",
    "      * variances on diagonal\n",
    "      * compact representation of all covariances\n",
    "      * correlation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Statistical Distributions\n",
    "* **Bernoulli$(p)$**:\n",
    " * description: discrete probability distribution for a bernoulli trial (1 for success probability of p and 0 for failure)\n",
    " * $P(X) = p^x(1-p)^{1-x}$\n",
    " * PMF = $P[success] = p$, $P[failure] = 1-p$\n",
    " * mean: $E[x] = p$\n",
    " * variance: $Var(x) = p(1-p)$\n",
    " * example: $Bernoulli(p)$ = single coin flip turns out to be heads\n",
    " * a random variable distribution that takes two possible values {0,1}\n",
    " * has a single parameter, $p$, to be $P(X=1)$\n",
    " * often indicates whether a trial was successful or not\n",
    " * e.g. classification task using logistic regression has assumptions that the label are Bernoulli distributed given the features\n",
    "* **Binomial$(n,p)$**:\n",
    " * description: discrete probability distribution of obtaining exactly $p$ successes out of $n$ trials\n",
    " * PMF: $(^n_k)p^k(1-p)^{n-k}$ for $0 \\leq k \\leq n$\n",
    " * mean: $np$\n",
    " * variance: $npq$ or $np(1-p)$\n",
    " * example: $Binomial(100,p)$ = # of coin flips out of 100 that turn out of to be heads\n",
    "* **Geometric$(p)$**:\n",
    " * description: discrete probability of some number $(X)$ of Bernoulli trials needed to get one success. It also refers to probability of $(X-1)$ failures before the first success\n",
    " * PMF: $p(1-p)^{k-1}$ for $k = 1,2,\\dots$\n",
    " * mean: $\\frac{1}{p}$\n",
    " * variance: $\\frac{1-p}{p^2}$\n",
    " * example: $Geometric(p)$ = # of trials until coin flip turns out to be heads\n",
    "* **Hypergeometric**:\n",
    " * description: discrete probability distribution that describes probability of $k$ successes in $n$ draws, without replacement\n",
    " * the hypergeometric test uses the hypergeometric distribution to calculate the statistical significance of having drawn $k$ successes in $n$ total draws\n",
    "     * example: urn with two types of marbles (red and green) and draw marbles without replacement\n",
    "     * define drawing a green marble as a success and drawing a red as a failure (analogous to binomial distribution)\n",
    "     * did i draw the **expected** number of green marbles?\n",
    "     * this is not a binomial distribution since the probability of success on each trial is not the same\n",
    " * models the game - texas hold 'em\n",
    "* **Poisson$(\\lambda)$**:\n",
    " * description: after the mean of an event happening per unit time is observed, obtain the discrete probability of $n$ events happening\n",
    " * PMF: $P(X=k)=\\frac{exp(-\\lambda)\\lambda^k}{k!}$ for $k = 0,1,2,\\dots$\n",
    " * mean: $\\lambda$\n",
    " * variance: $\\lambda$\n",
    " * example: $Poisson(\\lambda=10)$ = # of taxis passing a street corner in a given hour (on avg 10/hr)\n",
    " * deals with the arrival of events\n",
    "     * measures the probability of the number of events happening over a fixed period of time\n",
    "     * assumes a fixed average rate of occurrence\n",
    "     * assumes that the events take place independently of the time since the last event\n",
    " * is parameterized by average arrival rate $\\lambda$\n",
    " * mean of poisson random variable is $\\lambda$\n",
    " * variance is also $\\lambda$\n",
    "* **Uniform$(a,b)$**:\n",
    " * PDF: $\\frac{1}{b-a}\\forall x \\in (a,b)$\n",
    " * mean: $\\frac{a+b}{2}$\n",
    " * variance: $\\frac{(b-a)^2}{12}$\n",
    " * example: $Uniform(0,360)$ = Degrees between hour hand and minute hand\n",
    "* **Gaussian$(\\mu,\\sigma^2)$**: (aka normal distribution)\n",
    " * description: most widely distribution for continuous variables\n",
    " * PDF: $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\big(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\big)$ for $x \\in (-\\infty, \\infty)$\n",
    " * mean: $\\mu$\n",
    " * sigma: $\\sigma^2$\n",
    " * example: $Gaussian(100,10)$ = IQ Score\n",
    " * inverse of variance is called **precision** $(\\tau = \\frac{1}{\\sigma^2})$\n",
    " * approximates binomial distribution when the number of experiments is large\n",
    " * approximates poisson distribution when the average arrival rate is high\n",
    " * related to Law of Large Numbers\n",
    " * often assume noise in the system is Gaussian distributed\n",
    " * parameterized by mean $\\mu$ and variance $\\sigma^2$\n",
    "* **Exponential$(\\lambda)$**:\n",
    " * description: models the time between events for a poisson process (subcase of gamma distribution)\n",
    " * PDF: $\\lambda e^{-\\lambda x}$ $x \\geq 0, \\lambda > 0, x \\in (0,\\infty)$\n",
    " * mean: $\\frac{1}{\\lambda}$\n",
    " * variance: $\\frac{1}{\\lambda^2}$\n",
    " * example: $Exponential(\\lambda=10)$ = time until taxi will pass street corner\n",
    " * it is governed by a rate parameter $\\lambda$\n",
    "* **Beta$(\\alpha,\\beta)$**:\n",
    " * description: density function that is versatile in representing outcomes like proportions or probabilities. it works on a space between 0 and 1\n",
    " * PDF: $P(x\\mid \\alpha,\\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}$\n",
    "     * where $B$ is the beta function: $B(\\alpha,\\beta) = \\int_0^1 t^{\\alpha-1}(1-t)^{\\beta-1}dt$\n",
    " * mean: $\\frac{a}{a+b}$\n",
    " * variance: $\\frac{ab}{(a+b)^2(a+b+1)}$\n",
    " * example: useful in estimating success\n",
    " * uniform is actually a special case of Beta ($Uniform(0,1) = Beta(1,1)$)\n",
    " * there are two parameters that work together to determine if the distribution has a mode in the interior of unit interval and whether it is symmetrical\n",
    "* **Gamma$(k,\\theta)$**:\n",
    " * example: time until $n$ evnets in a processs with no memory\n",
    " * gamma is the sum of $k$ Exponentials\n",
    "* **Chi-square**:\n",
    " * example: useful for goodness of fit tests\n",
    " * chi-square is sum of squares of $k$ independent Gaussian random variables\n",
    "* **F-Distribution**:\n",
    " * example: useful for some statistical tests\n",
    " * f-dist is the ratio of two normalized chi-squared distributed random variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there are 10 lottery balls and we want to draw them all, how many possible orderings are there?\n",
    "import math\n",
    "math.factorial(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "# how many different pairs are there for afternoon sprints with 24 students? (order doesn't matter / combinations)\n",
    "from itertools import combinations\n",
    "spair = list(combinations(\"ABCDEFGHIJKLMNOPQRSTUVXY\",2))\n",
    "print len(spair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "from math import factorial\n",
    "def comb(n, k):\n",
    "    return factorial(n) / (factorial(k) * factorial(n-k))\n",
    "print int(comb(24,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permutations (order does matter)\n",
    "# on a baseball team of 20 players, how many different batting orders are there?\n",
    "from itertools import permutations\n",
    "len(list(permutations(\"ABCDEFGHIJKLMNOPQRST\",2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    }
   ],
   "source": [
    "from math import factorial\n",
    "def perm(n, k):\n",
    "    return factorial(n) / factorial(n-k)\n",
    "print int(perm(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
