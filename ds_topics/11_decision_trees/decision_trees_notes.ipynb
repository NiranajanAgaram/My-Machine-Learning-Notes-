{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (Non-parametric Model)\n",
    "\n",
    "1) Decision Tree Basics and Algorithm\n",
    "![decision_trees](decision_trees.png)\n",
    "* Tree Terminology:\n",
    "    * Nodes\n",
    "        * Root\n",
    "        * Leaves\n",
    "    * Edges\n",
    "    * Trees are a recursive structure - from any point, we can pick a node, pretend it's root, and treat it and what's below it as a *subtree*\n",
    "* Decision Tree split logic (binary):\n",
    "    * For a categorical variable, choose either value or not value (e.g. sunny or not sunny)\n",
    "    * For a continuous variable, choose a threshold and do $>$ or $\\leq$ the value (e.g. temperature $<$ 75 or $\\geq$ 75)\n",
    "* Building Trees:\n",
    "    * For every item in the dataset is in the same class or there is no feature left to split the data:\n",
    "        * return a leaf node with the class label\n",
    "    * else:\n",
    "        * find the best feature and value to split the data\n",
    "        * create a node\n",
    "        * split the dataset using the best split from above\n",
    "        * for each split:\n",
    "            * call BuildTree() and add the result as a child of the node\n",
    "        * return node\n",
    "* Decision Tree Classification Splitting Steps:\n",
    "    1. calculate the information gain for every possible split\n",
    "    2. select the split that has the highest information gain\n",
    "* Decision Tree Regression Splitting Steps:\n",
    "![decision_tree_reg_1](dt_regression_1.png)\n",
    "![decision_tree_reg_2](dt_regression_2.png)\n",
    "    * responses are real values, so we can't use cross-entropy or Gini-index\n",
    "    * can average the values at each leaf node to predict a continuous value\n",
    "    * can also use a combination of decision trees and linear regression on the leaf nodes (modelling the trees)\n",
    "    * when to stop? best practice to grow a very large tree and prune backwards\n",
    "    * choose the best splits using residual sum of squares (RSS) against the mean value of each leaf\n",
    "        * equation: $\\sum_{m=1}^{|T|}\\sum_{x_i \\in R_m}(y_i-\\hat{y}_{R_m})^2 + \\alpha |T|$\n",
    "            * let $|T| =$ # of terminal nodes\n",
    "            * then, for any penalty term, $\\alpha$, we have a subtree $T$ which minimizes the equation above\n",
    "            * cross-validate as usual to choose $\\alpha$ and it's corresponding tree\n",
    "            * $\\alpha$ is like $\\lambda$ for Lasso/Ridge to reduce high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Measure of how good the split is:\n",
    "![measure_split](https://static.commonlounge.com/fp/original/Gy8ohommQMaImi6o8tOF_wHwyHdHBss5IsrY1507278630)\n",
    "1. **Entropy/Cross-entropy** - measure of the amount of disorder in a set\n",
    "![entropy](entropy.png)\n",
    "    * equation: $H(X) = \\sum_{i=1}^n P(x_i)I(x_i) = -\\sum_{i=1}^n P(x_i)log_b P(x_i)$\n",
    "        * $P(x_i)$ - the percent of the group that belongs to a given class\n",
    "    * low entropy: if a set has all the same labels, that's pretty ordered\n",
    "    * high entropy: if a set has a good mix of labels, that's not very ordered\n",
    "    * numerically similar to Gini index\n",
    "2. **Information Gain** - decides which feature to split on at each step in building the tree\n",
    "    * equation: $I_G(X,a)=H(X)-H(X|a)$\n",
    "        * $I_G(X,a)$ - Information Gain\n",
    "        * $H(X)$ - Entropy (parent)\n",
    "        * $H(X|a)$ - Weighted Sum of Entropy (children)\n",
    "    * objective: to create splits that minimize entropy in each side of split\n",
    "        * if the splits on the boundary between classes is good, then the splits have more predictive power\n",
    "    * compare entropy of parent and children nodes:\n",
    "        * measure the entropy of the parent\n",
    "        * measure the entropy of the children of the proposed split and the difference to parent\n",
    "        * maximize entropy(parents - mean(entropy(children))\n",
    "    * splitting examples:\n",
    "        * Some Information Gain\n",
    "            ![some_info](split_1.png)\n",
    "            * Parent's entropy = $-(\\frac{1}{2}log_2\\frac{1}{2})-(\\frac{1}{2}log_2\\frac{1}{2})=1$\n",
    "            * Left child's entropy = $-(\\frac{2}{3}log_2\\frac{2}{3})-(\\frac{1}{3}log_2\\frac{1}{3})=0.918$\n",
    "            * Right child's entropy = $-(1log_21)=0$\n",
    "            * Information gain from splitting: $X = 1 - (\\frac{3}{4}* .918 + \\frac{1}{4}* 0) = 0.311$\n",
    "        * No Information Gain\n",
    "            ![no_info](split_2.png)\n",
    "            * Parent's entropy = $-(\\frac{1}{2}log_2\\frac{1}{2})-(\\frac{1}{2}log_2\\frac{1}{2})=1$\n",
    "            * Left child's entropy = $-(\\frac{1}{2}log_2\\frac{1}{2})-(\\frac{1}{2}log_2\\frac{1}{2})=1$\n",
    "            * Right child's entropy = $-(\\frac{1}{2}log_2\\frac{1}{2})-(\\frac{1}{2}log_2\\frac{1}{2})=1$\n",
    "            * Information gain from splitting: $Z = 1 - (\\frac{1}{2}* 1 + \\frac{1}{2}* 1) = 0$\n",
    "        * Max Information Gain (Perfect Split)\n",
    "            ![max_info](split_3.png)\n",
    "            * Parent's entropy = $-(\\frac{1}{2}log_2\\frac{1}{2})-(\\frac{1}{2}log_2\\frac{1}{2})=1$\n",
    "            * Left child's entropy = $-(1log_21)=0$\n",
    "            * Right child's entropy = $-(1log_21)=0$\n",
    "            * Information gain from splitting: $Y = 1 - (\\frac{1}{2}* 0 + \\frac{1}{2}* 0) = 1$\n",
    "3. **Gini Index/Impurity** - measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset\n",
    "    * equation: $G(X) = \\sum_{i=1}^n P(x_i)(1-P(x_i)) = 1 -\\sum_{i=1}^n P(x_i)^2$\n",
    "    * Steps for Gini Impurity:\n",
    "        1. Take random element from the set\n",
    "        2. Label it randomly according to the distribution of labels in the set\n",
    "        3. What is the probability that is it labeled incorrectly?\n",
    "    * small gini: if the class distribution all close to 0 or 1\n",
    "    * large gini: if the class distributions are more mixed, this takes on a larger value\n",
    "    * numerically similar to *cross-entropy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Optimizing Trees / Hyperparameters\n",
    "* **Pruning** - decreases variance of the model by stopping the decision tree algorithm early\n",
    "    * (-) Decision trees have high variance (Prone to overfitting)\n",
    "    * **Pre-pruning**\n",
    "        * **Leaf Size** - stop when there's a few data points at a node\n",
    "        * **Depth** - stop when a tree gets too deep\n",
    "        * **Class Mix** - stop when some percent of data points are the same class\n",
    "        * **Error Reduction/Threshold** - stop when the information gains are too little\n",
    "    * **Post-pruning**\n",
    "        * **cut off** - built full tree, then cut off some leaves / merge two leaves up to their parent, thus creating a new leaf\n",
    "    * Pruning pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Advantages/Disadvantages and Intuitions\n",
    "* Advantages:\n",
    "    * (+) easily interpretable\n",
    "    * (+) can model complex phenomenon / non-linear\n",
    "        * no assumption that the structure of the model is fixed\n",
    "        * feature interactions\n",
    "    * (+) computationally cheap to predict\n",
    "    * (+) can handle irrelevant features, missing values, and outliers well\n",
    "    * (+) very extensible (e.g. ties to bagging, random forest, boosting)\n",
    "* Disadvantages:\n",
    "    * (-) computationally expensive to train\n",
    "    * (-) greedy algorithm (gets stuck in local optima)\n",
    "    * (-) very high variance (super easy to overfit)\n",
    "* Intuitions for Decision Trees:\n",
    "    * Always prune to avoid overfitting\n",
    "    * Most likely will extend into using bagging, random forests, and boosting\n",
    "    * Gini or cross-entropy are both fine and not that different\n",
    "    * Sometimes fully splitting categorical features is preferred, but generally binary splits are fine\n",
    "    * Sklearn library for Decision Trees:\n",
    "        * (-) Doesn't support missing values\n",
    "        * Gini index is default, entropy is an option\n",
    "        * Pruning well supported (max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes)\n",
    "        * Does binary splits (you need modify categorical variables to be binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Decision Tree Variants\n",
    "1. **Iterative Dichotomiser 3 (ID3)**\n",
    "    * designed for **only** categorical features\n",
    "    * splits categorical features completely\n",
    "    * uses entropy and information gain to pick the best split\n",
    "2. **Classification and Regression Tree (CART)**\n",
    "    * handles both categorical and continuous data\n",
    "    * always uses binary splits\n",
    "    * uses gini impurity to pick the best split\n",
    "3. **C4.5**\n",
    "    * handles continuous data\n",
    "    * implements pruning to reduce overfitting\n",
    "4. **C5.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
