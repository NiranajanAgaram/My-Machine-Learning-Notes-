{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Non-Negative Matrix Factorization (Interpretable Topic Modeling)\n",
    "\n",
    "* Objectives:\n",
    "    * Motivation of Topic Modeling\n",
    "    * Thinking About Topics\n",
    "    * Topic Analysis Assumptions\n",
    "    * Math For NMF\n",
    "    * Topics as Latent Feature Bases\n",
    "    * Identifying Topics\n",
    "    * NMF Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Topic Modeling** - attempts to extract an underlying structure from the data as a form of unsupervised learning\n",
    "* To discover an underlying set of \"topics\" that describe high level ideas about the data well\n",
    "* Example: Consider having the term-frequency matrix for a corpus of documents, all coming from some sort of related source (e.g. articles from a newspaper)\n",
    "    * In trying to discover topics, or latent features, in these data, we might expect to find such overarching articles as: \"Sports\", \"International\", \"Arts and Leisure\", etc.\n",
    "* **Topic Analysis Assumptions**:\n",
    "    1. The observations are well described by underlying topics\n",
    "        * e.g. all \"Sports\" articles will have similar sport-sy words. In math, each topic has a corresponding distribution of words: $$tf(word \\mid topic)$$\n",
    "    2. The words in a document can be represented by an appropriate combination of topics\n",
    "        * e.g. an article about FIFA could be represented by the topics: \"International\" and \"Sports\". Math: $$tf(word \\mid doc)=\\sum_{t \\in T} tf(word \\mid topic) \\times w(t \\mid doc)$$ where $T$ is the set of topics and $w$ is some positive weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Non-Negative Matrix Factorization (NMF)** - a group of algorithms that takes a large dimensional matrix and factors into 2 (or more) smaller dimensional matrics\n",
    "* Another way to mathematically express the topic modeling assumptions is through the equation: $$W \\times H = V$$ $$(\\text{m} \\times \\mathbf{r}) \\times (\\mathbf{r} \\times \\text{n}) = (\\text{m} \\times \\text{n})$$ where each entry: $$v_{ij} \\geq 0$$ $$w_{ij} \\geq 0$$ $$h_{ij} \\geq 0$$\n",
    "![nmf](https://upload.wikimedia.org/wikipedia/commons/f/f9/NMF.png)\n",
    "    * $V$ can be approximated by the dot product of two matrices (aka factorization)\n",
    "    * Cannot be solved analytically, so approximated numerically\n",
    "    * $\\mathbf{r}$ set by user ($\\mathbf{r} < \\text{min(m,n)}$)\n",
    "    * Can easily rationalize that when the internal dimension, $\\mathbf{r}\\geq m$ can perfectly recreate $V$    \n",
    "    * notice the columns of $V$ are **sum of columns of $W$ weighted by corresponding column in $h_i$**: $$v_i = W \\times h_i$$\n",
    "    * NMF is a **relatively new way** of reducing dimensionality of data into linear combination of bases\n",
    "        * Columns of $W$ as basis\n",
    "        * Weighted by $h_i$\n",
    "    * **Non-negativity constraint** - unlike the other decompositions models\n",
    "* What happens when this dimension, $\\mathbf{r}<m$?\n",
    "    * In looking at the dimensions of the $W$ matrix from our decomposition, we notice that the number of rows remain the same, as we would expect\n",
    "    * Thus, the rows of $W$ must represent some information about each row in $V$\n",
    "    * When we use a smaller number of dimensions to represent our data in $W$ when ($k<m$), we will necessarily find that some sort of data compression is happening\n",
    "    * Or, in math, we are projecting our data onto a lower dimensioned basis\n",
    "* **Topics as Latent Feature Bases** - the values of each row in the latent feature space corresponds to their strength with the associated topic\n",
    "    * It, somewhat magically, turns out that when we conduct a **factorization** of this nature each of bases in the **lower** dimensional representation of $V$, aka $W$, can be viewed as a **latent feature**\n",
    "    * These **latent features** are discovered as somewhat of a side-effect of projecting into a smaller number of dimensions, of performing some sort of compression\n",
    "    * Example $W$ (m $\\times$ r) Matrix:\n",
    "\n",
    "|  Topic 1 |  Topic 2 | $\\cdots$ | Topic $r$ |              |\n",
    "|:--------:|:--------:|:--------:|:---------:|:------------:|\n",
    "| 0.3      | 5.1      | $\\cdots$ | 1.2       | **Document 1**   |\n",
    "| 9.76     | 0.04     | $\\cdots$ | 2.7       | **Document 2**   |\n",
    "| $\\vdots$ | $\\vdots$ | $\\cdots$ | $\\vdots$  | $\\vdots$     |\n",
    "| $\\vdots$ | $\\vdots$ | $\\cdots$ | $\\vdots$  | $\\vdots$     |\n",
    "| 0.06     | 0.3      | $\\cdots$ | 0.001     | **Document $m$** |\n",
    "        \n",
    "* **Identifying Topics** - figuring out what topics these latent feature bases correspond to\n",
    "    * This is an unsupervised approach (e.g. not explicitly telling it to look for \"sports\" word)\n",
    "    * To put \"labels\" on the latent features and identify them as a topics we can do one of two things:\n",
    "        1. Look at the obeservations that load heavily on each topic and manually inspect them, trying to identify some commonalities (aka **latent features**)\n",
    "        2. Inspect the $H$ matrix and see what features contribute to each topic\n",
    "    * Inspection of dimensions of $H$ matrix:\n",
    "        * The number of columns in $V$, $m$, is the same as in $H$\n",
    "        * The columns in $H$ represents some information about the columns in $V$\n",
    "        * More specifically, viewing the features in $V$ as being the basis for the latent topics\n",
    "    * Example $H$ (r $\\times$ n) Matrix:\n",
    "\n",
    "| Feature 1 | Feature 2 | $\\cdots$ | Feature $n$ |           |\n",
    "|:---------:|:---------:|:--------:|:-----------:|:---------:|\n",
    "| 0.3       | 5.1       | $\\cdots$ | 4.2         | **Topic 1**   |\n",
    "| 10.3      | 1.07      | $\\cdots$ | 0.08        | **Topic 2**   |\n",
    "| $\\vdots$  | $\\vdots$  | $\\cdots$ | $\\vdots$    | $\\vdots$  |\n",
    "| $\\vdots$  | $\\vdots$  | $\\cdots$ | $\\vdots$    | $\\vdots$  |\n",
    "| 2.03      | 0.3       | $\\cdots$ | 0.001       | **Topic $r$** |\n",
    "\n",
    "| \"president\" | \"coach\" | $\\cdots$ | \"team\" |           |\n",
    "|:---------:|:---------:|:--------:|:-----------:|:---------:|\n",
    "| 0.3       | 5.1       | $\\cdots$ | 4.2         | **Topic 1**   |\n",
    "| 10.3      | 1.07      | $\\cdots$ | 0.08        | **Topic 2**   |\n",
    "| $\\vdots$  | $\\vdots$  | $\\cdots$ | $\\vdots$    | $\\vdots$  |\n",
    "| $\\vdots$  | $\\vdots$  | $\\cdots$ | $\\vdots$    | $\\vdots$  |\n",
    "| 2.03      | 0.3       | $\\cdots$ | 0.001       | **Topic $r$** |\n",
    "* Predicting Latent Topics (From example above) From $H$ Matrix:\n",
    "    * Topic 1 $\\rightarrow$ \"Sports\" category?\n",
    "    * Topic 2 $\\rightarrow$ \"Politics\" category?\n",
    "* **Choosing $r$** from NMF\n",
    "    * Unfortunately, choosing $r$ is more of an art than a science\n",
    "    * Try examining how \"good\" the approximation of $W \\times H$ for $V$ and find the **smallest $r$** that makes it suitably small\n",
    "    * However, $r$, is likely going to be chosen based on intuition that is derived from inspecting the topics and possibly from some domain knowledge\n",
    "* Popular Applications of NMF\n",
    "    * Computer Visioning\n",
    "    ![nmf_vision](nmf_vision.png)\n",
    "        * identify or classify objects\n",
    "        * generally reducing feature space of images\n",
    "    * Document Clustering\n",
    "    ![nmf_doc_cluster](nmf_doc_cluster.png)\n",
    "    * Recommender Systems\n",
    "    ![nmf_rec_sys](nmf_rec_sys.png)\n",
    "* Document Clustering with NMF\n",
    "    * Example: 500 documents and 10,000 words\n",
    "    ![nmf_dim_doc_cluster](nmf_dim_doc_cluster.png)\n",
    "        * $W$: (words $\\times$ latent factors) - think of column of $W$ as **document archetype** where the higher the word's cell value, the higher the word's rank for that latent feature\n",
    "        * $H$: (latent factors $\\times$ documents) - think of column of $H$ as the **original document**, where cell value is document's rank for a particular latent feature\n",
    "        * $V$: (words $\\times$ documents) - think of **reconstituting a particular document** as linear combination of \"document archetypes\" weighed by how important they are\n",
    "* Reconstructing $V$ Matrix:\n",
    "    * $V$ is approximated by the inner product of $W$ and $H$\n",
    "    * How do we reconstruct **only one cell of $V$ (or $X$)**?\n",
    "        * Inner product of $W$ and $H$'s correct column and row\n",
    "        ![reconstructing_v](reconstructing_v.png)\n",
    "    * How do we find the non-negative matrices ($W$,$H$) that approximates $V$?\n",
    "        * With PCA/SVD, there is a **closed form solution** for finding those **factorizations**\n",
    "        * However, with NMF, there is no such closed form solution, but we can use biconvex optimization via **Alternating Least Squares (ALS)**\n",
    "* NMF Algorithm\n",
    "    * Minimize: $$\\Vert V-WH \\Vert^2$$ with respect to $W$ and $H$ and subject to $W, H \\geq 0$\n",
    "    * **Alternating Least Squares (ALS)** - take advantage of the biconvexivity by alternating matrix, $W$ or $H$, that is treated as stationary, solving for the other's optimal values, and then clipping all the negative values in that solution to 0\n",
    "        * Finding $W$ and $H$:\n",
    "            * This problem is a biconvex optimization issue where it's convex in either $W$ or $H$, but not both.\n",
    "            * There is a straightforward way to brute force an approximate solution in this case\n",
    "            * While there is no closed form solution for $W$ and $H$, if we hold one of these matrices constant there **is a closed form optimum** for the **other**\n",
    "        * ALS Steps:\n",
    "            1. Randomly initialize $W$ and $H$ to the **appropriate shapes of matrices**\n",
    "            2. Repeat the following:\n",
    "                * Holding $W$ fixed, update $H$ by minimizing sum of squared errors (Ensure all $H>0$)\n",
    "                * Holding $H$ fixed, update $W$ by minimizing sum of squared errors (Ensure all $W>0$)\n",
    "            3. Stop when some threshold is met\n",
    "                * Decrease in RMSE\n",
    "                * \\# of iterations\n",
    "        * Pseudo-code for ALS:\n",
    "            1. Initialize $W$ to small, positive random values\n",
    "            2. For max number of iterations:\n",
    "                1. Find the least squares solution to $X=W\\times H$ with respect to $H$\n",
    "                2. Clip negative values in $H$ to 0 ($H<0=0$)\n",
    "                3. Find the least squares solution to $X=W\\times H$ with respect to $W$\n",
    "                4. Clip negative values in $H$ to 0 ($W<0=0$)\n",
    "        * ALS Pros and Cons:\n",
    "            * (+) Fast algorithm\n",
    "            * (+) Works well in practice\n",
    "            * (-) Non-negativity enforced in an ad hoc way\n",
    "            * (-) Not guaranteed to find a local minimum (much less global)\n",
    "            * (-) No convergence theory (e.g. the function $y = \\frac{1}{x}$ converges to zero as $x$ increases)\n",
    "    * **Multiplicate Update** - solving optimization problem with gradient descent using a cost function and reducing the gradient descent updates by choosing correct step sizes\n",
    "        * Cost Function - defining the cost function for this optimization problem\n",
    "            1. Define how much we're missing in our approximation of $V$ ($WH$ as the reconstruction error)\n",
    "            2. Use generalization of Euclidean distance on matrices, also known as **Frobenius norm** on the reconstruction error to quantify how well we are approximating $V$\n",
    "        * Gradient Descent For NMF:\n",
    "            1. Let the Frobenius norm of the reconstruction error be the quantity that we are trying to minimize: $$min_{W,H} \\Vert V-WH \\Vert^2$$\n",
    "            2. From this, with a little bit of matrix calculus, we can determine that the update rules for a single entry in $W$ and $H$ are: $$W_{i,a} \\leftarrow W_{i,a}+v_{i,a}[(VH^T)_{i,a}-(WHH^T)_{i,a}]$$ $$H_{a,\\mu} \\leftarrow H_{a,\\mu}+v_{i,a}[(W^TV)_{a,\\mu}-(W^TWH)_{a,\\mu}]$$\n",
    "            3. If we are clever about the value that we choose as our step size, $v$, we can reduce those gradient descent updates to what we known as the multiplicate update rules: $$v_{i,a} = \\frac{W_{i,a}}{(WHH^T)_{i,a}}$$ $$v_{a,\\mu} = \\frac{H_{a,\\mu}}{(W^TWH)_{a,\\mu}}$$\n",
    "            4. Rewrite the gradient descent updates as: $$W_{i,a} \\leftarrow W_{i,a}\\frac{(VH^T)_{i,a}}{(WHH^T)_{i,a}}$$ $$H_{a,\\mu} \\leftarrow H_{a,\\mu}\\frac{(W^TV)_{a,\\mu}}{(W^TWH)_{a,\\mu}}$$\n",
    "            5. These updates will be iteratively performed from $W$ and $H$ initialized with random, small, positive values\n",
    "        * Multiplicative Update Rules Steps:\n",
    "            1. Start with random $W$ and $H$\n",
    "            2. Repeatedly adjust $W$ and $H$ to make RMSE smaller\n",
    "                * $W_{i,a} \\leftarrow W_{i,a}\\frac{(VH^T)_{i,a}}{(WHH^T)_{i,a}}$\n",
    "                * $H_{a,\\mu} \\leftarrow H_{a,\\mu}\\frac{(W^TV)_{a,\\mu}}{(W^TWH)_{a,\\mu}}$\n",
    "                * Lee and Seung's popular \"multipicative update rules\" offers compromise between speed and implementation\n",
    "                * Gradient descent is simple but can be slow. Also, convergence sensitive to choice of step size\n",
    "            3. Stop when some threshold is met\n",
    "                * Decrease in RMSE\n",
    "                * \\# of iterations\n",
    "* OLS for Transforming New Data\n",
    "    * Once we have a factorization set up we might want to be able to project a new document into our latent feature space using the same OLS technique\n",
    "    * What we have is a new row in our $V$ matrix, $V_{new}$, and we're trying to find a representation in the space spanned by the columns of $W$ matrix, $W_{new}$. Solve equation for $W_{new}$: $$V_{new}=W_{new}\\times H_{same}$$ $$(\\text{1} \\times \\text{n}) = (\\text{1} \\times \\mathbf{r}) \\times (\\mathbf{r} \\times \\text{n})$$\n",
    "    * Looks very similar to: $$y=X\\times \\beta$$ $$(\\text{n} \\times \\text{1}) = (\\text{n} \\times \\text{m}) \\times (\\text{m} \\times \\text{1})$$\n",
    "        * But, it's worth noting that this functionality is available to you built into existing NMF algorithms (e.g. sklearn's transform() method)\n",
    "* NMF as \"soft\" clustering:\n",
    "    * NMF is considered soft clustering because the latent features can be viewed as a clsuter\n",
    "        * Each observation can be partially in **more than one \"cluster\"**\n",
    "        * There are a number of penalties that have been devised to make factorizations more friendly to interpretation\n",
    "            * e.g. Simple Models (use Ridge/Lasso regularization)\n",
    "            * e.g. Complex Models (try to enforce sparsity in $W$ and $H$)\n",
    "* PCA/SVD vs NMF:\n",
    "    * PCA and SVD decompose into three matrics and NMF only two\n",
    "    * The bases in NMF are not orthogonal like in PCA/SVD\n",
    "    * The main difference is the **non-negativity constraint** for NMF\n",
    "        * Why do we care about having all the entries in the factorized matrics be positive (non-negative)? Interpretability of the topics\n",
    "        * How do we interpret negative values in the decomposed matrics?\n",
    "        ![pca_vs_nmf](pca_vs_nmf.png)\n",
    "        * Have **only additive components** of a topic is more interpretable (Non-negative)\n",
    "    * Summary of PCA/SVD vs NMF:\n",
    "    \n",
    "|                          |                             PCA/SVD                           |                  NMF                  |\n",
    "|:------------------------:|:----------------------------------------------------------:|:-------------------------------------:|\n",
    "| Dimensionality Reduction | Unsupervised dimensionality reduction                      | Unsupervised dimensionality reduction |\n",
    "| Coefficients             | Orthogonal vectors with positive and negative coefficients | Non-negative coefficients             |\n",
    "| Interpretation           | \"Holistic\"; difficult to interpret                         | \"Parts-based\"; easier to interpret    |\n",
    "| Algorithm                | Non-iterative                                              | Iterative (the presented algorithm)   |\n",
    "\n",
    "* Questions For Understanding NMF:\n",
    "    * What parameter choice must you make before performing NMF?\n",
    "    * When doing document clustering using NMF:\n",
    "        * What does a column in the $W$ matrix represent?\n",
    "        * What does a column in the $H$ matrix represent?\n",
    "        * How do we combine $W$ and $H$ to reconstitute a document in $V$ (column in $V$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
