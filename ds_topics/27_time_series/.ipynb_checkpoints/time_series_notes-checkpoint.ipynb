{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series\n",
    "\n",
    "* Objectives:\n",
    "    * Understanding key time series concepts\n",
    "    * Knowing graphical tools to analyze time series data\n",
    "    * Estimating ARIMA models using Box-Jenkins workflow\n",
    "    * Using Python's StatsModels to train and evalulate ARIMA models\n",
    "    * Describe Exponential Smoothing (ETS) model\n",
    "* (-) We are focus on forecasting the mean and not the quantiles\n",
    "* References: (arranged by increasing difficulty)\n",
    "    * Hyndman & Athanasopoulos: Forecasting: principles and practice (https://www.otexts.org/fpp)\n",
    "    * Enders: Applied Econometric Time Series\n",
    "    * Hamilton: Time Series Analysis\n",
    "    * Elliott & Timmermann: Economic forecasting\n",
    "* Python vs R:\n",
    "    * Python\n",
    "        * `pandas` - manipulate data and dates\n",
    "        * `statsmodels` - estimate core time series models\n",
    "    * R\n",
    "        * `lubridate` - manipulate dates\n",
    "        * Hyndman's `forecast` package\n",
    "        * Only serious option for ETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Time Series Concepts\n",
    "* **Time Series Data** - a sequence of observations of some quantity of interest which are collected over time\n",
    "    * e.g. GDP, price of toilet paper or a stock, demand for a good, unemployment rate, and web traffic (e.g. clicks, logins, posts, etc.)\n",
    "* Definition of Time Series - assume a timeseries, $\\{y_t\\}$, has the following properties:\n",
    "    * $y_t$ - an observation of the level of $y$ at time $t$\n",
    "    * $\\{y_t\\}$ - a collection of time series observations\n",
    "        * may extend back to $t=0$ or $t=-\\infty$, depending on the problem (e.g. $t \\in \\{0,\\dots,T\\}$)\n",
    "* Time Series Assumptions\n",
    "    * Discrete time - sampling at regular intervals (even if process is continuous)\n",
    "    * Evenly spaced observations\n",
    "    * No missing observations\n",
    "* Arrow of Time - if you're trying to predict the future given the past (e.g. tomorrow's weather, stock movements, etc.), you should **not** randomly shuffle data before splitting\n",
    "    * **Temporal leak** - model will effectively be trained on data from the future\n",
    "    * In such situations, you should always make sure all data in your test set is **posterior** to the data in the training set\n",
    "* **Nonstationary problems** - one class of unsolvable problems\n",
    "    * e.g. recommendation engine for clothing, using one month of data (August) to generate recommendations in the winter\n",
    "        * kinds of clothes people buy changes from season to season: clothes buying is a nonstationary phenomenon over the scale of a few months\n",
    "    * Need to model changes over time, which means constantly retraining model on data from the recent past, or gather data at a timescale where the problem is stationary\n",
    "    * For a cyclical problem like clothes buying, a few years' worth of data will suffice to capture seasonal variation (remember to make the time of year an input of your model!)\n",
    "    * Keep in mind that ML can only be used to memorize patterns that are present in your training data. You can only recognize what you've seen before. Using ML trained on past data to predict the future is making assumption that the future will behave like the past (which often isn't the case)\n",
    "* Difficulty with Time Series - time series are hard to model because we only observe **one realization of the path** of process\n",
    "    * Often have limited data\n",
    "    * Must impose structure (e.g. assumptions about correlation) in order to model\n",
    "    * Must project beyond support of the data\n",
    "* **Components of a Time Series** - consists of several different components that can be **additive** or **multiplicative** (Time Series Decomposition)\n",
    "![timeseries_comp](timeseries_comp.png)\n",
    "    * **Trend** - the long term movement in a time series, reflects the underlying level of the series\n",
    "        * represents the underlying level of the series\n",
    "        * is relatively stable, changing gradually\n",
    "        * can be affected by business cycle\n",
    "        * may be problematic to forecast close to the turning points.\n",
    "    * **Seasonal/Periodic** - A **seasonal** pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week). Seasonality is always of a fixed and known period. Hence, seasonal time series are sometimes called **periodic** time series.\n",
    "    * **Irregular** - the residual variation remaining after the trend-cycle and seasonality have been extracted from original time series. This component appears as short-term, unsystematic fluctuations around the trend that do not follow any systematic or repeated pattern, which could be captured by seasonal component.\n",
    "        * Irregularity is formed from all unpredictable effects that affect time series (e.g. unseasonable weather, sampling errors, other errors, etc.)\n",
    "        * By and large, irregularities are considered as random variables (white noise). It is assumed that the expected value of these factors is 0 (for an additive model) or 1 (for a multiplicative model).\n",
    "    * **Cyclic** - A cyclic pattern exists when data exhibit rises and falls that are not of fixed period. The duration of these fluctuations is usually of at least 2 years. Think of business cycles which usually last several years, but where the length of the current cycle is unknown beforehand.\n",
    "* Time Series Examples:\n",
    "![timeseries_example](timeseries_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Time Series Models\n",
    "* Popular Time Series Models:\n",
    "    1. **Auto Regressive Integrated Moving Average (ARIMA(p,d,q))** -  generalization of an autoregressive moving average (ARMA) model\n",
    "        * A benchmark model\n",
    "        * Captures key aspects of time series data\n",
    "    2. **Exponential Smoothing (ETS)** - technique for smoothing time series data using the exponential window function\n",
    "        * Also known as \"State space model\"\n",
    "        * Smooths out irregular shocks to model trend and seasonality\n",
    "        * Updates forecast with linear combination of past forecast and current value\n",
    "* Mathematical Notation For Time Series:\n",
    "    * $y_t$ - the level of some value of interest at time $t$\n",
    "    * $\\epsilon_t$ - the value of a shock, $\\epsilon$, at time $t$\n",
    "    * $\\hat{y}_{t+h|t}$ - the forecast for $y_{t+h}$ based on the information available at time $t$\n",
    "* **Lags** - often models use past values to predict future\n",
    "    * ARMA Models:\n",
    "        * **Auto Regressive (AR)**: $$AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i y_{t-i}+\\epsilon_t$$\n",
    "            * $\\varphi_1,\\dots,\\varphi_p$ - parameters of the model\n",
    "            * $c$ - constant\n",
    "            * $\\epsilon_t$ - white noise error\n",
    "        * **Moving Average (MA)**: $$MA(q) \\rightarrow y_t=\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}$$\n",
    "            * $\\theta_1,\\dots,\\theta_q$ - parameters of the model\n",
    "            * $\\epsilon_t,\\epsilon_{t-1},\\dots,\\epsilon_{t-q}$ - white noise error \n",
    "    * **Lag (Backshift) Operators** - operates on an element of a time series to produce the previous element ($L: x_t \\mapsto x_{t-1}$)\n",
    "        * **Auto Regressive (AR)**: $$AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i L^i y_{t}+\\epsilon_t$$\n",
    "        * **Moving Average (MA)**: $$MA(q) \\rightarrow y_t=\\mu + (1 + \\theta_1 L + \\cdots + \\theta_q L^q) \\epsilon_t$$\n",
    "* Time series concepts:\n",
    "    * Basic Statistics:\n",
    "        * **Expectation**: $$E[g(x)] = \\int g(x)f(x)dx$$ $$g(x)\\text{ - arbitrary function}$$ $$f(x)\\text{ - the probability density function}$$\n",
    "        * **Mean**: $$\\mu(x) = E[x]$$\n",
    "        * **Variance**: $$\\text{var}(x_t)=E[(x_t-\\mu(x_t))(x_t-\\mu(x_t))^T]$$ $$\\sigma^2(x_t)=\\text{var}(x_t)$$\n",
    "        * **Standard Deviation**: $$\\sigma(x_t)=\\sqrt{\\text{var}(x_t)}$$\n",
    "    * Measure of persistence of time series:\n",
    "        * **Autocovariance** - how much a lag predicts a future value of a time series $$\\text{acov}(x_t,x_{t-h})=E[(x_t-\\mu(x_t))(x_{t-h}-\\mu(x_{t-h}))^T]$$ \n",
    "            * also written as: $\\gamma(s,t)$ or $\\gamma(h)$ where $h = s - t$\n",
    "        * **Autocorrelation** - a dimensionless measure of the influence of one lag upon another. Helps determine which ARIMA model to use $$\\text{acorr}(x_t)=\\frac{\\text{acov}(x_t,x_{t+h})}{\\sigma(x_t)\\sigma(x_{t+h})}$$\n",
    "            * also written as: $\\rho(t) = \\frac{\\gamma(t)}{\\gamma(0)}$\n",
    "    * Special forms of time series (which are easier to forecast)\n",
    "        * In order to forecast, we need mean, variance, and correlation to be stable over time\n",
    "        * **Strictly Stationary** - $\\{x_t\\}$ is strictly stationary if $f(x_1,\\dots,x_t)=f(x_{1+h},\\dots,x_{t+h})\\text{ }\\forall\\text{ } h$\n",
    "        * **Weakly Stationary**\n",
    "            * mean is constant for all periods: $\\mu(x_t)=\\mu(x_{t+h})\\text{ }\\forall\\text{ } h$\n",
    "            * autocorrelation, $\\rho(s,t)$, depends on $|s-t|$\n",
    "        * **White Noise**\n",
    "            * $\\text{acov}(x_t,x_{t+h})=\\text{var}(x_t)$ iff $h=0$ and $0$ otherwise\n",
    "            * is weakly stationary\n",
    "            * is a key building block of time series models\n",
    "    * **Analog Principle** - replace expectations with sample averages when calculating statistics\n",
    "        * Intuition - Weak Law of Large Numbers\n",
    "            * mean: $E[x] = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "            * in general: $E[g(x)]= \\frac{1}{N}\\sum_{i=1}^N g(x_i)$\n",
    "        * Sometimes replace $N$ with $N-1$ (e.g. for variance)\n",
    "            * statistic is consistent\n",
    "            * makes estimator unbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Auto-Regressive Integrated Moving Average (ARIMA) Model\n",
    "* $ARIMA(p,d,q)$ consists of $AR(p)$, $I(d)$, and $MA(q)$:\n",
    "    * $AR(p)$ (Auto-regressive) - model captures the persistence of **past history**\n",
    "        * means **auto-regressive of order $p$**: $AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i y_{t-i}+\\epsilon_t$\n",
    "        * with lag operator and polynomials: $AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i L^i y_{t}+\\epsilon_t$\n",
    "    * $I(d)$ (Integrated) - model captures the **non-stationary trend**\n",
    "        * means **integrated of order $d$**: $y_t=y_{t-1}+\\mu+\\epsilon_t$\n",
    "        * $d$ - how many times you must difference the series so that it is stationary\n",
    "        * usually $d\\in \\{0,1,2\\}$\n",
    "        * differencing should remove the trend component\n",
    "        * Example: random walk (with drift)\n",
    "        * Compute differences with `np.diff(n=d)` or `pd.Series.diff(periods=d)` to turn ARIMA into ARMA\n",
    "    * $MA(q)$ (Moving Average) - model captures the persistence of **past shocks**\n",
    "        * means **moving average of order $q$**: $MA(q) \\rightarrow y_t=\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}$\n",
    "        * with lag operator and polynomials: $MA(q) \\rightarrow y_t=\\mu + (1 + \\theta_1 L + \\cdots + \\theta_q L^q) \\epsilon_t$\n",
    "        * **do not confuse with computing the moving average of $\\{y_t\\}$**, which is often used to aggregate data\n",
    "* ARIMA intuition:\n",
    "    * AR, I, and/or MA may be missing from a general ARIMA model\n",
    "    * May also include seasonal components: $ARIMA(p,d,q)(P,D,Q)$\n",
    "        * Can add higher order lags for seasonality\n",
    "    * If $d=0$, then ARIMA becomes ARMA\n",
    "    * If your complex algorithm isn't better than ARIMA, then use ARIMA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Estimating ARIMA models using **Box-Jenkins**\n",
    "* Box-Jenkin algorithm:\n",
    "    1. Exploratory Data Analysis (EDA)\n",
    "        * plot time series: Autocorrelation Function (ACF), Partial Autocorrelation function (PACF)\n",
    "        * identify hypotheses, models, and data issues\n",
    "        * aggregate to an appropriate gain\n",
    "    2. Fit model(s)\n",
    "        * difference model until stationary\n",
    "    3. Examine residuals: are they white noise?\n",
    "    4. Test and evaluate on out-of-sample data\n",
    "    5. Check for:\n",
    "        * Structural breaks\n",
    "        * Seasonality and periodicity\n",
    "        * Forecast reliability for large $h$ with limited data\n",
    "* Modeling flow chart:\n",
    "![modeling_flow](modeling_flow.png)\n",
    "* Plot data to develop understanding of data and possible models:\n",
    "    * Key diagnostic plots:\n",
    "        * Plot time series: $y_t$ vs. $t$\n",
    "        * Plot autocorrelation function (ACF): $\\rho(h)$ vs. $h$\n",
    "        * Plot partial autocorrelation function (PACF)\n",
    "    * Repeat for first and second differences, if necessary:\n",
    "        * Compute differences with `np.diff(n=d)` or `pd.Series.diff(periods=d)`\n",
    "        * Transform series, if necessary e.g. $y_t \\rightarrow log(y_t)$\n",
    "        * Check stationarity e.g. no trend and constant variance\n",
    "* **Autocorrelation Function (ACF)** - shows likely order of the $MA(q)$ part of the $ARIMA(p,d,q)$ model:\n",
    "    * Plots $\\rho(h)$ vs. lags $h$\n",
    "    * Find largest significant spike\n",
    "    * Consider order $q$, where $q=\\text{largest lag}$\n",
    "    * Sample ACF code:\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "    data = sm.tsa.arma_generate_sample(ar=[0.7,0.0,0.3], ma=[0.2,-0.1], nsample=100)\n",
    "    sm.graphics.tsa.plot_acf(data, lags=28, alpha=0.05)\n",
    "    plt.show()\n",
    "    ```\n",
    "* **Partial Autocorrelation Function (PACF)** - shows likely order of the $AR(p)$ part of the $ARIMA(p,d,q)$ model:\n",
    "    * Plots partial autocorrelation vs. lags $h$\n",
    "    * Partial autocorrelation uses a regression method to compute effect of just a single lag but not intermediate lags like ACF\n",
    "    * Consider order $p$, where $p=\\text{largest lag}$\n",
    "    * Sample PACF code:\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "    data = sm.tsa.arma_generate_sample(ar=[0.7,0.0,0.3], ma=[0.2,-0.1], nsample=100)\n",
    "    sm.graphics.tsa.plot_pacf(data, lags=28, alpha=0.05)\n",
    "    plt.show()\n",
    "    ```\n",
    "* Helper Function For Plotting ACF and PACF\n",
    "    * Helper function\n",
    "    ```python\n",
    "    def ts_diag_plot(data, lags=28):\n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        ax1 = fig.add_subplot(311)\n",
    "        ax1.plot(data)\n",
    "        ax1.set_title('y_t vs. t')\n",
    "        ax2 = fig.add_subplot(312)\n",
    "        sm.graphics.tsa.plot_acf(data, lags=lags, ax=ax2) \n",
    "        ax3 = fig.add_subplot(313) \n",
    "        sm.graphics.tsa.plot_pacf(data, lags=lags, ax=ax3) \n",
    "        fig.show()\n",
    "        return fig\n",
    "\n",
    "    from tsplot import ts_diag_plot\n",
    "    fake = sm.tsa.arma_generate_sample(ar=[ 0.7, 0.0, 0.3], ma=[0.2, -0.1], nsample=100)\n",
    "    fig = ts_diag_plot(fake)\n",
    "    ```\n",
    "    * Plot of ACF and PACF\n",
    "    ![acf_pacf](acf_pacf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Question To Think About For Time Series Data\n",
    "* Is it stationary?\n",
    "* Is there a trend?\n",
    "* Is the variance stable?\n",
    "* Are there seasonal or periodic components?\n",
    "* What $AR$ and $MA$ terms are likely present?\n",
    "* Are there structural breaks in the data?\n",
    "* Do I have enough data to forecast at horizon $h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Stabilizing the Time Series\n",
    "* If you need to stabilize the time series before estimating a model:\n",
    "    * Transform data to **stabilize variance**:\n",
    "        * $y_t \\rightarrow log(y_t)$\n",
    "        * Verify via **Box-Cox test**\n",
    "        * Verify by plotting\n",
    "    * Transform data so **series is stationary**:\n",
    "        * Compute first or second difference\n",
    "        * $y_t \\rightarrow \\Delta y_t$ or $y_t \\rightarrow \\Delta^2 y_t$\n",
    "        * Verify by **Portmanteau test**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Fitting an ARIMA model\n",
    "* Steps to fitting a model:\n",
    "    1. Split data into train set (earlier observations) and test set (later observations)\n",
    "    2. To forecast at horizon, $h$, should have at least $3h$ observations to train plus $h$ observations to test:\n",
    "        * e.g. you **cannot** forecast demand in two years if you only have three months of data\n",
    "        * If these conditions are violated, you need a \"panel of experts\"\n",
    "        * More data is better, especially if seasonality is present\n",
    "    3. To identify optimal order of model:\n",
    "        * Examine ACF and PACF\n",
    "        * Difference until stationary\n",
    "        * Number of differences is order $d$ for $l(d)$\n",
    "        * Use `sm.tsa.arma_order_select_ic` to generate compare several models\n",
    "        * Use cross validation\n",
    "* Code For Fitting ARIMA model:\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "data = sm.datasets.macrodata.load_pandas()\n",
    "df = data.data\n",
    "df.index = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\n",
    "y = df.m1\n",
    "X = df[['realgdp', 'cpi']]\n",
    "model = sm.tsa.ARIMA(endog=y, order=[1,1,1])\n",
    "# model2 = sm.tsa.ARIMA(endog=y, order=[1,1,1], exog=X) \n",
    "results = model.fit()\n",
    "results.summary()\n",
    "```\n",
    "* Results of Fitted ARIMA model:\n",
    "![arima_results](arima_results.png)\n",
    "* Advanced ARIMA techniques (for more complicated situations):\n",
    "    * Add **Fourier terms** to capture **periodic** behavior\n",
    "    * Add other **covariates** which can improve prediction\n",
    "    * Use **Vector Autoregressivev Integrated Moving Average (VARIMA)** model to capture dynamics of a system of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Forecasting with Prediction Intervals\n",
    "* **Prediction Intervals** - contains future realization of the mean $y_{t+h}$ with probability $1-\\alpha$ and increases the further you forecast into the future\n",
    "    * A forecast of $\\{y_t\\}$ at time $t+h$ computes:\n",
    "        * $\\hat{y}_{t+h|t}$ - the expected mean of $y_t$ at time $t+h$ conditional on the information available at $t$\n",
    "    * A prediction interval is **not a confidence interval**:\n",
    "        * A **prediction interval** contains the future realization of a random variable with percent certainty: $Pr=1-\\alpha$\n",
    "        * A **confidence interval** contains the true value of a parameter with percent certainty: $Pr=1-\\alpha$\n",
    "* **Forecasting** - can use `results.forecast` to compute out of sample predictions:\n",
    "    * Use `alpha`, $\\alpha$, to choose appropriate prediction interval, e.g. 80%, 90%, 95%, etc.\n",
    "    * Do not use the prediction interval to forecast quantiles of $\\hat{y}_{t+h|t}$\n",
    "    * Can supply (forecasted) value of exogenous predictors\n",
    "    ```python\n",
    "    y_hat, stderr, pred_int = results.forecast(steps=h, alpha=0.05)\n",
    "    ```\n",
    "    * Prediction plot includes a **prediction interval**:\n",
    "        * Contains future realization of $y_{t+h}$ with probability $1-\\alpha$\n",
    "        * A prediction interval is **not** a confidence interval\n",
    "        ```python\n",
    "        results.plot_predict('2009Q3', '2014Q4', dynamic=True, plot_insample=True)\n",
    "        plt.show()\n",
    "        ```\n",
    "    ![prediction_intervals](prediction_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Evaluating Forecasting Model\n",
    "* Evaluating Steps:\n",
    "    * Check residuals are white noise:\n",
    "        * Examine ACF and PACF\n",
    "        * Compute **Portmanteau test (Box-Pierce, Box-Ljung)** to see if residuals are correlated\n",
    "    * Check solver converged!\n",
    "    * Remember that simple models often outperform fancy models on new data\n",
    "    * Compare any forecast against the benchmark forecast:\n",
    "        * Choose a benchmark such as mean or random walk with drift\n",
    "        * Fit model on training set and evaluate on test set\n",
    "        * To compare multiple forecasts, use a sliding window\n",
    "* Common Evaluation Metrics:\n",
    "    * **Root Mean Squared Error (RMSE)**: $$RMSE=\\sqrt{\\frac{1}{H}\\sum_{i=1}^h (y_{t+h}-\\hat{y}_{t+h|t})^2}$$\n",
    "    * **Mean Absolute Error (MAE)**: $$MAE=\\frac{1}{H}\\sum_{i=1}^h |y_{t+h}-\\hat{y}_{t+h|t}|$$\n",
    "    * **Mean Absolute Percentage Error (MAPE)**: $$MAPE=\\frac{1}{H}\\sum_{i=1}^h \\Big|\\frac{y_{t+h}-\\hat{y}_{t+h|t}}{y_{t+h}}\\Big|$$\n",
    "* Model Selection - use **information criterion** to evaluate models\n",
    "    * Several information criteria exists: **AIC**, **AICc**, **BIC**\n",
    "        * Essentially, log-likelihood plus penality for adding parameters\n",
    "        * Measures fit vs. parsimony of model\n",
    "        * Different criteria have different finite sample properties\n",
    "    * Choose model with **lowest** information criterion\n",
    "    * Especially helpful if you have **limited** data\n",
    "    * Popular, pre-ML method, but consider **cross-validation** if you have enough data\n",
    "* Tips For Forecasting\n",
    "    * Work at the appropriate level of aggregation (grain):\n",
    "        * Don't use 5 minute resolution data to forecast at $h = \\text{one month}$\n",
    "    * Don't forecast beyond what the data will support\n",
    "        * Should have $4h$ amount of data to forecast at horizon $h$\n",
    "    * Err on the side of simplicity\n",
    "    * Or, take a machine learning approach:\n",
    "        * Try a set of lags and differences plus other predictors\n",
    "        * Use regularization and/or variable selection\n",
    "        * See Taieb & Hyndman for an approach which uses boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **Exponential Smoothing (ETS)** model\n",
    "* Benefits of ETS model:\n",
    "    * Robust performance\n",
    "    * Easy to explain to non-technical stakeholders\n",
    "    * Easy to estimate with limited computational resources\n",
    "    * Forecast well because of parsimony\n",
    "* ETS model consists of smoothing equations for:\n",
    "    * Forecast\n",
    "    * Level\n",
    "    * Trend (optional)\n",
    "    * Seasonality (optional)\n",
    "* Model Remarks:\n",
    "    * Can use either an additive or multiplicative specification\n",
    "    * Can use a state space formulation\n",
    "    * Usually a three-character string identifying method using the framework terminology of Hyndman et al. (2002) and Hyndman et al. (2008):\n",
    "        * In all cases, \"N\" = none, \"A\" = additive, \"M\" = multiplicative and \"Z\" = automatically selected\n",
    "        * The first letter denotes the **error** type (\"A\", \"M\" or \"Z\") \n",
    "        * The second letter denotes the **trend** type (\"N\",\"A\",\"M\" or \"Z\")\n",
    "        * The third letter denotes the **seasonality** type (\"N\",\"A\",\"M\" or \"Z\") \n",
    "        * For example, \"ANN\" is simple exponential smoothing with additive errors, \"MAM\" is multiplicative Holt-Winters' method with multiplicative errors, and so on.\n",
    "* Hyndman's Taxonomy:\n",
    "    * Hyndman categorizes exponential smoothing models as ETS:\n",
    "        * $E$ for type of **error**\n",
    "        * $T$ for type of **trend**\n",
    "        * $S$ for type of **seasonality**\n",
    "    * Typical values are:\n",
    "        * $A$ for **additive**\n",
    "        * $M$ for **multiplicative**\n",
    "        * $N$ for **none**\n",
    "        * $A_d$ for **additive damped**\n",
    "        * $M_d$ for **multiplicative damped**\n",
    "    * Example ETS model types:\n",
    "        * ETS(AAN)\n",
    "            * Has additive error and trend but no seasonality\n",
    "            * Simple exponential smoothing\n",
    "            * e.g. **Holt's Linear method**, \"double exponential smoothing\"\n",
    "        * ETS(AAA)\n",
    "            * **Holt-Winters' method**\n",
    "            * Adds seasonality\n",
    "* Popular ETS Models:\n",
    "    * **Simple Exponential Smoothing ETS(ANN)** - updates forecast based on latest realization of $y_t$\n",
    "        * Forecast equation: $\\hat{y}_{t+1|t}=\\mathcal{l}_t$\n",
    "        * Level equation: $\\mathcal{l}_t = \\alpha y_t + (1-\\alpha) \\mathcal{l}_{t-1}$\n",
    "        * If $y_t=\\hat{y}_{t|t-1}+\\epsilon_t$, can use **error correction** formulation:\n",
    "            * $y_t=\\mathcal{l}_{t-1}+\\epsilon_t$\n",
    "            * $\\mathcal{l}_t=\\mathcal{l}_{t-1}+\\alpha \\epsilon_t$\n",
    "    * **Holt's Linear Model ETS(AAN)** - adds slope to the model to better handle a trend\n",
    "        * Forecast equation: $\\hat{t}_{t+h|t}=\\mathcal{l}_t+h b_t$\n",
    "        * Level equation: $\\mathcal{l}_t=\\alpha y_t + (1-\\alpha)(\\mathcal{l}_{t-1}+b_{t-1})$\n",
    "        * Trend equation: $b_t=\\beta^*(\\mathcal{l}_t-\\mathcal{l}_{t-1})+(1-\\beta^*)b_{t-1}$\n",
    "* Python support for ETS model: (partial)\n",
    "    * Python package: `pandas.stats.moments.ewma`\n",
    "    * Is user unfriendly\n",
    "    * Best to use R's `ets` function in the `forecast` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) ETS vs. ARIMA\n",
    "* ARIMA features and benefits:\n",
    "    * Benchmark model for almost a century\n",
    "    * Much easier to estimate with modern computational resources\n",
    "    * Easy to diagnose models graphically\n",
    "    * Easy to fit using Box-Jenkins methodology\n",
    "* ETS features and benefits:\n",
    "    * Can handle non-linear and non-stationary processes\n",
    "    * Can be computed with limited computational resources\n",
    "    * Not always a subset of ARIMA\n",
    "    * Easier to explain to non-technical stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Testing Understanding of Time Series\n",
    "* What are the steps in the Box-Jenkins' approach?\n",
    "* How much data do I need to forecast at horizon $h$?\n",
    "* How should I evaluate a forecast?\n",
    "* What are the benefits of ARIMA vs ETS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
