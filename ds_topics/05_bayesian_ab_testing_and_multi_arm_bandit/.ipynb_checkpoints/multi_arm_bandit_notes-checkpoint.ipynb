{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Arm Bandit\n",
    "\n",
    "* Objectives:\n",
    "    * Exploration vs Exploitation\n",
    "    * Formalization\n",
    "    * Regret\n",
    "    * Strategies:\n",
    "        * Epsilon-Greedy\n",
    "        * UCB1\n",
    "        * Softmax\n",
    "        * Bayesian Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Motivation For Multi-Arm Bandit\n",
    "* Consider the following scenario:\n",
    "    * The company you work for is testing out a new version of it's website\n",
    "    * After running an A/B test for an afternoon the new version of the site appears to be performing slightly better than the old version\n",
    "    * After running the test over night, the new version of the site is performing better than the old version with a statistically significant p-value of 0.04\n",
    "    * **Should we stop the test, or keep the test running?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Exploration vs. Exploitation\n",
    "* When we are trying to determine which option, from a potential pool, is the best option what we are faced with is an information gathering challenge.\n",
    "    * **Exploration** - testing out the different options (e.g. of the website), to determine how good each one is. Acquiring more knowledge about the reward associated with the options.\n",
    "    * **Exploitation** - leveraging your current knowledge about the options to get the highest expected reward at that time.\n",
    "* Traditional A/B Testing (Frequentist) in terms of exploration vs. exploitation:\n",
    "    * Start with **pure exploration**, in which the same number of users are assigned to see each of the options. This is the testing phase.\n",
    "    * Once the test is complete and some conclusion has been reached, you switch to **pure exploitation**. In this phase, all of the users see the version that is chosen as a result of the test\n",
    "* Potential Inefficiencies of Traditional A/B Testing (Frequentist):\n",
    "    * (-) Need to wait for the experiment to conclude for certain statistical guarantees to be provided\n",
    "    * (-) Only after the experiment concludes can we capitalize on a potentially better option\n",
    "    * (-) This wastes time and money showing users the suboptimal site\n",
    "* Multi-Arm Bandit Approach\n",
    "    1. Show each user the site that you think is the best **most** of the time.\n",
    "        * The definition of most will be dictated by your **strategy**\n",
    "        * Depending on your strategy, you can balance **exploration** and **exploitation** instead of having to choose either one or the other\n",
    "    2. As the experiment runs and you send users to different sites, update your beliefs about each site\n",
    "    3. Run until a clear best site emerges\n",
    "* Origins of Multi-Arm Bandit Approach:\n",
    "    * This problem was originally motivated from the problem that a gambler faces when deciding which **slot machine** (individually called **one-armed bandits**) to play at\n",
    "    * Assuming that the gambler wants to be smart about their strategy they are faced, not only, with the choice of which machines to play, how many times they should play them and in what order\n",
    "    * All they know is that each bandit will provide a reward when its lever is pulled according to its individual (and static) distribution\n",
    "    * Goal: The gambler's objective is to **maximize** the sum of rewards they receive through a series of lever pulls\n",
    "* Use Cases For Multi-Arm Bandit:\n",
    "    * **Dynamic A/B Testing**\n",
    "    * Budget allocation amongst competing projects\n",
    "    * Clinical trials\n",
    "    * Adaptive routing in attempts to minimize network delays\n",
    "    * Reinforcement learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Formalization of Multi-Arm Bandit\n",
    "* Terminology\n",
    "    * The model is given by a set of real distributions $B=\\{R_1,\\dots,R_k\\}$, where each distribution is associated with a reward delivered by one of the $K\\in \\mathbb{N}^+$ levers\n",
    "    * We will define the mean values associated with each of these distributions as $\\mu_1,\\dots,\\mu_k$\n",
    "    * The agent plays one lever per round and observes the associated reward. The goal of the agent is to maximize the sum of the collective rewards, or alternatively minimize the **regret**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Minimizing **Regret**\n",
    "* The regret, $\\rho$, that an agent experiences after $T$ rounds is the difference between the expected sum reward associated with the optimal strategy and the sum of the rewards collected: $$\\rho=T\\mu^*-\\sum_{t=1}^T \\hat{\\rho}_t$$ $$\\mu^*=max_k\\{\\mu_k\\}$$ $$\\mu^* - \\text{maximal reward mean}$$ $$\\hat{\\rho}_t - \\text{reward at time }t$$\n",
    "* **Regret** - a measure of how often we choose the suboptimal bandit. Can be viewed as a cost function we are trying to minimize\n",
    "* **Zero-Regret Strategy** - defined as one who's average regret per round, $\\frac{\\rho}{T}$, goes to zero in the limit where the number of rounds, $T$ goes to infinity\n",
    "    * Zero-regret strategy does **not** guarantee that you will never choose a suboptimal outcome\n",
    "    * Instead, it guarantees that, as you continue to play you will tend to choose the optimal outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Multi-Arm Bandit Strategies\n",
    "* **Epsilon-Greedy** - always explore a certain amount\n",
    "    * Epsilon-Greedy Steps:\n",
    "        * **Explore** with some probability, $\\epsilon$, often chosen as 10%\n",
    "        * All other times, **exploit** or choose the bandit with the best performance so far: $$argmax_{j=1}^K\\{\\hat{\\mu}_j(t)\\}$$ $$\\hat{\\mu}_j(t)\\text{ - our best guess for }\\mu_j\\text{ at round }t$$\n",
    "        * Update our knowledge about $\\mu$'s after each round\n",
    "    * Is this a zero-regret strategy?\n",
    "* **Upper Confidence Bound (UCB1)** - greedily chooses the bandit with the highest $\\hat{\\mu}(t)$, but with a clever additional factor that **automatically balances exploration and exploitation**\n",
    "    * The choice at round $t$, after each lever is pulled once for a baseline: $$argmax_{j=1}^K\\Big\\{\\hat{\\mu}_j(t)+\\sqrt{\\frac{2\\ln(t)}{n_j}}\\Big\\}$$ $$\\hat{\\mu}_j(t)\\text{ - best guess for }\\mu_j\\text{ at time }t$$ $$n_j\\text{ - number of times that bandit }j\\text{ has been pulled }$$ $$t\\text{ - number of rounds that have been played in total}$$\n",
    "    * Is this a zero-regret strategy?\n",
    "* **Softmax** - creates a probability distribution over all the levers in proportion to how good we think each lever is $$P_t(\\text{choosing bandit }j) = \\frac{e^{\\frac{\\hat{\\mu}_j(t)}{\\tau}}}{\\sum_{j=1}^K e^{\\frac{\\hat{\\mu}_j(t)}{\\tau}}}$$ \n",
    "    * tau ($\\tau$) - temperature that controls the \"randomness\" of the distribution. Usually chosen around $0.001$\n",
    "    * At each round, the above distribution is sampled from where the bandit is chosen\n",
    "    * Softmax is a member of probability matching algorithms (has property of creating probability distributions)\n",
    "* **Bayesian Bandit** - use **Bayesian beta-binomial conjugate prior** to model each of the bandits\n",
    "    * Can use this technique to model click-through rate as a base model for each of the bandits\n",
    "    * This is also part of probability matching algorithm where we have a separate model for each of the bandits\n",
    "    * Bayesian Bandit Steps:\n",
    "        1. Sample from the distributions for each bandit\n",
    "        2. Choose the bandit with the highest sampled $\\mu$\n",
    "        3. Update the distribution of the chosen bandit with the knowledge gained from choosing it\n",
    "    ![bayesian_bandit](bayesian_bandit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
