{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation Systems (Matrix Factorization Recommenders)\n",
    "\n",
    "* Objectives: \n",
    "    * Where are recommenders used?\n",
    "    * What does our dataset look like?\n",
    "    * What are the high-level approaches to building a recommender?\n",
    "        * Content-based\n",
    "        * Collaborative filtering\n",
    "        * Matrix factorization\n",
    "    * How do we evaluate our recommender\n",
    "    * How to deal with \"cold start\"?\n",
    "    * What are the computational performance concerns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Motivation / Basics\n",
    "* Where are recommenders used?\n",
    "    * Amazon item recommendation\n",
    "    * Pandora music recommendation\n",
    "    * Coursera course recommendation\n",
    "    * Netflix show/movie recommendation\n",
    "* Business goal for recommender systems to answer:\n",
    "    * Name a business that cares about each of these questions, and indicate why they care:\n",
    "        * What will the user **like**?\n",
    "        * What will the user **buy**?\n",
    "        * What will the user **click**?\n",
    "* Netflix (Kaggle-style) Competition From Oct. 2006 - July 2009:\n",
    "    * Goal: Beat Netflix's own recommender by 10%\n",
    "    * Length: Took almost 3 years\n",
    "    * The winning team used gradient boosted decision trees over the predictions of 500 other models\n",
    "    * Netflix never deployed the winning algorithm!\n",
    "* For recommenders, learn to build, evaluate, and deploy your recommender\n",
    "* Types of High-Level Approaches to Building Recommenders:\n",
    "    1. **Popularity** - makes the **same** recommendation to **every** user, based only on the popularity of an item\n",
    "        * e.g. Twitter \"moments\"\n",
    "    2. **Content-based (or Content Filtering)** - predictions are made based on the properties/characteristics of an item. User behavior is **not** considered.\n",
    "        * e.g. Pandora Radio\n",
    "    3. **Collaborative Filtering** - only considers past user behavior (**not** content properties)\n",
    "        * **User-User Similarity**\n",
    "        * **Item-Item Similarity**\n",
    "        * e.g. Netflix & Amazon Recommendations\n",
    "            * e.g. Google Ads\n",
    "            * e.g. Facebook Ads, Search, Friends Recommendations, News Feed, Trending News, Rank Notifications, Rank Comments\n",
    "    4. **Matrix Factorization** - finding latent features (or factors)\n",
    "* What does our dataset look like?\n",
    "    * Setup For **Sparse Ratings Matrix** (or **Utility Matrix**)\n",
    "    * Matrix can be very, very sparse (99% of entries unknown)\n",
    "* Dataset Type: Has **explicit** ratings and mostly missing values    \n",
    "    \n",
    "|              |  Movie 1 |  Movie 2 |  Movie 3 | $\\cdots$ | Movie $m$ |\n",
    "|:------------:|:--------:|:--------:|:--------:|:--------:|:---------:|\n",
    "| **User 1**   | 4        | ?        | ?        | $\\cdots$ | 1         |\n",
    "| **User 2**   | 3        | 3        | 2        | $\\cdots$ | 2         |\n",
    "| **User 3**   | ?        | 3        | ?        | $\\cdots$ | ?         |\n",
    "| $\\vdots$     | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\ddots$ | $\\vdots$  |\n",
    "| **User $n$** | ?        | 5        | 4        | $\\cdots$ | 5         |\n",
    "\n",
    "* Dataset Type: Has **implicit** ratings and mostly missing values\n",
    "![implicit_ratings](implicit_ratings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Collaborative Filtering - only considers past user behavior (**not** content properties)\n",
    "* **User-User Similarities** - examine all pairs of **users** and calculate their similarities of row vectors (e.g. euclidean)\n",
    "![user_user](user_user.png)\n",
    "* **Item-Item Similarities** - examine all pairs of **items** and calculate their similarities of column vectors (e.g. cosine similarity)\n",
    "![item_item](item_item.png)\n",
    "* Does user-user or item-item algorithm have better efficiency?\n",
    "    * Let $m$ = # of users and $n$ = # of items\n",
    "    * Compute the similarities of all pairs\n",
    "    * Compute the performance or complexity of an algorithm ($O$)\n",
    "        * The Big $O$ - can be used to describe the execution time required or the space used (e.g. in memory or on disk) by an algorithm\n",
    "        * **User-User** = $O(m^2n)$\n",
    "        * **Item-Item** = $O(mn^2)$\n",
    "* Types of Similarity Metrics:\n",
    "    * **Euclidean Distance**\n",
    "        * Normal equation: $$dist(a,b)=\\Vert a-b \\Vert=\\sqrt{\\sum_{i=1}(a_i-b_i)^2}$$\n",
    "        * Similarity equation: $$similarity(a,b)=\\frac{1}{1+dist(a,b)}$$\n",
    "    * **Cosine Similarity**\n",
    "        * Normal equation: $$cos(\\theta_{a,b})=\\frac{a \\cdot b}{\\Vert a \\Vert \\Vert b \\Vert}=\\frac{\\sum_{i=1}a_i b_i}{\\sqrt{\\sum_{i=1}a_i^2}\\sqrt{\\sum_{i=1}b_i^2}}$$\n",
    "        * Standardized Similarity equation: $$similarity(a,b)=0.5+0.5 \\times cos(\\theta_{a,b})$$\n",
    "    * **Pearson's Correlation**\n",
    "        * Normal equation: $$pearson(a,b)=\\frac{cov(a,b)}{std(a) \\times std(b)}=\\frac{\\sum_{i=1}(a_i-\\bar{a})(b_i-\\bar{b})}{\\sqrt{\\sum_{i=1}(a_i-\\bar{a})^2}\\sqrt{\\sum_{i=1}(b_i-\\bar{b})^2}}$$\n",
    "        * Similarity equation: $$similarity(a,b)=0.5+0.5 \\times pearson(a,b)$$\n",
    "    * **Jaccard Index**\n",
    "        * Similarity equation: $$similarity(a,b)=\\frac{|U_a \\cap U_b|}{|U_a \\cup U_b|}$$\n",
    "        * $U_k$ denotes the set of users who rated item $k$\n",
    "* The Similarity Matrix:\n",
    "    * Pick a similarity metric, and create the similarity matrix:\n",
    "    \n",
    "|            |  Item 1  |  Item 2  |  Item 3  | $\\cdots$ |\n",
    "|:----------:|:--------:|:--------:|:--------:|:--------:|\n",
    "| **Item 1** | 1        | 0.3      | 0.2      | $\\cdots$ |\n",
    "| **Item 2** | 0.3      | 1        | 0.7      | $\\cdots$ |\n",
    "| **Item 3** | 0.2      | 0.7      | 1        | $\\cdots$ |\n",
    "| $\\vdots$   | $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\ddots$ |\n",
    "\n",
    "* Making Predictions with Collaborative Filtering:\n",
    "    * Example: user $u$ hasn't rated item $i$ and we want to predict the rating that this user **would** give this item: $$rating(u,i)=\\frac{\\sum_{j\\in I_u} similarity(i,j) (r_{u,j})}{\\sum_{j\\in I_u} similarity(i,j)}$$ $$I_u = \\text{set of items rated by user } u$$ $$r_{u,j} = \\text{user }u\\text{'s ratings of item }j$$\n",
    "    * Order by descending predicted rating for a single user, and recommend the top $k$ items to the user\n",
    "* Making Predictions **Using Neighborhoods** with Collaborative Filtering:\n",
    "    * This calculation of predicted ratings can be **very costly**\n",
    "    * To **mitigate** this issue, we will only consider the **$n$ most similar items** to an item when calculating the prediction: $$rating(u,i)=\\frac{\\sum_{j\\in I_u \\cap N_i} similarity(i,j) (r_{u,j})}{\\sum_{j\\in I_u \\cap N_i} similarity(i,j)}$$ $$I_u = \\text{set of items rated by user } u$$ $$r_{u,j} = \\text{user }u\\text{'s ratings of item }j$$ $$N_i \\text{ is the } n \\text{ items which are most similar to item }i$$\n",
    "    * Order by descending predicted rating for a single user, and recommend the top $k$ items to the user\n",
    "* Deploying the CF Recommender:\n",
    "    * Compute similarities between all pairs of items\n",
    "    * Compute the neighborhood of each item\n",
    "    * At request time, predict scores for candidate items and make a recommendation\n",
    "* Evaluating the Recommenders:\n",
    "    * Is it possible to do cross-validation like normal?\n",
    "        * (-) Recommenders are inherently hard to validate\n",
    "        * (-) There is no \"one\" answer for all dataset\n",
    "    * Calculate MSE between targets and our predictions over the holdout set\n",
    "    ![mse_cv](mse_cv.png)\n",
    "        * question marks denotes the holdout set values (**not** missing values)\n",
    "        * K-fold cross-validation is optional\n",
    "        * Why isn't the method above a true estimate of a recommender's performance in the field?\n",
    "        * Why would A/B Testing be better?\n",
    "    * Another validation method: **Splitting dataset by time**\n",
    "    ![split_data](split_data.png)\n",
    "        * Why might we prefer doing this instead of the more \"normal\" cross-validation from the previous slide?\n",
    "    * Bad validation split: Splitting dataset by movie\n",
    "    ![split_by_movie](split_by_movie.png)\n",
    "* Dealing with **\"Cold Start\"** items or users:\n",
    "    * **Cold Start** - refers to scenario where a new user or item is introduced into the dataset with no information\n",
    "    * Example: A new **user** signs up\n",
    "        * What will our recommender do assuming we're using item-item similarities?\n",
    "            * One Strategy: Force users to rate 5 items as part of the sign-up process and/or recommend popular items at first\n",
    "        * What will our recommender do assuming we're YouTube and we're using item popularity to make recommendations?\n",
    "            * Not much of a problem\n",
    "    * Example: A new **item** is introduced\n",
    "        * What will our recommender do assuming we're using item-item similarities?\n",
    "            * One Strategy: Put it in the \"new releases\" section until enough users rate it and/or use item metadata if any exists\n",
    "        * What will our recommender do assuming we're YouTube and we're using item popularity to make recommendations?\n",
    "            * Don't use **total number of views** as the popularity metric, and use a different strategy\n",
    "* Downfall of **Collaborative Filtering**\n",
    "    * **Item-Item Collaborative Filtering**\n",
    "        * Example: \"I like action movies\" $\\rightarrow$ rate \"Top Gun\" and \"Mission Impossible\" $\\rightarrow$ 5s\n",
    "            * (-) Item-Item Recommender: Recommends \"Jerry Maguire\" even though I won't like it\n",
    "    * **User-User Collaborative Filtering**\n",
    "        * Example: \"I like Tom Cruise\" $\\rightarrow$ rate \"Top Gun\" and \"Mission Impossible\" $\\rightarrow$ 5s \n",
    "            * (-) User-User Recommender: Recommends \"Transformers\" even though I won't like it \n",
    "* Movies Have Attributes\n",
    "    * Genres: Action, Romance, Comedy, etc.\n",
    "    * Actors: Tom Cruise, Tom Hanks, Megan Fox, etc.\n",
    "    * Description: Long, Short, Subtitles, Foreign, Happy, Sad, etc.\n",
    "* What about using Linear Regression for Rating Prediction?\n",
    "    * Example: Rating Prediction = $$\\beta_0 + \\beta_1 \\times actionness + \\dots + \\beta_i \\times foxiness + \\dots + \\beta_j \\times sadness + \\epsilon$$\n",
    "    * Possible, but we would have to come up with some measure of actionness, etc. This is both subject to error and rather brittle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Using Matrix Factorization to Predict Ratings (or Recommendations)\n",
    "* Benefits of **Matrix Factorization**:\n",
    "    * Matrix Factorization could account for something along the lines of these attributes like linear regression\n",
    "    * All of the matrix factorization models that we know can be **interpreted as a linear combination of bases**\n",
    "    * There is a chance, especially with NMF, that those bases, latent features, could correspond with some of these \"attributes\" that we're looking to describe the movies\n",
    "* Factorization Issue/Requirement:\n",
    "\n",
    "|                  |                                                              UVD                                                             |                                                                SVD                                                                |                                                                  NMF                                                                  |\n",
    "|:----------------:|:----------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------:|\n",
    "| **Equation**     | $X \\approx UV$                                                                                                               | $X = USV^T$                                                                                                                       | $V \\approx WH$                                                                                                                        |\n",
    "| **Matrix Shape** | $U$ and $V$ will not (likely) be orthogonal                                                                                  | $U$ is an orthogonal matrix<br/>$S$ is a diagonal matrix of decreasing positive \"singular\" values<br/>$V$ is an orthogonal matrix | Same as UVD, but with one extra constraint: **all values of $V, W, H$ must be non-negative**<br/>NMF is a specialization of UVD |\n",
    "| **Solution**     | Has many approximate non-unique solutions<br/>Non-convex optimization with many local minima<br/>Has a tunable parameter $k$ | Has a unique, exact solution                                                                                                      | Both NMF and UVD are approximate factorizations, and both optimize to reduce the RSS                                              |\n",
    "* PCA/SVD/NMF vs UVD:\n",
    "    * Problem: PCA, SVD, and NMF **all** must be computed on a **dense** data matrix, $X$\n",
    "        * Possible Solution: Impute missing values, naively, with something like the mean of the known values. (note: this is what sklearn does when it says it factorizes sparse matrices)\n",
    "    * Missing Values:\n",
    "        * **SVD** - works **poorly** if $X$ has missing values\n",
    "            * (-) Forced to fill in missing values\n",
    "            * (-) Solution fits these fill-values\n",
    "            * (-) Makes for a much larger memory footprint\n",
    "            * (-) Slow to compute for large matrices\n",
    "        * **UVD** - handles missing values when computed via **SGD**\n",
    "* Factorization Goal:\n",
    "    * Create a factorization for a sparse data matrix, $X$, into $U V$, such that the reconstruction to $\\hat{X}$ serves as a model: $$X_{m\\times n}\\approx U_{m\\times k}V_{k\\times n}$$ $$x_{i,j} \\approx u_i v_j$$\n",
    "    * More formally, for a previously unknown entry in $X$:\n",
    "        * $X_{i,j}$ for the corresponding entry in $\\hat{X}$\n",
    "        * $\\hat{X}_{i,j}$ serves as a prediction\n",
    "    * Since we could easily **overfit** the known values in $X$, we will want to **regularize**\n",
    "        * Regularization by reducing the inner dimension in $U_k$ and $V_k$\n",
    "    * Factorization visualized:\n",
    "    ![factorization](factorization.png)\n",
    "    * Reconstruction visualized:\n",
    "    ![reconstruction](reconstruction.png)\n",
    "* Difference between Collaborative Filtering (CF) and Matrix Factorization (MF):\n",
    "    * **Collaborative Filtering (Neighborhood Models)** $\\rightarrow$ **Memory Based**\n",
    "        * Just store data so we can query what or whom is most similar when asked to recommend\n",
    "    * **Factorization Techniques** $\\rightarrow$ **Model Based**\n",
    "        * Creates predictions, from which the most suitable can be recommended\n",
    "* Computing the Factorization\n",
    "    * Similar to what we did to find the factorization in NMF, we're going to **minimize a cost function**\n",
    "    * Now, though we can't minimize at the level of the **entirety** of $X$, since it is **sparse** (99% missing data)\n",
    "    * However, we can **optimize with respect to the data in $X$** that we do have (1% of known data)\n",
    "* Factorization Plan\n",
    "    * **UV Decomposition (UVD)** Steps:\n",
    "        1. Choose $K$\n",
    "        2. $UV$ approximates $X$ by necessity if $k$ is less than the rank of $R$\n",
    "        3. Usually choose: $k< min(n,m)$\n",
    "        4. Compute $U$ and $V$ such that: (least squares algorithm) $$arg\\text{ }min_{U,V}\\sum_{(i,j) \\in K}(X_{i,j}-U_i V_j)^2$$ \n",
    "    * For each of the known ratings in $X_{i,j}$, we want to minimize the square error in the prediction that results from $U_i  V_j$: $$min_{U,V}\\sum_{(i,j) \\in K}(X_{i,j}-U_i V_j)^2$$ \n",
    "    * where $U_i$ is the i$^{th}$ row of $U$\n",
    "    * $V_j$ is the j$^{th}$ row of $V$\n",
    "    * K is the set of indices in $X$ that have data\n",
    "* Reconstructing a Single Entry:\n",
    "![reconstructing_single](reconstructing_single.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Factorization Algorithms\n",
    "* Types of Algorithms\n",
    "    * **Alternating Least Squares (ALS)** - minimization by rotating between fixing the $U_i$ to solve for the $V_j$ and fixing the $V_j$ to solve for the $U_i$\n",
    "    * **Funk SVD** - developed by Simon Funk during the Netflix prize which is a popular alternative version of gradient descent\n",
    "* ALS vs SGD:\n",
    "\n",
    "|                          |                                          ALS                                          |                            SGD                           |\n",
    "|:------------------------:|:-------------------------------------------------------------------------------------:|:--------------------------------------------------------:|\n",
    "| **Speed**                | Parallelizes very well                                                                | Faster (if on single machine)                            |\n",
    "| **Learning Rate**        |                                                                                       | Requires tuning learning rate                            |\n",
    "| **Availability/Results** | Available in Spark/MLlib                                                              | Anecdotal evidence of better results                     |\n",
    "| **Missing Values**       | Only appropriate for matrices that don't have missing values (needs **dense** matrix) | Works with missing values (works with **sparse** matrix) |\n",
    "\n",
    "* Questions to consider if considering SVD algorithm for recommendations:\n",
    "    * Would using SVD be good for this sparse utility matrix (we used it previously for finding latent features)?\n",
    "    * What's the problem with using SVD on this sparse utility matrix?\n",
    "    * What UVD (or NMF) work better than SVD to find latent factors when the utility matrix is sparse?\n",
    "* UVD (or NMF) + SGD is normally the best option for Recommender Systems:\n",
    "    * NMF + SGD is \"best in class\" option for **many** recommender domains:\n",
    "        * (+) no need to impute missing values\n",
    "        * (+) use regularization to avoid overfitting\n",
    "        * (+) optionally include biases terms to communicate prior knowledge\n",
    "        * (+) can handle time-dynamics (e.g. change in user preference over time)\n",
    "        * (+) used by the winning entry in the Netflix challenge\n",
    "* **Funk SVD**\n",
    "    * Define the error on a particular prediction in $X$: $$e_{i,j}=X_{i,j}-\\hat{X}_{i,j}$$\n",
    "    * Then, we can update the columns in $U$ and $V$ with: $$U_i \\leftarrow U_i + v(e_{i,j}V_j)$$ $$V_j \\leftarrow V_j + v(e_{i,j}U_i)$$\n",
    "    * Funk SVD Algorithm:\n",
    "        * Initialize $U$ and $V$ with small random values\n",
    "        * While error is decreasing:\n",
    "            * For each user, $i$:\n",
    "                * For each item rated by that user, $j$:\n",
    "                    1. Predict rating, $\\hat{X}_{i,j}$\n",
    "                    2. Calculate $e_{i,j}$\n",
    "                    3. Update $U_i$ and $V_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Factorization Nuances\n",
    "* **Baseline Predictors (Biases)**\n",
    "    * (-) Much of the observed ratings are associated with a specific user's personality (user bias) or an item's intrinsic value (item bias), **not an interaction between the item and user**, which is what we get captured in the factorization\n",
    "        * e.g. Some items (e.g. movies) have a tendency to be rated high, some low\n",
    "        * e.g. Some users have a tendency to rate high, some low\n",
    "    * To encapsulate these effects, which do not involve user-item interactions, we introduce **baseline predictors**: $$b_{i,j}=\\mu+b_i+b_j$$\n",
    "        * $b_{i,j} \\rightarrow$ overall bias of the rating by user $i$ for item $j$\n",
    "        * $\\mu \\rightarrow$ overall average rating in $X$\n",
    "        * $b_i \\rightarrow$ user $i$'s average deviation from the overall average\n",
    "        * $b_j \\rightarrow$ item $j$'s average deviation from the overall average\n",
    "    * From this, we can describe our predictions with: $$\\hat{X}_{i,j}=\\mu+b_i+b_j+U_i V_j$$\n",
    "        * $X_{i,j} \\rightarrow$ the prediction of user $i$ rating item $j$\n",
    "        * $\\mu \\rightarrow$ the average rating\n",
    "        * $b_i \\rightarrow$ user $i$'s tendency to deviate from the average\n",
    "        * $b_j \\rightarrow$ item $j$'s tendency to deviate from the average\n",
    "        * $U_i V_j \\rightarrow$ the prediction of how user $i$ will interact with item $j$\n",
    "* **Regularization of Spare Dataset**\n",
    "    * Another way to regularize our decomposition to help prevent from overfitting to our sparse data is via a penalty, $\\lambda$, placed on the magnitude of: $b_i, b_j, U_i, V_j$. The most common is the $L_2$ norm\n",
    "    * Such a penalty changes our cost function: $$min_{b,U,V}\\sum_{(i,j) \\in K}(X_{i,j}-U_i V_j)^2 + \\lambda(b_i^2+b_j^2+|U_i|^2 + |V_j|^2)$$\n",
    "    * With these consideration, the regularization update rules become: $$b_i \\leftarrow b_i + v(e_{i,j}-\\lambda b_i)$$ $$b_j \\leftarrow b_j + v(e_{i,j}-\\lambda b_j)$$ $$U_i \\leftarrow U_i + v(e_{i,j}V_j-\\lambda U_i)$$ $$V_j \\leftarrow V_j + v(e_{i,j}U_i-\\lambda V_j)$$\n",
    "* **Validation For Recommenders**\n",
    "    * Validating any recommender is difficult, but it is necessary as we're going to want to tune the hyperparameters that we introduced into our model, $v$ and $\\lambda$\n",
    "    * The most frequently used metric is RMSE on the known data: $$RMSE=\\sqrt{\\sum_{(i,j)\\in K}(X_{i,j}-\\hat{X}_{i,j})^2}$$\n",
    "    * Example: RMSE over the Netflix dataset using various matrix factorization models\n",
    "    ![netflix_example](netflix_example.png)\n",
    "        * Numbers on the chart denote each model's dimensionality, $k$\n",
    "        * The more refined models perform better and have lower errors\n",
    "        * **Netflix's inhouse model performs at RMSE=0.9514 on this dataset** (the worst score), so even the **simple** matrix factorization models are beating it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Matrix Factorization Pros & Cons\n",
    "* (+) Decent with sparsity, so long as we regularize\n",
    "* (+) Prediction is fast, only need to do an inner product\n",
    "* (+) Can inspect latent features for topical meaning\n",
    "* (+) Can be extended to include side information\n",
    "* (-) Need to re-factorize with new data. Very slow\n",
    "* (-) Fails in the cold start case\n",
    "* (-) Not great open source tools for huge matrices\n",
    "* (-) Difficult to tune directly to the type of recommendation you want to make. Tied to the difficulty of measuring success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Advanced Factorization Models\n",
    "1. **Non-negativity constraint** - more interpretable latent features\n",
    "2. **SVD++** - uses implicit feedback (e.g. clicks, likes, etc.) to enhance model\n",
    "3. **Time-aware Factor Model** - accounts for temporal information about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
